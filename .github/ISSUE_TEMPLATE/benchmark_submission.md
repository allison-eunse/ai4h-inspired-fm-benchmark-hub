---
name: ðŸ“Š Benchmark Submission
about: Submit your model's benchmark results for review
title: "[SUBMISSION] Model Name - Benchmark Name"
labels: submission, pending-review
assignees: allison-eunse
---

## Model Information

**Model Name**: 
**Model Type**: (e.g., fMRI Foundation Model, Genomics FM, etc.)
**Paper/Preprint**: (link if available)
**Code Repository**: (link if public)

## Benchmark Results

**Benchmark ID**: (e.g., BM-001, BM-FMRI-GRANULAR, robustness_testing)
**Dataset Used**: 

### Overall Metrics

| Metric | Value |
|--------|-------|
| AUROC | |
| Accuracy | |
| F1-Score | |
| (other) | |

### Stratified Results (if applicable)

<details>
<summary>By Scanner</summary>

| Scanner | AUROC | Accuracy | N |
|---------|-------|----------|---|
| Siemens | | | |
| GE | | | |
| Philips | | | |

</details>

<details>
<summary>By Site/Dataset</summary>

| Site | AUROC | N |
|------|-------|---|
| | | |

</details>

## Evaluation Details

**Hardware Used**: 
**Runtime**: 
**Reproducibility Seed**: 
**fmbench Version**: 

## Checklist

- [ ] I ran the official fmbench evaluation suite
- [ ] Results are reproducible with the provided seed
- [ ] I have included all required metrics
- [ ] Model is publicly available or described in a paper

## Additional Notes

(Any other relevant information about the evaluation)

