# Report Generation Quality Benchmark
# For FMs that generate clinical reports, summaries, or interpretations

benchmark_id: BM-REPORT-GEN
name: "Clinical Report Generation Quality"
version: "1.0.0"

# DEL10: Health Topic Definition
health_domain: "Cross-Domain"
health_topic: "Automated Clinical Reporting"
clinical_relevance: >
  Foundation models increasingly generate clinical reports, radiology 
  interpretations, and patient summaries. Quality metrics must capture
  both linguistic fluency and clinical accuracy/safety.

# DEL10: AI Task Specification
ai_task: "Generation"
input_modality: "Multi-modal (Images, Text, Structured Data)"
output_type: "Report/Text"

# Report Quality Metrics
metrics:
  # Linguistic Quality
  linguistic:
    - metric_id: "bleu"
      name: "BLEU Score"
      description: "N-gram precision vs reference reports"
      range: [0, 100]
      
    - metric_id: "rouge_l"
      name: "ROUGE-L"
      description: "Longest common subsequence with reference"
      range: [0, 1]
      
    - metric_id: "bertscore"
      name: "BERTScore"
      description: "Semantic similarity using BERT embeddings"
      range: [0, 1]
      
    - metric_id: "meteor"
      name: "METEOR"
      description: "Alignment-based metric with synonyms/stems"
      range: [0, 1]
  
  # Clinical Quality
  clinical:
    - metric_id: "clinical_accuracy"
      name: "Clinical Accuracy"
      description: "Factual correctness of medical statements"
      range: [0, 1]
      requires: "Expert annotation"
      
    - metric_id: "finding_recall"
      name: "Finding Recall"
      description: "Fraction of true findings mentioned"
      range: [0, 1]
      
    - metric_id: "finding_precision"
      name: "Finding Precision"
      description: "Fraction of mentioned findings that are correct"
      range: [0, 1]
      
    - metric_id: "hallucination_rate"
      name: "Hallucination Rate"
      description: "Fraction of statements not grounded in input"
      range: [0, 1]
      lower_is_better: true
      
    - metric_id: "omission_rate"
      name: "Critical Omission Rate"
      description: "Missing clinically important findings"
      range: [0, 1]
      lower_is_better: true
  
  # Safety Metrics
  safety:
    - metric_id: "harmful_content"
      name: "Harmful Content Rate"
      description: "Potentially dangerous recommendations"
      range: [0, 1]
      lower_is_better: true
      critical: true
      
    - metric_id: "uncertainty_calibration"
      name: "Uncertainty Calibration"
      description: "Appropriate hedging for uncertain findings"
      range: [0, 1]
  
  # Readability
  readability:
    - metric_id: "flesch_kincaid"
      name: "Flesch-Kincaid Grade Level"
      description: "Reading difficulty level"
      target_range: [8, 12]
      
    - metric_id: "structure_score"
      name: "Structure Score"
      description: "Adherence to standard report structure"
      range: [0, 1]

# Aggregate scores
aggregate_metrics:
  - metric_id: "report_quality_score"
    name: "Overall Report Quality"
    formula: "0.3*clinical_accuracy + 0.2*finding_f1 + 0.2*bertscore + 0.15*(1-hallucination_rate) + 0.15*(1-omission_rate)"
    range: [0, 1]

# Stratification for sub-leaderboards
stratification:
  - "Report Type"  # Radiology, pathology, discharge summary
  - "Modality"     # CT, MRI, X-ray, pathology slides
  - "Complexity"   # Simple, moderate, complex cases
  - "Institution"  # Multi-site generalization

# DEL3: Requirements
requirements:
  ground_truth: "Reference reports from board-certified specialists"
  annotation: "Clinical accuracy requires expert review"
  safety_review: "Mandatory harmful content screening"

# Example report types
report_types:
  radiology:
    - "Chest X-ray interpretation"
    - "Brain MRI report"
    - "CT abdomen findings"
  pathology:
    - "Histopathology report"
    - "Cytology findings"
  clinical:
    - "Discharge summary"
    - "Progress note"
    - "Consultation report"

# References
references:
  - "ITU-T FG-AI4H DEL3 (Requirements)"
  - "BLEU: Papineni et al., 2002"
  - "BERTScore: Zhang et al., 2020"
  - "RadGraph: Jain et al., 2021"

