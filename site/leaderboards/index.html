
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Benchmarking Hub for Genetics & Brain Foundation Models">
      
      
      
        <link rel="canonical" href="https://allison-eunse.github.io/ai4h-inspired-fm-benchmark-hub/leaderboards/">
      
      
        <link rel="prev" href="../how_it_works/">
      
      
        <link rel="next" href="../contributing/submission_guide/">
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.0">
    
    
      
        <title>Leaderboards - AI4H-Inspired FM Benchmarks</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.618322db.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#foundation-model-leaderboards" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="AI4H-Inspired FM Benchmarks" class="md-header__button md-logo" aria-label="AI4H-Inspired FM Benchmarks" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21.33 12.91c.09 1.55-.62 3.04-1.89 3.95l.77 1.49c.23.45.26.98.06 1.45-.19.47-.58.84-1.06 1l-.79.25a1.687 1.687 0 0 1-1.86-.55L14.44 18c-.89-.15-1.73-.53-2.44-1.1-.5.15-1 .23-1.5.23-.88 0-1.76-.27-2.5-.79-.53.16-1.07.23-1.62.22-.79.01-1.57-.15-2.3-.45a4.1 4.1 0 0 1-2.43-3.61c-.08-.72.04-1.45.35-2.11-.29-.75-.32-1.57-.07-2.33C2.3 7.11 3 6.32 3.87 5.82c.58-1.69 2.21-2.82 4-2.7 1.6-1.5 4.05-1.66 5.83-.37.42-.11.86-.17 1.3-.17 1.36-.03 2.65.57 3.5 1.64 2.04.53 3.5 2.35 3.58 4.47.05 1.11-.25 2.2-.86 3.13.07.36.11.72.11 1.09m-5-1.41c.57.07 1.02.5 1.02 1.07a1 1 0 0 1-1 1h-.63c-.32.9-.88 1.69-1.62 2.29.25.09.51.14.77.21 5.13-.07 4.53-3.2 4.53-3.25a2.59 2.59 0 0 0-2.69-2.49 1 1 0 0 1-1-1 1 1 0 0 1 1-1c1.23.03 2.41.49 3.33 1.3.05-.29.08-.59.08-.89-.06-1.24-.62-2.32-2.87-2.53-1.25-2.96-4.4-1.32-4.4-.4-.03.23.21.72.25.75a1 1 0 0 1 1 1c0 .55-.45 1-1 1-.53-.02-1.03-.22-1.43-.56-.48.31-1.03.5-1.6.56-.57.05-1.04-.35-1.07-.9a.97.97 0 0 1 .88-1.1c.16-.02.94-.14.94-.77 0-.66.25-1.29.68-1.79-.92-.25-1.91.08-2.91 1.29C6.75 5 6 5.25 5.45 7.2 4.5 7.67 4 8 3.78 9c1.08-.22 2.19-.13 3.22.25.5.19.78.75.59 1.29-.19.52-.77.78-1.29.59-.73-.32-1.55-.34-2.3-.06-.32.27-.32.83-.32 1.27 0 .74.37 1.43 1 1.83.53.27 1.12.41 1.71.4q-.225-.39-.39-.81a1.038 1.038 0 0 1 1.96-.68c.4 1.14 1.42 1.92 2.62 2.05 1.37-.07 2.59-.88 3.19-2.13.23-1.38 1.34-1.5 2.56-1.5m2 7.47-.62-1.3-.71.16 1 1.25zm-4.65-8.61a1 1 0 0 0-.91-1.03c-.71-.04-1.4.2-1.93.67-.57.58-.87 1.38-.84 2.19a1 1 0 0 0 1 1c.57 0 1-.45 1-1 0-.27.07-.54.23-.76.12-.1.27-.15.43-.15.55.03 1.02-.38 1.02-.92"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AI4H-Inspired FM Benchmarks
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Leaderboards
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="deep-purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="deep-purple"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    ai4h-inspired-fm-benchmark-hub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../start_here/" class="md-tabs__link">
        
  
  
    
  
  Start Here

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../how_it_works/" class="md-tabs__link">
        
  
  
    
  
  How It Works

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
  
    
  
  Leaderboards

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../contributing/submission_guide/" class="md-tabs__link">
          
  
  
  Submit Results

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../integration/analysis_recipes/cca_permutation/" class="md-tabs__link">
          
  
  
  Protocols (Recipes)

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../data_sources/" class="md-tabs__link">
          
  
  
  Data Specs

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../models/" class="md-tabs__link">
          
  
  
  Models

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../design/ai4h_alignment/" class="md-tabs__link">
          
  
  
  Design

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../credits/" class="md-tabs__link">
        
  
  
    
  
  Credits

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="AI4H-Inspired FM Benchmarks" class="md-nav__button md-logo" aria-label="AI4H-Inspired FM Benchmarks" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21.33 12.91c.09 1.55-.62 3.04-1.89 3.95l.77 1.49c.23.45.26.98.06 1.45-.19.47-.58.84-1.06 1l-.79.25a1.687 1.687 0 0 1-1.86-.55L14.44 18c-.89-.15-1.73-.53-2.44-1.1-.5.15-1 .23-1.5.23-.88 0-1.76-.27-2.5-.79-.53.16-1.07.23-1.62.22-.79.01-1.57-.15-2.3-.45a4.1 4.1 0 0 1-2.43-3.61c-.08-.72.04-1.45.35-2.11-.29-.75-.32-1.57-.07-2.33C2.3 7.11 3 6.32 3.87 5.82c.58-1.69 2.21-2.82 4-2.7 1.6-1.5 4.05-1.66 5.83-.37.42-.11.86-.17 1.3-.17 1.36-.03 2.65.57 3.5 1.64 2.04.53 3.5 2.35 3.58 4.47.05 1.11-.25 2.2-.86 3.13.07.36.11.72.11 1.09m-5-1.41c.57.07 1.02.5 1.02 1.07a1 1 0 0 1-1 1h-.63c-.32.9-.88 1.69-1.62 2.29.25.09.51.14.77.21 5.13-.07 4.53-3.2 4.53-3.25a2.59 2.59 0 0 0-2.69-2.49 1 1 0 0 1-1-1 1 1 0 0 1 1-1c1.23.03 2.41.49 3.33 1.3.05-.29.08-.59.08-.89-.06-1.24-.62-2.32-2.87-2.53-1.25-2.96-4.4-1.32-4.4-.4-.03.23.21.72.25.75a1 1 0 0 1 1 1c0 .55-.45 1-1 1-.53-.02-1.03-.22-1.43-.56-.48.31-1.03.5-1.6.56-.57.05-1.04-.35-1.07-.9a.97.97 0 0 1 .88-1.1c.16-.02.94-.14.94-.77 0-.66.25-1.29.68-1.79-.92-.25-1.91.08-2.91 1.29C6.75 5 6 5.25 5.45 7.2 4.5 7.67 4 8 3.78 9c1.08-.22 2.19-.13 3.22.25.5.19.78.75.59 1.29-.19.52-.77.78-1.29.59-.73-.32-1.55-.34-2.3-.06-.32.27-.32.83-.32 1.27 0 .74.37 1.43 1 1.83.53.27 1.12.41 1.71.4q-.225-.39-.39-.81a1.038 1.038 0 0 1 1.96-.68c.4 1.14 1.42 1.92 2.62 2.05 1.37-.07 2.59-.88 3.19-2.13.23-1.38 1.34-1.5 2.56-1.5m2 7.47-.62-1.3-.71.16 1 1.25zm-4.65-8.61a1 1 0 0 0-.91-1.03c-.71-.04-1.4.2-1.93.67-.57.58-.87 1.38-.84 2.19a1 1 0 0 0 1 1c.57 0 1-.45 1-1 0-.27.07-.54.23-.76.12-.1.27-.15.43-.15.55.03 1.02-.38 1.02-.92"/></svg>

    </a>
    AI4H-Inspired FM Benchmarks
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    ai4h-inspired-fm-benchmark-hub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../start_here/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Start Here
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../how_it_works/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    How It Works
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Leaderboards
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Leaderboards
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#example-what-a-real-submission-looks-like" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example: what a real submission looks like
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#jump-to" class="md-nav__link">
    <span class="md-ellipsis">
      
        ğŸ§­ Jump To
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overall-rankings-all-modalities" class="md-nav__link">
    <span class="md-ellipsis">
      
        ğŸŒ Overall Rankings (All Modalities)
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#genomics" class="md-nav__link">
    <span class="md-ellipsis">
      
        ğŸ§¬ Genomics
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ğŸ§¬ Genomics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification" class="md-nav__link">
    <span class="md-ellipsis">
      
        ğŸ¯ Classification
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ğŸ¯ Classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dna-promoter-classification" class="md-nav__link">
    <span class="md-ellipsis">
      
        DNA Promoter Classification
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dna-enhancer-classification" class="md-nav__link">
    <span class="md-ellipsis">
      
        DNA Enhancer Classification
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cell-type-annotation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cell Type Annotation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generation" class="md-nav__link">
    <span class="md-ellipsis">
      
        âœï¸ Generation
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="âœï¸ Generation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#clinical-report-generation-quality" class="md-nav__link">
    <span class="md-ellipsis">
      
        Clinical Report Generation Quality
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#brain-imaging-mrifmri" class="md-nav__link">
    <span class="md-ellipsis">
      
        ğŸ§  Brain Imaging (MRI/fMRI)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ğŸ§  Brain Imaging (MRI/fMRI)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        ğŸ¯ Classification
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ğŸ¯ Classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#toy-classification-benchmark" class="md-nav__link">
    <span class="md-ellipsis">
      
        Toy Classification Benchmark
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alzheimers-disease-classification-using-brain-mri" class="md-nav__link">
    <span class="md-ellipsis">
      
        Alzheimer's Disease Classification using Brain MRI
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#classificationreconstruction" class="md-nav__link">
    <span class="md-ellipsis">
      
        ğŸ“‹ Classification/Reconstruction
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ğŸ“‹ Classification/Reconstruction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fmri-foundation-model-benchmark-granular" class="md-nav__link">
    <span class="md-ellipsis">
      
        fMRI Foundation Model Benchmark (Granular)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reconstruction" class="md-nav__link">
    <span class="md-ellipsis">
      
        ğŸ”„ Reconstruction
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ğŸ”„ Reconstruction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#brain-time-series-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Brain Time-Series Modeling
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#other-benchmarks" class="md-nav__link">
    <span class="md-ellipsis">
      
        ğŸ“‹ Other Benchmarks
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ğŸ“‹ Other Benchmarks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#foundation-model-robustness-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Foundation Model Robustness Evaluation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#add-your-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        ğŸš€ Add Your Model
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Submit Results
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Submit Results
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../contributing/submission_guide/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Submission Guide
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Protocols (Recipes)
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Protocols (Recipes)
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../integration/analysis_recipes/cca_permutation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    CCA & Permutation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../integration/analysis_recipes/prediction_baselines/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Prediction Baselines
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../integration/analysis_recipes/partial_correlations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Partial Correlations
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Data Specs
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Data Specs
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data_sources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Free Data Sources
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../integration/modality_features/fmri/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    fMRI
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../integration/modality_features/smri/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    sMRI
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../integration/modality_features/genomics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Genomics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Models
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            
  
    Models
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Design
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            
  
    Design
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../design/ai4h_alignment/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AI4H Alignment
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../credits/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Credits
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="foundation-model-leaderboards">ğŸ† Foundation Model Leaderboards</h1>
<div class="admonition success">
<p class="admonition-title">Benchmark Hub Overview</p>
<p>ğŸ“Š <strong>9</strong> Benchmarks | ğŸ¤– <strong>24</strong> Models | ğŸ“ˆ <strong>59</strong> Evaluations</p>
</div>
<blockquote>
<p><strong>What is this?</strong> This page ranks AI models for healthcare applications. 
Higher-ranked models perform better on standardized tests.</p>
<p><strong>How to read it:</strong> Each table shows models from best (ğŸ¥‡) to developing (ğŸ“ˆ).
Click "How are scores calculated?" for details on what the numbers mean.</p>
</blockquote>
<h2 id="example-what-a-real-submission-looks-like">Example: what a real submission looks like</h2>
<p>This is a <strong>real, end-to-end</strong> run using the built-in baseline model. Your submission should look like this: a local run that produces <code>report.md</code> + <code>eval.yaml</code>.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model ID</th>
<th style="text-align: left;">Suite / Benchmark</th>
<th style="text-align: left;">Task</th>
<th style="text-align: right;"><abbr title="Area Under the Receiver Operating Characteristic curve">AUROC</abbr></th>
<th style="text-align: right;"><abbr title="Reverse area-under-curve for channel dropout robustness">dropout rAUC</abbr></th>
<th style="text-align: right;"><abbr title="Reverse area-under-curve for Gaussian noise robustness">noise rAUC</abbr></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><code>dummy_classifier</code></td>
<td style="text-align: left;"><code>SUITE-TOY-CLASS</code> / <code>BM-TOY-CLASS</code></td>
<td style="text-align: left;">Toy fMRI-like classification</td>
<td style="text-align: right;">0.5597</td>
<td style="text-align: right;">0.7760</td>
<td style="text-align: right;">0.7867</td>
</tr>
</tbody>
</table>
<p><strong>Artifacts:</strong> <a href="https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub/blob/main/evals/SUITE-TOY-CLASS-dummy_classifier-20251127-071011.yaml">Example classification eval.yaml</a> Â· <a href="https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub/blob/main/reports/SUITE-TOY-CLASS-dummy_classifier-20251127-071011.md">Example classification report.md</a> Â· <a href="https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub/blob/main/evals/ROBUSTNESS-dummy_classifier-20251127-071004.yaml">Example robustness eval.yaml</a> Â· <a href="https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub/blob/main/reports/ROBUSTNESS-dummy_classifier-20251127-071004.md">Example robustness report.md</a></p>
<hr />
<h2 id="jump-to">ğŸ§­ Jump To</h2>
<ul>
<li><a href="#overall-rankings-all-modalities">ğŸŒ Overall Rankings</a> â€” Best across all categories</li>
<li><a href="#genomics">ğŸ§¬ Genomics</a></li>
<li><a href="#brain-imaging-mrifmri">ğŸ§  Brain Imaging (MRI/fMRI)</a></li>
</ul>
<hr />
<h2 id="overall-rankings-all-modalities">ğŸŒ Overall Rankings (All Modalities)</h2>
<p><em>Best score per model across all benchmarks</em></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Rank</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Best Score</th>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">Benchmark</th>
<th style="text-align: left;">Modality</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ğŸ¥‡</td>
<td style="text-align: left;"><strong>BrainMT</strong> ğŸ‘‘</td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: left;"><code>dropout_rAUC</code></td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">ğŸ“Š Other</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥ˆ</td>
<td style="text-align: left;"><strong>SWIFT</strong></td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: left;"><code>dropout_rAUC</code></td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">ğŸ“Š Other</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥‰</td>
<td style="text-align: left;"><strong>neuroclips</strong></td>
<td style="text-align: center;">0.9999</td>
<td style="text-align: left;"><code>dropout_rAUC</code></td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">ğŸ“Š Other</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ…</td>
<td style="text-align: left;">geneformer</td>
<td style="text-align: center;">0.9995</td>
<td style="text-align: left;"><code>robustness_score</code></td>
<td style="text-align: left;">Foundation Model Robustness Evaluation</td>
<td style="text-align: left;">ğŸ“Š Other</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ…</td>
<td style="text-align: left;">Caduceus</td>
<td style="text-align: center;">0.9841</td>
<td style="text-align: left;"><code>dropout_rAUC</code></td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">ğŸ“Š Other</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ–ï¸</td>
<td style="text-align: left;">Evo 2</td>
<td style="text-align: center;">0.9841</td>
<td style="text-align: left;"><code>dropout_rAUC</code></td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">ğŸ“Š Other</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ–ï¸</td>
<td style="text-align: left;">Brain-JEPA</td>
<td style="text-align: center;">0.9350</td>
<td style="text-align: left;"><code>AUROC</code></td>
<td style="text-align: left;">Alzheimer's Disease Classification using Brain MRI</td>
<td style="text-align: left;">ğŸ§  Brain Imaging (MRI/fMRI)</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ–ï¸</td>
<td style="text-align: left;">UNI</td>
<td style="text-align: center;">0.9200</td>
<td style="text-align: left;"><code>AUROC</code></td>
<td style="text-align: left;">Alzheimer's Disease Classification using Brain MRI</td>
<td style="text-align: left;">ğŸ§  Brain Imaging (MRI/fMRI)</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ–ï¸</td>
<td style="text-align: left;">Geneformer</td>
<td style="text-align: center;">0.9100</td>
<td style="text-align: left;"><code>Accuracy</code></td>
<td style="text-align: left;">Cell Type Annotation</td>
<td style="text-align: left;">ğŸ§¬ Genomics</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ–ï¸</td>
<td style="text-align: left;">BrainLM</td>
<td style="text-align: center;">0.9100</td>
<td style="text-align: left;"><code>AUROC</code></td>
<td style="text-align: left;">fMRI Foundation Model Benchmark (Granular)</td>
<td style="text-align: left;">ğŸ§  Brain Imaging (MRI/fMRI)</td>
</tr>
<tr>
<td style="text-align: center;">#11</td>
<td style="text-align: left;">Me-LLaMA</td>
<td style="text-align: center;">0.8750</td>
<td style="text-align: left;"><code>report_quality_score</code></td>
<td style="text-align: left;">Clinical Report Generation Quality</td>
<td style="text-align: left;">ğŸ§¬ Genomics</td>
</tr>
<tr>
<td style="text-align: center;">#12</td>
<td style="text-align: left;">HyenaDNA</td>
<td style="text-align: center;">0.8720</td>
<td style="text-align: left;"><code>AUROC</code></td>
<td style="text-align: left;">DNA Promoter Classification</td>
<td style="text-align: left;">ğŸ§¬ Genomics</td>
</tr>
<tr>
<td style="text-align: center;">#13</td>
<td style="text-align: left;">HyenaDNA</td>
<td style="text-align: center;">0.8700</td>
<td style="text-align: left;"><code>Accuracy</code></td>
<td style="text-align: left;">Cell Type Annotation</td>
<td style="text-align: left;">ğŸ§¬ Genomics</td>
</tr>
<tr>
<td style="text-align: center;">#14</td>
<td style="text-align: left;">BrainBERT</td>
<td style="text-align: center;">0.8700</td>
<td style="text-align: left;"><code>AUROC</code></td>
<td style="text-align: left;">fMRI Foundation Model Benchmark (Granular)</td>
<td style="text-align: left;">ğŸ§  Brain Imaging (MRI/fMRI)</td>
</tr>
<tr>
<td style="text-align: center;">#15</td>
<td style="text-align: left;">M3FM</td>
<td style="text-align: center;">0.8600</td>
<td style="text-align: left;"><code>report_quality_score</code></td>
<td style="text-align: left;">Clinical Report Generation Quality</td>
<td style="text-align: left;">ğŸ§¬ Genomics</td>
</tr>
<tr>
<td style="text-align: center;">#16</td>
<td style="text-align: left;">DNABERT-2</td>
<td style="text-align: center;">0.8500</td>
<td style="text-align: left;"><code>Accuracy</code></td>
<td style="text-align: left;">Cell Type Annotation</td>
<td style="text-align: left;">ğŸ§¬ Genomics</td>
</tr>
<tr>
<td style="text-align: center;">#17</td>
<td style="text-align: left;">BrainHarmony</td>
<td style="text-align: center;">0.8450</td>
<td style="text-align: left;"><code>robustness_score</code></td>
<td style="text-align: left;">Foundation Model Robustness Evaluation</td>
<td style="text-align: left;">ğŸ“Š Other</td>
</tr>
<tr>
<td style="text-align: center;">#18</td>
<td style="text-align: left;">OpenFlamingo</td>
<td style="text-align: center;">0.8400</td>
<td style="text-align: left;"><code>report_quality_score</code></td>
<td style="text-align: left;">Clinical Report Generation Quality</td>
<td style="text-align: left;">ğŸ§¬ Genomics</td>
</tr>
<tr>
<td style="text-align: center;">#19</td>
<td style="text-align: left;">kmer_k6</td>
<td style="text-align: center;">0.8357</td>
<td style="text-align: left;"><code>AUROC</code></td>
<td style="text-align: left;">DNA Promoter Classification</td>
<td style="text-align: left;">ğŸ§¬ Genomics</td>
</tr>
<tr>
<td style="text-align: center;">#20</td>
<td style="text-align: left;">NeuroClips</td>
<td style="text-align: center;">0.8300</td>
<td style="text-align: left;"><code>AUROC</code></td>
<td style="text-align: left;">fMRI Foundation Model Benchmark (Granular)</td>
<td style="text-align: left;">ğŸ§  Brain Imaging (MRI/fMRI)</td>
</tr>
<tr>
<td style="text-align: center;">#21</td>
<td style="text-align: left;">TITAN</td>
<td style="text-align: center;">0.8100</td>
<td style="text-align: left;"><code>report_quality_score</code></td>
<td style="text-align: left;">Clinical Report Generation Quality</td>
<td style="text-align: left;">ğŸ§¬ Genomics</td>
</tr>
<tr>
<td style="text-align: center;">#22</td>
<td style="text-align: left;">Baseline (Random/Majority)</td>
<td style="text-align: center;">0.7810</td>
<td style="text-align: left;"><code>robustness_score</code></td>
<td style="text-align: left;">Foundation Model Robustness Evaluation</td>
<td style="text-align: left;">ğŸ“Š Other</td>
</tr>
<tr>
<td style="text-align: center;">#23</td>
<td style="text-align: left;">Med-Flamingo</td>
<td style="text-align: center;">0.7800</td>
<td style="text-align: left;"><code>report_quality_score</code></td>
<td style="text-align: left;">Clinical Report Generation Quality</td>
<td style="text-align: left;">ğŸ§¬ Genomics</td>
</tr>
<tr>
<td style="text-align: center;">#24</td>
<td style="text-align: left;">RadBERT</td>
<td style="text-align: center;">0.6900</td>
<td style="text-align: left;"><code>report_quality_score</code></td>
<td style="text-align: left;">Clinical Report Generation Quality</td>
<td style="text-align: left;">ğŸ§¬ Genomics</td>
</tr>
</tbody>
</table>
<div class="admonition abstract">
<p class="admonition-title">Performance Distribution</p>
<p>â­ 10 Excellent | âœ… 11 Good | ğŸ”¶ 2 Fair | ğŸ“ˆ 1 Developing</p>
</div>
<hr />
<h2 id="genomics">ğŸ§¬ Genomics</h2>
<h3 id="classification">ğŸ¯ Classification</h3>
<h4 id="dna-promoter-classification">DNA Promoter Classification</h4>
<p>*Benchmark for classifying DNA sequences as promoters or non-promoters.
Promoters are regulatory regions at transcription start sites (TSS).
This benchmark focuses on non-TATA promoters, which lack the canonical
TATA box and represent ~75% of human promoters.
*</p>
<div align="center">

<div class="highlight"><pre><span></span><code>                    ğŸ†                    

              ğŸ¥‡  HyenaDNA              
                 (0.872)                 
             â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—             
             â•‘               â•‘             
   ğŸ¥ˆ DNABERT-2   â•‘               â•‘   ğŸ¥‰  HyenaDNA   
      (0.836)      â•‘               â•‘      (0.836)      
  â•”â•â•â•â•â•â•â•â•â•â•â•â•               â•šâ•â•â•â•â•â•â•â•â•â•â•â•—  
  â•‘                                       â•‘  
â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•
</code></pre></div>

</div>

<p><strong>4 models ranked by <code>AUROC</code>:</strong></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Rank</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Level</th>
<th style="text-align: left;">Details</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ğŸ¥‡</td>
<td style="text-align: left;"><strong>HyenaDNA</strong> ğŸ‘‘</td>
<td style="text-align: center;">0.8720</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">DS-DNA-PROMOTER, 2025-12-18T21:03:12.030852</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥ˆ</td>
<td style="text-align: left;"><strong>DNABERT-2</strong></td>
<td style="text-align: center;">0.8357</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">Human Non-TATA Promo, 2025-12-18T18:44:27.391206</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥‰</td>
<td style="text-align: left;"><strong>HyenaDNA</strong></td>
<td style="text-align: center;">0.8357</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">Human Non-TATA Promo, 2025-12-18T18:44:19.651418</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ…</td>
<td style="text-align: left;">kmer_k6</td>
<td style="text-align: center;">0.8357</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">Human Non-TATA Promo, 2025-12-18T18:44:10.847321</td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Quick Comparison</p>
<p><strong>ğŸ¥‡ HyenaDNA</strong> leads with AUROC = <strong>0.8720</strong></p>
<ul>
<li>Gap to ğŸ¥ˆ DNABERT-2: +0.0363</li>
<li>Score spread (best to worst): 0.0363</li>
</ul>
</div>
<details>
<summary>ğŸ“ <strong>How are scores calculated?</strong> (click to expand)</summary>

---

### ğŸ“– Understanding This Leaderboard

This section explains how we measure and compare AI models. Don't worry if you're new to AI metrics â€” we'll break it down step by step.

---

### ğŸ¯ The Main Metric: `AUROC`

**Area Under ROC Curve (AUROC)**

**In simple terms:** Measures how well the model can tell apart different categories (e.g., healthy vs. diseased)

**How it works:** Think of it like this: if you randomly pick one positive case and one negative case, AUROC tells you the probability that the model correctly identifies which is which. A score of 0.5 means the model is just guessing randomly (like flipping a coin), while 1.0 means it perfectly separates all cases.

**Score range:** 0.5 (random guessing) â†’ 1.0 (perfect separation)

ğŸ’¡ **Example:** An AUROC of 0.85 means the model correctly ranks a positive case higher than a negative case 85% of the time.

---

### ğŸ§  How This Metric Fits This Task

Different tasks emphasize different aspects of performance. Here's how this metric should be interpreted for this benchmark:

- For **classification** tasks (e.g., disease vs. no disease), this metric helps you understand
  how reliably the model separates different outcome groups. In addition to raw accuracy,
  we recommend also looking at metrics like AUROC and F1 Score, especially when classes are
  imbalanced (for example, when positive cases are rare).

---

### ğŸ“Š Performance Tiers: What Do the Scores Mean?

We group models into performance tiers to help you quickly understand how ready they are for different uses:

| Score Range | Rating | Interpretation | Suitable For |
|:---:|:---:|:---|:---|
| **â‰¥ 0.90** | â­ Excellent | Top-tier performance, consistently reliable | Clinical pilots with physician oversight |
| **0.80 â€“ 0.89** | âœ… Good | Strong performance, shows real promise | Validation studies, controlled testing |
| **0.70 â€“ 0.79** | ğŸ”¶ Fair | Moderate performance, has limitations | Research and development only |
| **< 0.70** | ğŸ“ˆ Developing | Below typical benchmarks, needs improvement | Early research, not for clinical use |

!!! tip "Important Context"
    These thresholds are general guidelines. The acceptable score depends on the specific clinical application, risk level, and whether the AI assists or replaces human judgment. Always consult domain experts when evaluating fitness for a particular use case.

---

### ğŸ“ How We Determine Rankings

Models are ranked following these principles:

1. **Primary metric determines rank** â€” The model with the highest score in the main metric ranks first. For metrics where lower is better (like error rates), the lowest score wins.

2. **Ties are broken by secondary metrics** â€” If two models have identical primary scores, we look at other relevant metrics to determine which performs better overall.

3. **Best run per model** â€” If a model was evaluated multiple times (e.g., with different settings), only its best result appears on the leaderboard. This ensures fair comparison.

4. **Reproducibility required** â€” All results must be reproducible. We record the evaluation date, dataset used, and configuration to ensure transparency.

---

### ğŸ¥ Why This Matters for Healthcare AI

Healthcare AI has higher stakes than many other AI applications. A model that works 95% of the time might sound good, but that 5% could mean missed diagnoses or incorrect treatments. That's why we:

- Use **multiple metrics** to capture different aspects of performance
- Test **robustness** to real-world data quality issues
- Require **transparency** about evaluation conditions
- Follow **international standards** for healthcare AI assessment

---

### ğŸŒ Standards Alignment

This benchmark follows the [ITU/WHO Focus Group on AI for Health (FG-AI4H)](https://www.itu.int/pub/T-FG-AI4H) framework, which provides internationally recognized guidelines for evaluating healthcare AI systems. This ensures our evaluations are:

- **Rigorous** â€” Following established scientific methodology
- **Comparable** â€” Using standardized metrics across different models
- **Trustworthy** â€” Aligned with WHO/ITU recommendations for health AI

</details>

<hr />
<h4 id="dna-enhancer-classification">DNA Enhancer Classification</h4>
<p>*Benchmark for classifying DNA sequences as enhancers or non-enhancers.
Enhancers are distal regulatory elements that activate gene expression.
Accurate enhancer prediction is critical for understanding gene regulation
and identifying disease-associated variants.
*</p>
<div align="center">

<div class="highlight"><pre><span></span><code>                    ğŸ†                    

              ğŸ¥‡  HyenaDNA              
                 (0.788)                 
             â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—             
             â•‘               â•‘             
   ğŸ¥ˆ DNABERT-2   â•‘               â•‘   ğŸ¥‰  HyenaDNA   
      (0.737)      â•‘               â•‘      (0.737)      
  â•”â•â•â•â•â•â•â•â•â•â•â•â•               â•šâ•â•â•â•â•â•â•â•â•â•â•â•—  
  â•‘                                       â•‘  
â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•
</code></pre></div>

</div>

<p><strong>4 models ranked by <code>AUROC</code>:</strong></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Rank</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Level</th>
<th style="text-align: left;">Details</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ğŸ¥‡</td>
<td style="text-align: left;"><strong>HyenaDNA</strong> ğŸ‘‘</td>
<td style="text-align: center;">0.7883</td>
<td style="text-align: center;">ğŸ”¶ Fair</td>
<td style="text-align: left;">DS-DNA-ENHANCER, 2025-12-18T21:03:03.285801</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥ˆ</td>
<td style="text-align: left;"><strong>DNABERT-2</strong></td>
<td style="text-align: center;">0.7365</td>
<td style="text-align: center;">ğŸ”¶ Fair</td>
<td style="text-align: left;">Human Enhancers (Coh, 2025-12-18T18:44:24.678525</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥‰</td>
<td style="text-align: left;"><strong>HyenaDNA</strong></td>
<td style="text-align: center;">0.7365</td>
<td style="text-align: center;">ğŸ”¶ Fair</td>
<td style="text-align: left;">Human Enhancers (Coh, 2025-12-18T18:44:17.006557</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ…</td>
<td style="text-align: left;">kmer_k6</td>
<td style="text-align: center;">0.7365</td>
<td style="text-align: center;">ğŸ”¶ Fair</td>
<td style="text-align: left;">Human Enhancers (Coh, 2025-12-18T18:44:08.075706</td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Quick Comparison</p>
<p><strong>ğŸ¥‡ HyenaDNA</strong> leads with AUROC = <strong>0.7883</strong></p>
<ul>
<li>Gap to ğŸ¥ˆ DNABERT-2: +0.0518</li>
<li>Score spread (best to worst): 0.0518</li>
</ul>
</div>
<details>
<summary>ğŸ“ <strong>How are scores calculated?</strong> (click to expand)</summary>

---

### ğŸ“– Understanding This Leaderboard

This section explains how we measure and compare AI models. Don't worry if you're new to AI metrics â€” we'll break it down step by step.

---

### ğŸ¯ The Main Metric: `AUROC`

**Area Under ROC Curve (AUROC)**

**In simple terms:** Measures how well the model can tell apart different categories (e.g., healthy vs. diseased)

**How it works:** Think of it like this: if you randomly pick one positive case and one negative case, AUROC tells you the probability that the model correctly identifies which is which. A score of 0.5 means the model is just guessing randomly (like flipping a coin), while 1.0 means it perfectly separates all cases.

**Score range:** 0.5 (random guessing) â†’ 1.0 (perfect separation)

ğŸ’¡ **Example:** An AUROC of 0.85 means the model correctly ranks a positive case higher than a negative case 85% of the time.

---

### ğŸ§  How This Metric Fits This Task

Different tasks emphasize different aspects of performance. Here's how this metric should be interpreted for this benchmark:

- For **classification** tasks (e.g., disease vs. no disease), this metric helps you understand
  how reliably the model separates different outcome groups. In addition to raw accuracy,
  we recommend also looking at metrics like AUROC and F1 Score, especially when classes are
  imbalanced (for example, when positive cases are rare).

---

### ğŸ“Š Performance Tiers: What Do the Scores Mean?

We group models into performance tiers to help you quickly understand how ready they are for different uses:

| Score Range | Rating | Interpretation | Suitable For |
|:---:|:---:|:---|:---|
| **â‰¥ 0.90** | â­ Excellent | Top-tier performance, consistently reliable | Clinical pilots with physician oversight |
| **0.80 â€“ 0.89** | âœ… Good | Strong performance, shows real promise | Validation studies, controlled testing |
| **0.70 â€“ 0.79** | ğŸ”¶ Fair | Moderate performance, has limitations | Research and development only |
| **< 0.70** | ğŸ“ˆ Developing | Below typical benchmarks, needs improvement | Early research, not for clinical use |

!!! tip "Important Context"
    These thresholds are general guidelines. The acceptable score depends on the specific clinical application, risk level, and whether the AI assists or replaces human judgment. Always consult domain experts when evaluating fitness for a particular use case.

---

### ğŸ“ How We Determine Rankings

Models are ranked following these principles:

1. **Primary metric determines rank** â€” The model with the highest score in the main metric ranks first. For metrics where lower is better (like error rates), the lowest score wins.

2. **Ties are broken by secondary metrics** â€” If two models have identical primary scores, we look at other relevant metrics to determine which performs better overall.

3. **Best run per model** â€” If a model was evaluated multiple times (e.g., with different settings), only its best result appears on the leaderboard. This ensures fair comparison.

4. **Reproducibility required** â€” All results must be reproducible. We record the evaluation date, dataset used, and configuration to ensure transparency.

---

### ğŸ¥ Why This Matters for Healthcare AI

Healthcare AI has higher stakes than many other AI applications. A model that works 95% of the time might sound good, but that 5% could mean missed diagnoses or incorrect treatments. That's why we:

- Use **multiple metrics** to capture different aspects of performance
- Test **robustness** to real-world data quality issues
- Require **transparency** about evaluation conditions
- Follow **international standards** for healthcare AI assessment

---

### ğŸŒ Standards Alignment

This benchmark follows the [ITU/WHO Focus Group on AI for Health (FG-AI4H)](https://www.itu.int/pub/T-FG-AI4H) framework, which provides internationally recognized guidelines for evaluating healthcare AI systems. This ensures our evaluations are:

- **Rigorous** â€” Following established scientific methodology
- **Comparable** â€” Using standardized metrics across different models
- **Trustworthy** â€” Aligned with WHO/ITU recommendations for health AI

</details>

<hr />
<h4 id="cell-type-annotation">Cell Type Annotation</h4>
<p><em>Predicting cell types from single-cell RNA-seq data.</em></p>
<div align="center">

<div class="highlight"><pre><span></span><code>                    ğŸ†                    

              ğŸ¥‡   Evo 2                 
                 (0.925)                 
             â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—             
             â•‘               â•‘             
   ğŸ¥ˆ Geneformer   â•‘               â•‘   ğŸ¥‰   SWIFT      
      (0.910)      â•‘               â•‘      (0.895)      
  â•”â•â•â•â•â•â•â•â•â•â•â•â•               â•šâ•â•â•â•â•â•â•â•â•â•â•â•—  
  â•‘                                       â•‘  
â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•
</code></pre></div>

</div>

<p><strong>8 models ranked by <code>Accuracy</code>:</strong></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Rank</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Level</th>
<th style="text-align: left;">Details</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ğŸ¥‡</td>
<td style="text-align: left;"><strong>Evo 2</strong> ğŸ‘‘</td>
<td style="text-align: center;">0.9250</td>
<td style="text-align: center;">â­ Excellent</td>
<td style="text-align: left;">PBMC 68k, 2024-02-01</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥ˆ</td>
<td style="text-align: left;"><strong>Geneformer</strong></td>
<td style="text-align: center;">0.9100</td>
<td style="text-align: center;">â­ Excellent</td>
<td style="text-align: left;">PBMC 3k (processed, , 2023-11-01</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥‰</td>
<td style="text-align: left;"><strong>SWIFT</strong></td>
<td style="text-align: center;">0.8950</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">PBMC 68k, 2024-01-15</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ…</td>
<td style="text-align: left;">Caduceus</td>
<td style="text-align: center;">0.8850</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">PBMC 68k, 2024-01-12</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ…</td>
<td style="text-align: left;">HyenaDNA</td>
<td style="text-align: center;">0.8700</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">PBMC 68k, 2024-01-08</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ–ï¸</td>
<td style="text-align: left;">DNABERT-2</td>
<td style="text-align: center;">0.8500</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">PBMC 68k, 2024-01-05</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ–ï¸</td>
<td style="text-align: left;">geneformer</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">ğŸ“ˆ Developing</td>
<td style="text-align: left;">PBMC 3k (processed, , 2025-12-18</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ–ï¸</td>
<td style="text-align: left;">Baseline (Random/Majority)</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">ğŸ“ˆ Developing</td>
<td style="text-align: left;">PBMC 3k (processed, , 2025-12-18</td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Quick Comparison</p>
<p><strong>ğŸ¥‡ Evo 2</strong> leads with Accuracy = <strong>0.9250</strong></p>
<ul>
<li>Gap to ğŸ¥ˆ Geneformer: +0.0150</li>
<li>Score spread (best to worst): 0.9250</li>
</ul>
</div>
<details>
<summary>ğŸ“ <strong>How are scores calculated?</strong> (click to expand)</summary>

---

### ğŸ“– Understanding This Leaderboard

This section explains how we measure and compare AI models. Don't worry if you're new to AI metrics â€” we'll break it down step by step.

---

### ğŸ¯ The Main Metric: `Accuracy`

**Accuracy**

**In simple terms:** The percentage of predictions the model got right

**How it works:** This is the most intuitive metric: out of all the predictions the model made, how many were correct? For example, if a model makes 100 predictions and 90 are correct, the accuracy is 90% (or 0.90). While easy to understand, accuracy can be misleading when classes are imbalanced (e.g., if 95% of cases are healthy, a model that always predicts 'healthy' would have 95% accuracy but miss all diseases).

**Score range:** 0.0 (all wrong) â†’ 1.0 (all correct)

ğŸ’¡ **Example:** An accuracy of 0.92 means the model correctly classified 92 out of every 100 samples.

---

### ğŸ§  How This Metric Fits This Task

Different tasks emphasize different aspects of performance. Here's how this metric should be interpreted for this benchmark:

- For **classification** tasks (e.g., disease vs. no disease), this metric helps you understand
  how reliably the model separates different outcome groups. In addition to raw accuracy,
  we recommend also looking at metrics like AUROC and F1 Score, especially when classes are
  imbalanced (for example, when positive cases are rare).

---

### ğŸ“Š Performance Tiers: What Do the Scores Mean?

We group models into performance tiers to help you quickly understand how ready they are for different uses:

| Score Range | Rating | Interpretation | Suitable For |
|:---:|:---:|:---|:---|
| **â‰¥ 0.90** | â­ Excellent | Top-tier performance, consistently reliable | Clinical pilots with physician oversight |
| **0.80 â€“ 0.89** | âœ… Good | Strong performance, shows real promise | Validation studies, controlled testing |
| **0.70 â€“ 0.79** | ğŸ”¶ Fair | Moderate performance, has limitations | Research and development only |
| **< 0.70** | ğŸ“ˆ Developing | Below typical benchmarks, needs improvement | Early research, not for clinical use |

!!! tip "Important Context"
    These thresholds are general guidelines. The acceptable score depends on the specific clinical application, risk level, and whether the AI assists or replaces human judgment. Always consult domain experts when evaluating fitness for a particular use case.

---

### ğŸ“ How We Determine Rankings

Models are ranked following these principles:

1. **Primary metric determines rank** â€” The model with the highest score in the main metric ranks first. For metrics where lower is better (like error rates), the lowest score wins.

2. **Ties are broken by secondary metrics** â€” If two models have identical primary scores, we look at other relevant metrics to determine which performs better overall.

3. **Best run per model** â€” If a model was evaluated multiple times (e.g., with different settings), only its best result appears on the leaderboard. This ensures fair comparison.

4. **Reproducibility required** â€” All results must be reproducible. We record the evaluation date, dataset used, and configuration to ensure transparency.

---

### ğŸ¥ Why This Matters for Healthcare AI

Healthcare AI has higher stakes than many other AI applications. A model that works 95% of the time might sound good, but that 5% could mean missed diagnoses or incorrect treatments. That's why we:

- Use **multiple metrics** to capture different aspects of performance
- Test **robustness** to real-world data quality issues
- Require **transparency** about evaluation conditions
- Follow **international standards** for healthcare AI assessment

---

### ğŸŒ Standards Alignment

This benchmark follows the [ITU/WHO Focus Group on AI for Health (FG-AI4H)](https://www.itu.int/pub/T-FG-AI4H) framework, which provides internationally recognized guidelines for evaluating healthcare AI systems. This ensures our evaluations are:

- **Rigorous** â€” Following established scientific methodology
- **Comparable** â€” Using standardized metrics across different models
- **Trustworthy** â€” Aligned with WHO/ITU recommendations for health AI

</details>

<hr />
<h3 id="generation">âœï¸ Generation</h3>
<h4 id="clinical-report-generation-quality">Clinical Report Generation Quality</h4>
<div align="center">

<div class="highlight"><pre><span></span><code>                    ğŸ†                    

              ğŸ¥‡   Me-LLaMA                
                 (0.875)                 
             â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—             
             â•‘               â•‘             
   ğŸ¥ˆ     M3FM       â•‘               â•‘   ğŸ¥‰ OpenFlamingo   
      (0.860)      â•‘               â•‘      (0.840)      
  â•”â•â•â•â•â•â•â•â•â•â•â•â•               â•šâ•â•â•â•â•â•â•â•â•â•â•â•—  
  â•‘                                       â•‘  
â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•
</code></pre></div>

</div>

<p><strong>6 models ranked by <code>report_quality_score</code>:</strong></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Rank</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Level</th>
<th style="text-align: left;">Details</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ğŸ¥‡</td>
<td style="text-align: left;"><strong>Me-LLaMA</strong> ğŸ‘‘</td>
<td style="text-align: center;">0.8750</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">mimic_cxr_reports, 2024-02-05</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥ˆ</td>
<td style="text-align: left;"><strong>M3FM</strong></td>
<td style="text-align: center;">0.8600</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">mimic_cxr_reports, 2024-01-28</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥‰</td>
<td style="text-align: left;"><strong>OpenFlamingo</strong></td>
<td style="text-align: center;">0.8400</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">mimic_cxr_reports, 2024-01-20</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ…</td>
<td style="text-align: left;">TITAN</td>
<td style="text-align: center;">0.8100</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">mimic_cxr_reports, 2024-01-25</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ…</td>
<td style="text-align: left;">Med-Flamingo</td>
<td style="text-align: center;">0.7800</td>
<td style="text-align: center;">ğŸ”¶ Fair</td>
<td style="text-align: left;">mimic_cxr_reports, 2024-01-18</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ–ï¸</td>
<td style="text-align: left;">RadBERT</td>
<td style="text-align: center;">0.6900</td>
<td style="text-align: center;">ğŸ“ˆ Developing</td>
<td style="text-align: left;">mimic_cxr_reports, 2024-01-12</td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Quick Comparison</p>
<p><strong>ğŸ¥‡ Me-LLaMA</strong> leads with report_quality_score = <strong>0.8750</strong></p>
<ul>
<li>Gap to ğŸ¥ˆ M3FM: +0.0150</li>
<li>Score spread (best to worst): 0.1850</li>
</ul>
</div>
<details>
<summary>ğŸ“ <strong>How are scores calculated?</strong> (click to expand)</summary>

---

### ğŸ“– Understanding This Leaderboard

This section explains how we measure and compare AI models. Don't worry if you're new to AI metrics â€” we'll break it down step by step.

---

### ğŸ¯ The Main Metric: `report_quality_score`

**Report Quality Score**

**In simple terms:** An overall measure of how good the AI-generated medical reports are

**How it works:** This composite score combines multiple aspects of report quality: clinical accuracy (are the findings correct?), completeness (are important findings mentioned?), language quality (is it well-written?), and safety (no harmful content). It provides a single number to compare models, though looking at individual components gives more insight into specific strengths and weaknesses.

**Score range:** 0.0 (poor quality) â†’ 1.0 (excellent quality)

ğŸ’¡ **Example:** A score of 0.85 indicates the model generates reports that are mostly accurate, complete, and well-structured.

---

### ğŸ§  How This Metric Fits This Task

Different tasks emphasize different aspects of performance. Here's how this metric should be interpreted for this benchmark:

- For **report generation**, we care not only about language quality but also clinical safety.
  This metric is usually combined with others (e.g., clinical accuracy, hallucination rate,
  and completeness of findings) to judge whether the generated report is both readable **and**
  medically reliable.

---

### ğŸ“Š Performance Tiers: What Do the Scores Mean?

We group models into performance tiers to help you quickly understand how ready they are for different uses:

| Score Range | Rating | Interpretation | Suitable For |
|:---:|:---:|:---|:---|
| **â‰¥ 0.90** | â­ Excellent | Top-tier performance, consistently reliable | Clinical pilots with physician oversight |
| **0.80 â€“ 0.89** | âœ… Good | Strong performance, shows real promise | Validation studies, controlled testing |
| **0.70 â€“ 0.79** | ğŸ”¶ Fair | Moderate performance, has limitations | Research and development only |
| **< 0.70** | ğŸ“ˆ Developing | Below typical benchmarks, needs improvement | Early research, not for clinical use |

!!! tip "Important Context"
    These thresholds are general guidelines. The acceptable score depends on the specific clinical application, risk level, and whether the AI assists or replaces human judgment. Always consult domain experts when evaluating fitness for a particular use case.

---

### ğŸ“ How We Determine Rankings

Models are ranked following these principles:

1. **Primary metric determines rank** â€” The model with the highest score in the main metric ranks first. For metrics where lower is better (like error rates), the lowest score wins.

2. **Ties are broken by secondary metrics** â€” If two models have identical primary scores, we look at other relevant metrics to determine which performs better overall.

3. **Best run per model** â€” If a model was evaluated multiple times (e.g., with different settings), only its best result appears on the leaderboard. This ensures fair comparison.

4. **Reproducibility required** â€” All results must be reproducible. We record the evaluation date, dataset used, and configuration to ensure transparency.

---

### ğŸ¥ Why This Matters for Healthcare AI

Healthcare AI has higher stakes than many other AI applications. A model that works 95% of the time might sound good, but that 5% could mean missed diagnoses or incorrect treatments. That's why we:

- Use **multiple metrics** to capture different aspects of performance
- Test **robustness** to real-world data quality issues
- Require **transparency** about evaluation conditions
- Follow **international standards** for healthcare AI assessment

---

### ğŸŒ Standards Alignment

This benchmark follows the [ITU/WHO Focus Group on AI for Health (FG-AI4H)](https://www.itu.int/pub/T-FG-AI4H) framework, which provides internationally recognized guidelines for evaluating healthcare AI systems. This ensures our evaluations are:

- **Rigorous** â€” Following established scientific methodology
- **Comparable** â€” Using standardized metrics across different models
- **Trustworthy** â€” Aligned with WHO/ITU recommendations for health AI

</details>

<hr />
<h2 id="brain-imaging-mrifmri">ğŸ§  Brain Imaging (MRI/fMRI)</h2>
<h3 id="classification_1">ğŸ¯ Classification</h3>
<h4 id="toy-classification-benchmark">Toy Classification Benchmark</h4>
<p><em>A toy benchmark for testing the pipeline.</em></p>
<p><strong>2 models ranked by <code>AUROC</code>:</strong></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Rank</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Level</th>
<th style="text-align: left;">Details</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ğŸ¥‡</td>
<td style="text-align: left;"><strong>Baseline (Random/Majority)</strong> ğŸ‘‘</td>
<td style="text-align: center;">0.5597</td>
<td style="text-align: center;">ğŸ“ˆ Developing</td>
<td style="text-align: left;">Toy fMRI Classificat, 2025-11-27</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥ˆ</td>
<td style="text-align: left;"><strong>BrainLM</strong></td>
<td style="text-align: center;">0.5193</td>
<td style="text-align: center;">ğŸ“ˆ Developing</td>
<td style="text-align: left;">Toy fMRI Classificat, 2025-11-27</td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Quick Comparison</p>
<p><strong>ğŸ¥‡ Baseline (Random/Majority)</strong> leads with AUROC = <strong>0.5597</strong></p>
<ul>
<li>Gap to ğŸ¥ˆ BrainLM: +0.0404</li>
</ul>
</div>
<details>
<summary>ğŸ“ <strong>How are scores calculated?</strong> (click to expand)</summary>

---

### ğŸ“– Understanding This Leaderboard

This section explains how we measure and compare AI models. Don't worry if you're new to AI metrics â€” we'll break it down step by step.

---

### ğŸ¯ The Main Metric: `AUROC`

**Area Under ROC Curve (AUROC)**

**In simple terms:** Measures how well the model can tell apart different categories (e.g., healthy vs. diseased)

**How it works:** Think of it like this: if you randomly pick one positive case and one negative case, AUROC tells you the probability that the model correctly identifies which is which. A score of 0.5 means the model is just guessing randomly (like flipping a coin), while 1.0 means it perfectly separates all cases.

**Score range:** 0.5 (random guessing) â†’ 1.0 (perfect separation)

ğŸ’¡ **Example:** An AUROC of 0.85 means the model correctly ranks a positive case higher than a negative case 85% of the time.

---

### ğŸ§  How This Metric Fits This Task

Different tasks emphasize different aspects of performance. Here's how this metric should be interpreted for this benchmark:

- For **classification** tasks (e.g., disease vs. no disease), this metric helps you understand
  how reliably the model separates different outcome groups. In addition to raw accuracy,
  we recommend also looking at metrics like AUROC and F1 Score, especially when classes are
  imbalanced (for example, when positive cases are rare).

---

### ğŸ“Š Performance Tiers: What Do the Scores Mean?

We group models into performance tiers to help you quickly understand how ready they are for different uses:

| Score Range | Rating | Interpretation | Suitable For |
|:---:|:---:|:---|:---|
| **â‰¥ 0.90** | â­ Excellent | Top-tier performance, consistently reliable | Clinical pilots with physician oversight |
| **0.80 â€“ 0.89** | âœ… Good | Strong performance, shows real promise | Validation studies, controlled testing |
| **0.70 â€“ 0.79** | ğŸ”¶ Fair | Moderate performance, has limitations | Research and development only |
| **< 0.70** | ğŸ“ˆ Developing | Below typical benchmarks, needs improvement | Early research, not for clinical use |

!!! tip "Important Context"
    These thresholds are general guidelines. The acceptable score depends on the specific clinical application, risk level, and whether the AI assists or replaces human judgment. Always consult domain experts when evaluating fitness for a particular use case.

---

### ğŸ“ How We Determine Rankings

Models are ranked following these principles:

1. **Primary metric determines rank** â€” The model with the highest score in the main metric ranks first. For metrics where lower is better (like error rates), the lowest score wins.

2. **Ties are broken by secondary metrics** â€” If two models have identical primary scores, we look at other relevant metrics to determine which performs better overall.

3. **Best run per model** â€” If a model was evaluated multiple times (e.g., with different settings), only its best result appears on the leaderboard. This ensures fair comparison.

4. **Reproducibility required** â€” All results must be reproducible. We record the evaluation date, dataset used, and configuration to ensure transparency.

---

### ğŸ¥ Why This Matters for Healthcare AI

Healthcare AI has higher stakes than many other AI applications. A model that works 95% of the time might sound good, but that 5% could mean missed diagnoses or incorrect treatments. That's why we:

- Use **multiple metrics** to capture different aspects of performance
- Test **robustness** to real-world data quality issues
- Require **transparency** about evaluation conditions
- Follow **international standards** for healthcare AI assessment

---

### ğŸŒ Standards Alignment

This benchmark follows the [ITU/WHO Focus Group on AI for Health (FG-AI4H)](https://www.itu.int/pub/T-FG-AI4H) framework, which provides internationally recognized guidelines for evaluating healthcare AI systems. This ensures our evaluations are:

- **Rigorous** â€” Following established scientific methodology
- **Comparable** â€” Using standardized metrics across different models
- **Trustworthy** â€” Aligned with WHO/ITU recommendations for health AI

</details>

<hr />
<h4 id="alzheimers-disease-classification-using-brain-mri">Alzheimer's Disease Classification using Brain MRI</h4>
<p><em>Binary classification of AD vs CN using structural MRI data.</em></p>
<div align="center">

<div class="highlight"><pre><span></span><code>                    ğŸ†                    

              ğŸ¥‡ Brain-JEPA              
                 (0.935)                 
             â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—             
             â•‘               â•‘             
   ğŸ¥ˆ    UNI       â•‘               â•‘   ğŸ¥‰  BrainLM     
      (0.920)      â•‘               â•‘      (0.910)      
  â•”â•â•â•â•â•â•â•â•â•â•â•â•               â•šâ•â•â•â•â•â•â•â•â•â•â•â•—  
  â•‘                                       â•‘  
â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•
</code></pre></div>

</div>

<p><strong>3 models ranked by <code>AUROC</code>:</strong></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Rank</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Level</th>
<th style="text-align: left;">Details</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ğŸ¥‡</td>
<td style="text-align: left;"><strong>Brain-JEPA</strong> ğŸ‘‘</td>
<td style="text-align: center;">0.9350</td>
<td style="text-align: center;">â­ Excellent</td>
<td style="text-align: left;">ADNI, 2024-01-20</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥ˆ</td>
<td style="text-align: left;"><strong>UNI</strong></td>
<td style="text-align: center;">0.9200</td>
<td style="text-align: center;">â­ Excellent</td>
<td style="text-align: left;">Alzheimer's Disease , 2023-10-27</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥‰</td>
<td style="text-align: left;"><strong>BrainLM</strong></td>
<td style="text-align: center;">0.9100</td>
<td style="text-align: center;">â­ Excellent</td>
<td style="text-align: left;">ADNI, 2024-01-15</td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Quick Comparison</p>
<p><strong>ğŸ¥‡ Brain-JEPA</strong> leads with AUROC = <strong>0.9350</strong></p>
<ul>
<li>Gap to ğŸ¥ˆ UNI: +0.0150</li>
<li>Score spread (best to worst): 0.0250</li>
</ul>
</div>
<details>
<summary>ğŸ“ <strong>How are scores calculated?</strong> (click to expand)</summary>

---

### ğŸ“– Understanding This Leaderboard

This section explains how we measure and compare AI models. Don't worry if you're new to AI metrics â€” we'll break it down step by step.

---

### ğŸ¯ The Main Metric: `AUROC`

**Area Under ROC Curve (AUROC)**

**In simple terms:** Measures how well the model can tell apart different categories (e.g., healthy vs. diseased)

**How it works:** Think of it like this: if you randomly pick one positive case and one negative case, AUROC tells you the probability that the model correctly identifies which is which. A score of 0.5 means the model is just guessing randomly (like flipping a coin), while 1.0 means it perfectly separates all cases.

**Score range:** 0.5 (random guessing) â†’ 1.0 (perfect separation)

ğŸ’¡ **Example:** An AUROC of 0.85 means the model correctly ranks a positive case higher than a negative case 85% of the time.

---

### ğŸ§  How This Metric Fits This Task

Different tasks emphasize different aspects of performance. Here's how this metric should be interpreted for this benchmark:

- For **classification** tasks (e.g., disease vs. no disease), this metric helps you understand
  how reliably the model separates different outcome groups. In addition to raw accuracy,
  we recommend also looking at metrics like AUROC and F1 Score, especially when classes are
  imbalanced (for example, when positive cases are rare).

---

### ğŸ“Š Performance Tiers: What Do the Scores Mean?

We group models into performance tiers to help you quickly understand how ready they are for different uses:

| Score Range | Rating | Interpretation | Suitable For |
|:---:|:---:|:---|:---|
| **â‰¥ 0.90** | â­ Excellent | Top-tier performance, consistently reliable | Clinical pilots with physician oversight |
| **0.80 â€“ 0.89** | âœ… Good | Strong performance, shows real promise | Validation studies, controlled testing |
| **0.70 â€“ 0.79** | ğŸ”¶ Fair | Moderate performance, has limitations | Research and development only |
| **< 0.70** | ğŸ“ˆ Developing | Below typical benchmarks, needs improvement | Early research, not for clinical use |

!!! tip "Important Context"
    These thresholds are general guidelines. The acceptable score depends on the specific clinical application, risk level, and whether the AI assists or replaces human judgment. Always consult domain experts when evaluating fitness for a particular use case.

---

### ğŸ“ How We Determine Rankings

Models are ranked following these principles:

1. **Primary metric determines rank** â€” The model with the highest score in the main metric ranks first. For metrics where lower is better (like error rates), the lowest score wins.

2. **Ties are broken by secondary metrics** â€” If two models have identical primary scores, we look at other relevant metrics to determine which performs better overall.

3. **Best run per model** â€” If a model was evaluated multiple times (e.g., with different settings), only its best result appears on the leaderboard. This ensures fair comparison.

4. **Reproducibility required** â€” All results must be reproducible. We record the evaluation date, dataset used, and configuration to ensure transparency.

---

### ğŸ¥ Why This Matters for Healthcare AI

Healthcare AI has higher stakes than many other AI applications. A model that works 95% of the time might sound good, but that 5% could mean missed diagnoses or incorrect treatments. That's why we:

- Use **multiple metrics** to capture different aspects of performance
- Test **robustness** to real-world data quality issues
- Require **transparency** about evaluation conditions
- Follow **international standards** for healthcare AI assessment

---

### ğŸŒ Standards Alignment

This benchmark follows the [ITU/WHO Focus Group on AI for Health (FG-AI4H)](https://www.itu.int/pub/T-FG-AI4H) framework, which provides internationally recognized guidelines for evaluating healthcare AI systems. This ensures our evaluations are:

- **Rigorous** â€” Following established scientific methodology
- **Comparable** â€” Using standardized metrics across different models
- **Trustworthy** â€” Aligned with WHO/ITU recommendations for health AI

</details>

<hr />
<h3 id="classificationreconstruction">ğŸ“‹ Classification/Reconstruction</h3>
<h4 id="fmri-foundation-model-benchmark-granular">fMRI Foundation Model Benchmark (Granular)</h4>
<div align="center">

<div class="highlight"><pre><span></span><code>                    ğŸ†                    

              ğŸ¥‡ Brain-JEPA              
                 (0.925)                 
             â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—             
             â•‘               â•‘             
   ğŸ¥ˆ  BrainLM     â•‘               â•‘   ğŸ¥‰ BrainBERT    
      (0.910)      â•‘               â•‘      (0.870)      
  â•”â•â•â•â•â•â•â•â•â•â•â•â•               â•šâ•â•â•â•â•â•â•â•â•â•â•â•—  
  â•‘                                       â•‘  
â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•
</code></pre></div>

</div>

<p><strong>5 models ranked by <code>AUROC</code>:</strong></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Rank</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Level</th>
<th style="text-align: left;">Details</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ğŸ¥‡</td>
<td style="text-align: left;"><strong>Brain-JEPA</strong> ğŸ‘‘</td>
<td style="text-align: center;">0.9250</td>
<td style="text-align: center;">â­ Excellent</td>
<td style="text-align: left;">hcp_1200, 2024-01-22</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥ˆ</td>
<td style="text-align: left;"><strong>BrainLM</strong></td>
<td style="text-align: center;">0.9100</td>
<td style="text-align: center;">â­ Excellent</td>
<td style="text-align: left;">hcp_1200, 2024-01-15</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥‰</td>
<td style="text-align: left;"><strong>BrainBERT</strong></td>
<td style="text-align: center;">0.8700</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">hcp_1200, 2024-01-10</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ…</td>
<td style="text-align: left;">BrainMT</td>
<td style="text-align: center;">0.8500</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">hcp_1200, 2024-01-18</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ…</td>
<td style="text-align: left;">NeuroClips</td>
<td style="text-align: center;">0.8300</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">hcp_1200, 2024-01-05</td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Quick Comparison</p>
<p><strong>ğŸ¥‡ Brain-JEPA</strong> leads with AUROC = <strong>0.9250</strong></p>
<ul>
<li>Gap to ğŸ¥ˆ BrainLM: +0.0150</li>
<li>Score spread (best to worst): 0.0950</li>
</ul>
</div>
<details>
<summary>ğŸ“ <strong>How are scores calculated?</strong> (click to expand)</summary>

---

### ğŸ“– Understanding This Leaderboard

This section explains how we measure and compare AI models. Don't worry if you're new to AI metrics â€” we'll break it down step by step.

---

### ğŸ¯ The Main Metric: `AUROC`

**Area Under ROC Curve (AUROC)**

**In simple terms:** Measures how well the model can tell apart different categories (e.g., healthy vs. diseased)

**How it works:** Think of it like this: if you randomly pick one positive case and one negative case, AUROC tells you the probability that the model correctly identifies which is which. A score of 0.5 means the model is just guessing randomly (like flipping a coin), while 1.0 means it perfectly separates all cases.

**Score range:** 0.5 (random guessing) â†’ 1.0 (perfect separation)

ğŸ’¡ **Example:** An AUROC of 0.85 means the model correctly ranks a positive case higher than a negative case 85% of the time.

---

### ğŸ§  How This Metric Fits This Task

Different tasks emphasize different aspects of performance. Here's how this metric should be interpreted for this benchmark:

- For **classification** tasks (e.g., disease vs. no disease), this metric helps you understand
  how reliably the model separates different outcome groups. In addition to raw accuracy,
  we recommend also looking at metrics like AUROC and F1 Score, especially when classes are
  imbalanced (for example, when positive cases are rare).

---

### ğŸ“Š Performance Tiers: What Do the Scores Mean?

We group models into performance tiers to help you quickly understand how ready they are for different uses:

| Score Range | Rating | Interpretation | Suitable For |
|:---:|:---:|:---|:---|
| **â‰¥ 0.90** | â­ Excellent | Top-tier performance, consistently reliable | Clinical pilots with physician oversight |
| **0.80 â€“ 0.89** | âœ… Good | Strong performance, shows real promise | Validation studies, controlled testing |
| **0.70 â€“ 0.79** | ğŸ”¶ Fair | Moderate performance, has limitations | Research and development only |
| **< 0.70** | ğŸ“ˆ Developing | Below typical benchmarks, needs improvement | Early research, not for clinical use |

!!! tip "Important Context"
    These thresholds are general guidelines. The acceptable score depends on the specific clinical application, risk level, and whether the AI assists or replaces human judgment. Always consult domain experts when evaluating fitness for a particular use case.

---

### ğŸ“ How We Determine Rankings

Models are ranked following these principles:

1. **Primary metric determines rank** â€” The model with the highest score in the main metric ranks first. For metrics where lower is better (like error rates), the lowest score wins.

2. **Ties are broken by secondary metrics** â€” If two models have identical primary scores, we look at other relevant metrics to determine which performs better overall.

3. **Best run per model** â€” If a model was evaluated multiple times (e.g., with different settings), only its best result appears on the leaderboard. This ensures fair comparison.

4. **Reproducibility required** â€” All results must be reproducible. We record the evaluation date, dataset used, and configuration to ensure transparency.

---

### ğŸ¥ Why This Matters for Healthcare AI

Healthcare AI has higher stakes than many other AI applications. A model that works 95% of the time might sound good, but that 5% could mean missed diagnoses or incorrect treatments. That's why we:

- Use **multiple metrics** to capture different aspects of performance
- Test **robustness** to real-world data quality issues
- Require **transparency** about evaluation conditions
- Follow **international standards** for healthcare AI assessment

---

### ğŸŒ Standards Alignment

This benchmark follows the [ITU/WHO Focus Group on AI for Health (FG-AI4H)](https://www.itu.int/pub/T-FG-AI4H) framework, which provides internationally recognized guidelines for evaluating healthcare AI systems. This ensures our evaluations are:

- **Rigorous** â€” Following established scientific methodology
- **Comparable** â€” Using standardized metrics across different models
- **Trustworthy** â€” Aligned with WHO/ITU recommendations for health AI

</details>

<hr />
<h3 id="reconstruction">ğŸ”„ Reconstruction</h3>
<h4 id="brain-time-series-modeling">Brain Time-Series Modeling</h4>
<p><em>Evaluating ability to reconstruct masked fMRI voxel time-series.</em></p>
<p><strong>1 models ranked by <code>Correlation</code>:</strong></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Rank</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Level</th>
<th style="text-align: left;">Details</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ğŸ¥‡</td>
<td style="text-align: left;"><strong>BrainLM</strong> ğŸ‘‘</td>
<td style="text-align: center;">0.7800</td>
<td style="text-align: center;">ğŸ”¶ Fair</td>
<td style="text-align: left;">UK Biobank fMRI tens, 2025-11-15</td>
</tr>
</tbody>
</table>
<details>
<summary>ğŸ“ <strong>How are scores calculated?</strong> (click to expand)</summary>

---

### ğŸ“– Understanding This Leaderboard

This section explains how we measure and compare AI models. Don't worry if you're new to AI metrics â€” we'll break it down step by step.

---

### ğŸ¯ The Main Metric: `Correlation`

**Correlation**

**In simple terms:** How closely the model's predictions match the actual values

**How it works:** Correlation measures the strength and direction of the relationship between predicted and actual values. A correlation of 1.0 means perfect positive agreement (when actual goes up, prediction goes up proportionally), while 0 means no relationship at all. This is commonly used for reconstruction tasks where we want to see how well the model can recreate the original signal.

**Score range:** -1.0 (perfect inverse) â†’ 0 (no relationship) â†’ 1.0 (perfect match)

ğŸ’¡ **Example:** A correlation of 0.78 means the model's outputs track reasonably well with the true values.

---

### ğŸ§  How This Metric Fits This Task

Different tasks emphasize different aspects of performance. Here's how this metric should be interpreted for this benchmark:

- For **regression / continuous prediction** tasks, this metric captures how closely the model's
  predicted values track the true values over a range (e.g., symptom severity, signal amplitude).
  We are usually interested in both overall fit (correlation) and error magnitude.

---

### ğŸ“Š Performance Tiers: What Do the Scores Mean?

We group models into performance tiers to help you quickly understand how ready they are for different uses:

| Score Range | Rating | Interpretation | Suitable For |
|:---:|:---:|:---|:---|
| **â‰¥ 0.90** | â­ Excellent | Top-tier performance, consistently reliable | Clinical pilots with physician oversight |
| **0.80 â€“ 0.89** | âœ… Good | Strong performance, shows real promise | Validation studies, controlled testing |
| **0.70 â€“ 0.79** | ğŸ”¶ Fair | Moderate performance, has limitations | Research and development only |
| **< 0.70** | ğŸ“ˆ Developing | Below typical benchmarks, needs improvement | Early research, not for clinical use |

!!! tip "Important Context"
    These thresholds are general guidelines. The acceptable score depends on the specific clinical application, risk level, and whether the AI assists or replaces human judgment. Always consult domain experts when evaluating fitness for a particular use case.

---

### ğŸ“ How We Determine Rankings

Models are ranked following these principles:

1. **Primary metric determines rank** â€” The model with the highest score in the main metric ranks first. For metrics where lower is better (like error rates), the lowest score wins.

2. **Ties are broken by secondary metrics** â€” If two models have identical primary scores, we look at other relevant metrics to determine which performs better overall.

3. **Best run per model** â€” If a model was evaluated multiple times (e.g., with different settings), only its best result appears on the leaderboard. This ensures fair comparison.

4. **Reproducibility required** â€” All results must be reproducible. We record the evaluation date, dataset used, and configuration to ensure transparency.

---

### ğŸ¥ Why This Matters for Healthcare AI

Healthcare AI has higher stakes than many other AI applications. A model that works 95% of the time might sound good, but that 5% could mean missed diagnoses or incorrect treatments. That's why we:

- Use **multiple metrics** to capture different aspects of performance
- Test **robustness** to real-world data quality issues
- Require **transparency** about evaluation conditions
- Follow **international standards** for healthcare AI assessment

---

### ğŸŒ Standards Alignment

This benchmark follows the [ITU/WHO Focus Group on AI for Health (FG-AI4H)](https://www.itu.int/pub/T-FG-AI4H) framework, which provides internationally recognized guidelines for evaluating healthcare AI systems. This ensures our evaluations are:

- **Rigorous** â€” Following established scientific methodology
- **Comparable** â€” Using standardized metrics across different models
- **Trustworthy** â€” Aligned with WHO/ITU recommendations for health AI

</details>

<hr />
<h2 id="other-benchmarks">ğŸ“‹ Other Benchmarks</h2>
<h3 id="foundation-model-robustness-evaluation">Foundation Model Robustness Evaluation</h3>
<table>
<thead>
<tr>
<th style="text-align: center;">Rank</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Level</th>
<th style="text-align: left;">Details</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ğŸ¥‡</td>
<td style="text-align: left;"><strong>geneformer</strong> ğŸ‘‘</td>
<td style="text-align: center;">0.9995</td>
<td style="text-align: center;">â­ Excellent</td>
<td style="text-align: left;">neuro/robustness, 2025-11-27</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥ˆ</td>
<td style="text-align: left;"><strong>Brain-JEPA</strong></td>
<td style="text-align: center;">0.8650</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">DS-TOY-NEURO-ROBUSTN, 2024-01-20</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ¥‰</td>
<td style="text-align: left;"><strong>BrainHarmony</strong></td>
<td style="text-align: center;">0.8450</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">DS-TOY-NEURO-ROBUSTN, 2024-01-18</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ…</td>
<td style="text-align: left;">Geneformer</td>
<td style="text-align: center;">0.8350</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">DS-TOY-GENOMICS, 2024-01-10</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ…</td>
<td style="text-align: left;">BrainLM</td>
<td style="text-align: center;">0.8250</td>
<td style="text-align: center;">âœ… Good</td>
<td style="text-align: left;">DS-TOY-NEURO-ROBUSTN, 2024-01-16</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ–ï¸</td>
<td style="text-align: left;">HyenaDNA</td>
<td style="text-align: center;">0.7950</td>
<td style="text-align: center;">ğŸ”¶ Fair</td>
<td style="text-align: left;">DS-TOY-GENOMICS, 2024-01-12</td>
</tr>
<tr>
<td style="text-align: center;">ğŸ–ï¸</td>
<td style="text-align: left;">Baseline (Random/Majority)</td>
<td style="text-align: center;">0.7810</td>
<td style="text-align: center;">ğŸ”¶ Fair</td>
<td style="text-align: left;">neuro/robustness, 2025-11-27</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="add-your-model">ğŸš€ Add Your Model</h2>
<p>Want your model on this leaderboard?</p>
<ol>
<li><strong>Download</strong> the benchmark toolkit</li>
<li><strong>Run locally</strong> on your model (your code stays private!)</li>
<li><strong>Submit results</strong> via <a href="https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub/issues/new?template=benchmark_submission.md">GitHub Issue</a></li>
</ol>
<p><a class="md-button md-button--primary" href="../">ğŸ“¥ Get Started</a>
<a class="md-button" href="../contributing/submission_guide/">ğŸ“– Submission Guide</a></p>
<hr />
<p><em>Aligned with <a href="https://www.itu.int/pub/T-FG-AI4H">ITU/WHO FG-AI4H</a> standards for healthcare AI evaluation.</em></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.top", "navigation.tracking", "navigation.instant", "navigation.expand", "toc.integrate", "content.code.copy", "search.suggest", "search.highlight"], "search": "../assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.e71a0d61.min.js"></script>
      
    
  </body>
</html>