# Flamingo-Med Report Generation Evaluation
# Demonstrates report quality metrics

eval_id: REPORT-GEN-flamingo-20240120
benchmark_id: BM-REPORT-GEN

model_ids:
  candidate: flamingo

dataset_id: mimic_cxr_reports

run_metadata:
  date: "2024-01-20"
  runner: "fmbench"
  hardware: "4x NVIDIA A100"
  notes: "Chest X-ray report generation evaluation"

metrics:
  # Linguistic quality
  bleu: 38.5
  rouge_l: 0.62
  bertscore: 0.87
  meteor: 0.54
  
  # Clinical quality
  clinical_accuracy: 0.89
  finding_recall: 0.85
  finding_precision: 0.91
  hallucination_rate: 0.06
  omission_rate: 0.09
  
  # Safety
  harmful_content: 0.001
  uncertainty_calibration: 0.82
  
  # Readability
  flesch_kincaid: 10.2
  structure_score: 0.88
  
  # Aggregate
  report_quality_score: 0.84
  
  # Stratified by report type
  stratified:
    report_type:
      chest_xray:
        clinical_accuracy: 0.91
        finding_recall: 0.87
        bertscore: 0.88
        N: 2000
      ct_abdomen:
        clinical_accuracy: 0.86
        finding_recall: 0.82
        bertscore: 0.85
        N: 800
      brain_mri:
        clinical_accuracy: 0.88
        finding_recall: 0.84
        bertscore: 0.86
        N: 600
    
    complexity:
      simple:
        clinical_accuracy: 0.94
        hallucination_rate: 0.03
        N: 1500
      moderate:
        clinical_accuracy: 0.88
        hallucination_rate: 0.06
        N: 1200
      complex:
        clinical_accuracy: 0.82
        hallucination_rate: 0.10
        N: 700
    
    finding_count:
      "1-2_findings":
        clinical_accuracy: 0.93
        N: 1400
      "3-5_findings":
        clinical_accuracy: 0.87
        N: 1100
      "6+_findings":
        clinical_accuracy: 0.79
        N: 900

status: Completed

