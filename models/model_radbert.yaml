model_id: radbert
name: "RadBERT"
version: "1.0.0"
modality: "radiology_text"
upstream_repo: "https://huggingface.co/zzxslp/RadBERT-RoBERTa-4m"
code_repo: "https://github.com/zzxslp/RadBERT"
local_path: "external/radbert"
paper: "https://pubs.rsna.org/doi/10.1148/ryai.210258"
notes: >
  RoBERTa model pretrained on 4 million deidentified radiology reports from US VA hospitals.
  Outperforms BioBERT, Clinical-BERT, BLUE-BERT on radiology NLP tasks including 
  abnormal sentence classification, report coding, and report summarization.
arch: "RoBERTa-base"
params: "~125M"
license: "Apache 2.0"
