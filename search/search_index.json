{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AI4H-Inspired Foundation Model Benchmarks","text":"<p>Standardized Evaluation for Neurogenomic Foundation Models</p> <p>Welcome to the AI4H-inspired benchmarking hub for Genetics and Brain Imaging foundation models. Our framework is directly inspired by the principles and deliverables of the ITU/WHO Focus Group on AI for Health (FG-AI4H), specifically aiming for rigorous, transparent, and clinically relevant evaluation standards.</p>"},{"location":"#mission","title":"\ud83c\udfaf Mission","text":"<p>This repository hosts an open benchmarking hub that provides:</p> <ul> <li>Standardized evaluation protocols for foundation models in healthcare AI</li> <li>Transparent leaderboards with task-specific and granular performance metrics</li> <li>Robustness testing to assess model resilience to real-world perturbations</li> <li>Downloadable evaluation tools for local testing and validation</li> </ul>"},{"location":"#domain-focus","title":"\ud83e\uddec\ud83e\udde0 Domain Focus","text":""},{"location":"#genomics","title":"Genomics","text":"<ul> <li>Single-cell analysis and cell type annotation</li> <li>Variant interpretation</li> <li>Gene expression modeling</li> <li>Sequence classification and generation</li> </ul>"},{"location":"#neurology","title":"Neurology","text":"<ul> <li>Brain MRI/fMRI/EEG analysis</li> <li>Neurodegenerative disease classification</li> <li>Functional connectivity reconstruction</li> <li>Multi-modal brain imaging</li> </ul>"},{"location":"#key-features","title":"\u2728 Key Features","text":""},{"location":"#1-interactive-evaluation-tool","title":"1. Interactive Evaluation Tool","text":"<p>We provide a downloadable evaluation suite that allows researchers to test their models against standardized baselines, following the AI4H System Requirement Specifications (DEL3).</p> <ul> <li>Toy Samples: Representative datasets for initial testing and debugging</li> <li>Local Evaluation: Run evaluations on your own infrastructure to ensure data privacy</li> <li>Automated Reporting: Detailed performance reports characterizing model capabilities</li> </ul>"},{"location":"#2-comprehensive-leaderboards","title":"2. Comprehensive Leaderboards","text":"<p>View our interactive leaderboards that rank existing open-source Foundation Models by:</p> <ul> <li>Task-Specific Rankings: Separate leaderboards for distinct tasks</li> <li>Granular Metrics: Performance breakdowns by data characteristics</li> <li>Resource Usage: Inference time and computational costs</li> <li>Clinical Relevance: Metrics aligned with real-world healthcare applications</li> </ul>"},{"location":"#robustness-testing","title":"3. Robustness Testing","text":"<p>Built-in robustness probes test foundation model resilience against realistic perturbations:</p> <ul> <li>Channel Dropout: Simulates sensor failures and missing data</li> <li>Gaussian Noise: Tests performance under various SNR conditions</li> <li>Line Interference: Evaluates robustness to powerline artifacts (50/60 Hz)</li> <li>Channel Permutation: Tests equivariance to electrode/channel ordering</li> <li>Temporal Shift: Measures sensitivity to timing jitter</li> </ul> <p>This produces rAUC (Reverse Area Under Curve) scores that quantify how stable model outputs remain as perturbations increase.</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub.git\ncd ai4h-inspired-fm-benchmark-hub\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"#generate-toy-data","title":"Generate Toy Data","text":"<pre><code># Generate synthetic datasets for testing (includes robustness test data)\npython -m fmbench generate-toy-data\n</code></pre> <p>This creates toy datasets in the <code>toy_data/</code> directory: - <code>toy_data/neuro/</code>: Synthetic fMRI-like classification data - <code>toy_data/genomics/</code>: Synthetic gene expression data - <code>toy_data/neuro/robustness/</code>: Data for robustness testing</p>"},{"location":"#run-your-first-benchmark","title":"Run Your First Benchmark","text":"<pre><code># Run a classification benchmark using the built-in dummy model\npython -m fmbench run \\\n    --suite SUITE-TOY-CLASS \\\n    --model configs/model_dummy_classifier.yaml \\\n    --out results/my_first_run\n\n# View the results\ncat results/my_first_run/report.md\n</code></pre>"},{"location":"#test-model-robustness","title":"Test Model Robustness","text":"<pre><code># Run robustness evaluation\npython -m fmbench run-robustness \\\n    --model configs/model_dummy_classifier.yaml \\\n    --data toy_data/neuro/robustness \\\n    --out results/robustness_eval \\\n    --probes dropout,noise,line_noise,permutation,shift\n\n# Check the robustness report\ncat results/robustness_eval/report.md\n</code></pre>"},{"location":"#build-the-leaderboard","title":"Build the Leaderboard","text":"<pre><code># Generate the leaderboard from all evaluation results\npython -m fmbench build-leaderboard\n\n# View the leaderboard\ncat docs/leaderboards/index.md\n</code></pre>"},{"location":"#available-benchmarks","title":"\ud83d\udcca Available Benchmarks","text":"<p>List all available benchmark suites:</p> <pre><code>python -m fmbench list-suites\n</code></pre> <p>List benchmark definitions:</p> <pre><code>python -m fmbench list-benchmarks\n</code></pre>"},{"location":"#alignment-with-ituwho-fg-ai4h-standards","title":"\ud83c\udf93 Alignment with ITU/WHO FG-AI4H Standards","text":"<p>This project explicitly references and adapts the following ITU-T Focus Group deliverables:</p> <ul> <li> <p>DEL3: AI4H Requirement Specifications: We adopt the System Requirement Specifications (SyRS) framework for defining functional, operational, and performance requirements.</p> </li> <li> <p>DEL0.1: Common Unified Terms: We utilize standard terminology (e.g., \"AI Solution\", \"Benchmarking Run\") to ensure consistency.</p> </li> <li> <p>DEL10.8: Topic Description Document for Neurology: Our Neurology benchmarks are structured according to the TDD template.</p> </li> </ul> <p>Learn more about our AI4H alignment approach.</p>"},{"location":"#documentation","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>Leaderboards: View current model rankings and performance metrics</li> <li>Models Overview: Catalog of evaluated foundation models</li> <li>Analysis Protocols: Standardized analysis recipes</li> <li>Data Specifications: Format requirements for different modalities</li> <li>AI4H Alignment: How we implement ITU/WHO standards</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Whether you want to:</p> <ul> <li>Add a new foundation model to the leaderboard</li> <li>Submit benchmark results</li> <li>Propose new evaluation protocols</li> <li>Improve documentation</li> </ul> <p>Please see our contribution guidelines and open an issue or pull request.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"#credits-attribution","title":"\ud83d\ude4f Credits &amp; Attribution","text":"<p>The methodology and framework of this benchmark suite are derived from the public deliverables of the ITU/WHO Focus Group on Artificial Intelligence for Health (FG-AI4H).</p> <ul> <li>DEL3: AI4H requirement specifications (03/2023) - PDF</li> <li>DEL0.1: Common unified terms in artificial intelligence for health (2022) - PDF</li> <li>DEL10.8: Topic Description Document for the Topic Group on AI for neurological disorders (TG-Neuro) (2023) - PDF</li> </ul> <p>\u00a9 ITU 2025. These publications are available under the Creative Commons Attribution-Non Commercial-Share Alike 3.0 IGO licence (CC BY-NC-SA 3.0 IGO).</p>"},{"location":"contributing/submission_guide/","title":"FM Benchmark Submission Guide","text":"<p>Welcome! This guide explains how to get your Foundation Model evaluated and added to our leaderboards.</p> <p>Curated Benchmark Hub</p> <p>This is a curated benchmark hub. All submissions are reviewed before being added to the leaderboards. We do not accept direct code contributions - only benchmark result submissions and protocol proposals.</p>"},{"location":"contributing/submission_guide/#submission-process","title":"Submission Process","text":"<pre><code>graph LR\n    A[Run Benchmarks Locally] --&gt; B[Open Issue with Results]\n    B --&gt; C[Review by Maintainer]\n    C --&gt; D[Added to Leaderboard]\n</code></pre>"},{"location":"contributing/submission_guide/#step-1-run-benchmarks-locally","title":"Step 1: Run Benchmarks Locally","text":"<pre><code># 1. Clone and install\ngit clone https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub.git\ncd ai4h-inspired-fm-benchmark-hub\npip install -e .\n\n# 2. Generate toy data for testing\npython -m fmbench generate-toy-data\n\n# 3. Run your model\npython -m fmbench run \\\n    --suite SUITE-TOY-CLASS \\\n    --model path/to/your_model_config.yaml \\\n    --out results/my_model_run\n</code></pre>"},{"location":"contributing/submission_guide/#step-2-submit-via-issue","title":"Step 2: Submit via Issue","text":"<p>Do NOT open a Pull Request. Instead:</p> <ol> <li>Go to New Issue \u2192 \ud83d\udcca Benchmark Submission</li> <li>Fill out the template with your results</li> <li>Wait for review</li> </ol>"},{"location":"contributing/submission_guide/#step-3-review-addition","title":"Step 3: Review &amp; Addition","text":"<p>We will review your submission and: - Verify the results format - Add your model to the leaderboard - Notify you when it's live</p>"},{"location":"contributing/submission_guide/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"contributing/submission_guide/#1-create-your-model-configuration","title":"1. Create Your Model Configuration","text":"<p>Create a YAML file in <code>configs/</code> describing your model:</p> <pre><code># configs/model_my_awesome_fm.yaml\nmodel_id: my_awesome_fm\nname: \"My Awesome Foundation Model\"\nversion: \"1.0.0\"\n\n# How to load the model\ntype: python_class\nimport_path: \"my_package.models:MyModel\"\ninit_kwargs:\n  checkpoint: \"path/to/checkpoint.pt\"\n\n# Metadata\nmodality: [\"fMRI\", \"EEG\"]  # or \"genomics\", \"MRI\"\npaper: \"https://arxiv.org/abs/xxxx.xxxxx\"\ncode: \"https://github.com/my-org/my-model\"\nlicense: \"MIT\"\n</code></pre>"},{"location":"contributing/submission_guide/#2-run-evaluation","title":"2. Run Evaluation","text":""},{"location":"contributing/submission_guide/#classification-task","title":"Classification Task","text":"<pre><code>python -m fmbench run \\\n    --suite SUITE-NEURO-CLASS-001 \\\n    --model configs/model_my_awesome_fm.yaml \\\n    --out results/my_fm_neuro\n</code></pre>"},{"location":"contributing/submission_guide/#robustness-evaluation","title":"Robustness Evaluation","text":"<pre><code>python -m fmbench run-robustness \\\n    --model configs/model_my_awesome_fm.yaml \\\n    --data toy_data/neuro/robustness \\\n    --out results/my_fm_robustness\n</code></pre>"},{"location":"contributing/submission_guide/#3-review-your-results","title":"3. Review Your Results","text":"<p>Check the generated files:</p> <pre><code>results/my_fm_neuro/\n\u251c\u2500\u2500 eval.yaml      # Evaluation record (submit this!)\n\u2514\u2500\u2500 report.md      # Human-readable report\n</code></pre>"},{"location":"contributing/submission_guide/#4-submit-via-issue","title":"4. Submit via Issue","text":"<ol> <li>Go to Issues \u2192 New Issue \u2192 \ud83d\udcca Benchmark Submission</li> <li>Paste your <code>eval.yaml</code> contents in the issue</li> <li>Attach any supporting files (model config, etc.)</li> <li>The maintainer will review and add your results to the leaderboard</li> </ol>"},{"location":"contributing/submission_guide/#evaluation-yaml-format","title":"Evaluation YAML Format","text":"<p>Your <code>evals/*.yaml</code> file should follow this format:</p> <pre><code># evals/SUITE-NEURO-CLASS-my_fm-20240101-120000.yaml\neval_id: SUITE-NEURO-CLASS-my_fm-20240101-120000\nbenchmark_id: BM-001  # Must match existing benchmark\n\nmodel_ids:\n  candidate: my_awesome_fm\n\ndataset_id: DS-ADNI  # Or your dataset\n\nrun_metadata:\n  date: \"2024-01-01\"\n  runner: \"fmbench\"\n  hardware: \"NVIDIA A100 80GB\"\n  runtime_seconds: 3600\n\n# Overall metrics\nmetrics:\n  AUROC: 0.92\n  Accuracy: 0.88\n  F1-Score: 0.87\n\n  # Stratified metrics (for granular leaderboards)\n  stratified:\n    scanner:\n      Siemens:\n        AUROC: 0.94\n        Accuracy: 0.90\n        N: 150\n      GE:\n        AUROC: 0.89\n        Accuracy: 0.85\n        N: 120\n      Philips:\n        AUROC: 0.91\n        Accuracy: 0.87\n        N: 100\n\n    acquisition_type:\n      resting_state:\n        AUROC: 0.93\n        Accuracy: 0.89\n        N: 200\n      task_based:\n        AUROC: 0.90\n        Accuracy: 0.86\n        N: 170\n\n    site:\n      Site_A:\n        AUROC: 0.92\n        N: 100\n      Site_B:\n        AUROC: 0.91\n        N: 120\n\nstatus: Completed\n</code></pre>"},{"location":"contributing/submission_guide/#stratification-categories","title":"Stratification Categories","text":"<p>To appear in granular sub-leaderboards, include these stratified metrics:</p>"},{"location":"contributing/submission_guide/#for-fmrineuro","title":"For fMRI/Neuro","text":"Category Values <code>scanner</code> Siemens, GE, Philips <code>acquisition_type</code> resting_state, task_based, naturalistic <code>field_strength</code> 1.5T, 3T, 7T <code>preprocessing</code> fmriprep, hcp, conn, minimal <code>site</code> Your site names"},{"location":"contributing/submission_guide/#for-genomics","title":"For Genomics","text":"Category Values <code>sequencing_platform</code> Illumina, PacBio, ONT <code>cell_type</code> T-cell, B-cell, Monocyte, etc. <code>tissue</code> Blood, Brain, Liver, etc."},{"location":"contributing/submission_guide/#for-all-benchmarks","title":"For All Benchmarks","text":"Category Values <code>sex</code> M, F <code>age_group</code> age_20-40, age_40-60, age_60-80, age_80-100 <code>disease_stage</code> CN, MCI, AD, etc. <code>ethnicity</code> Your categories"},{"location":"contributing/submission_guide/#report-quality-metrics-for-generation-tasks","title":"Report Quality Metrics (for Generation Tasks)","text":"<p>If your model generates reports/text, include these metrics:</p> <pre><code>metrics:\n  # Linguistic\n  bleu: 42.5\n  rouge_l: 0.68\n  bertscore: 0.89\n\n  # Clinical\n  clinical_accuracy: 0.92\n  finding_recall: 0.88\n  finding_precision: 0.91\n  hallucination_rate: 0.05  # Lower is better\n  omission_rate: 0.08       # Lower is better\n\n  # Aggregate\n  report_quality_score: 0.87\n</code></pre>"},{"location":"contributing/submission_guide/#robustness-metrics","title":"Robustness Metrics","text":"<p>For robustness evaluations, include:</p> <pre><code>metrics:\n  robustness_score: 0.78\n  dropout_rAUC: 0.82\n  noise_rAUC: 0.75\n  line_noise_rAUC: 0.80\n  perm_equivariance: 0.85\n  shift_sensitivity: 0.79\n\n  # For genomics\n  masking_rAUC: 0.77\n  expression_rAUC: 0.81\n</code></pre>"},{"location":"contributing/submission_guide/#submission-checklist","title":"Submission Checklist","text":"<p>Before submitting your issue, ensure:</p> <ul> <li>[ ] Model config YAML is valid</li> <li>[ ] Eval YAML has correct <code>benchmark_id</code></li> <li>[ ] Metrics include both overall and stratified results</li> <li>[ ] All required metadata is present</li> <li>[ ] Results are reproducible (include seed if applicable)</li> </ul>"},{"location":"contributing/submission_guide/#questions","title":"Questions?","text":"<p>Open an Issue with your question.</p> <p>Thank you for your interest in FM benchmarking! \ud83c\udf89</p>"},{"location":"design/ai4h_alignment/","title":"AI4H Alignment","text":""},{"location":"design/ai4h_alignment/#overview","title":"Overview","text":"<p>This benchmark hub is explicitly designed to align with the ITU/WHO Focus Group on Artificial Intelligence for Health (FG-AI4H) standards and deliverables. This page documents how our framework implements and extends these standards for foundation model evaluation.</p>"},{"location":"design/ai4h_alignment/#itu-fg-ai4h-background","title":"ITU FG-AI4H Background","text":"<p>The ITU/WHO Focus Group on AI for Health was established to develop international standards for AI in healthcare, focusing on:</p> <ul> <li>Safety: Ensuring AI systems are safe for clinical use</li> <li>Effectiveness: Establishing evidence-based validation methods</li> <li>Transparency: Promoting explainability and interpretability</li> <li>Ethics: Addressing fairness, bias, and patient rights</li> <li>Interoperability: Enabling cross-system compatibility</li> </ul>"},{"location":"design/ai4h_alignment/#key-deliverables-used","title":"Key Deliverables Used","text":""},{"location":"design/ai4h_alignment/#del01-common-unified-terms","title":"DEL0.1: Common Unified Terms","text":"<p>Reference: ITU-T FG-AI4H-DEL0.1 (2022)</p> <p>Purpose: Establish standardized terminology for AI4H systems</p> <p>Our Implementation:</p> AI4H Term Our Usage AI Solution Foundation models (e.g., BrainLM, Geneformer) Benchmarking Run Evaluation instance (<code>eval_id</code> in results) Reference Implementation Baseline models (logistic regression, random forest) Health Topic Domain area (e.g., \"Alzheimer's Disease\") AI Task ML task type (classification, reconstruction, regression) Test Dataset Standardized evaluation data (e.g., PBMC 68k, ADNI) <p>Example from our schema:</p> <pre><code># From benchmarks/bm_ad_classification.yaml\nbenchmark_id: AD-CLASSIFICATION\nhealth_topic: Alzheimer's Disease\nai_task: Classification\n</code></pre>"},{"location":"design/ai4h_alignment/#del3-ai4h-requirement-specifications","title":"DEL3: AI4H Requirement Specifications","text":"<p>Reference: ITU-T FG-AI4H-DEL3 (2023)</p> <p>Purpose: Define System Requirements Specification (SyRS) framework for AI4H systems</p> <p>Our Implementation:</p>"},{"location":"design/ai4h_alignment/#1-functional-requirements-del3-section-4","title":"1. Functional Requirements (DEL3 Section 4)","text":"<p>We define functional requirements for each benchmark:</p> <pre><code># Example: Cell type annotation benchmark\nbenchmark_id: CELL-TYPE-ANNOTATION\nfunctional_requirements:\n  - input: Single-cell RNA-seq count matrix\n  - output: Cell type labels from standardized ontology\n  - performance_threshold: F1 &gt; 0.80 (vs random baseline)\n</code></pre>"},{"location":"design/ai4h_alignment/#2-performance-requirements-del3-section-6","title":"2. Performance Requirements (DEL3 Section 6)","text":"<p>Our leaderboards track multiple performance dimensions:</p> <ul> <li>Accuracy metrics: AUROC, F1-score, balanced accuracy</li> <li>Robustness: rAUC scores under perturbations</li> <li>Resource usage: Inference time, memory footprint</li> <li>Fairness: Stratified performance by demographic groups</li> </ul> <p>Example:</p> <pre><code># From fmbench/runners.py\nmetrics = {\n    'auroc': roc_auc_score(y_true, y_prob),\n    'accuracy': accuracy_score(y_true, y_pred),\n    'f1_score': f1_score(y_true, y_pred, average='weighted'),\n    'stratified': {\n        'by_age': compute_stratified_metrics(y_true, y_pred, age_groups),\n        'by_sex': compute_stratified_metrics(y_true, y_pred, sex_groups),\n    }\n}\n</code></pre>"},{"location":"design/ai4h_alignment/#3-data-requirements-del3-section-42","title":"3. Data Requirements (DEL3 Section 4.2)","text":"<p>We enforce standardized data formats:</p> <ul> <li>Neuroimaging: NIfTI, preprocessed per fMRI specs</li> <li>Genomics: AnnData (scRNA-seq), FASTA (DNA), VCF (variants)</li> <li>Metadata: YAML schema with required fields</li> </ul> <pre><code># From datasets/*.yaml\ndataset_id: pbmc_68k\nname: PBMC 68k\nmodality: scRNA-seq\nn_samples: 68579\npreprocessing: scanpy_1.9.1\nquality_control:\n  min_genes_per_cell: 200\n  max_pct_mito: 5\n</code></pre>"},{"location":"design/ai4h_alignment/#4-validation-verification-del3-section-5","title":"4. Validation &amp; Verification (DEL3 Section 5)","text":"<p>Our framework includes:</p> <ul> <li>\u2705 Cross-validation: Stratified k-fold for robust estimates</li> <li>\u2705 Baseline comparison: Always compare to random, majority, linear baselines</li> <li>\u2705 Statistical significance: Permutation testing, confidence intervals</li> <li>\u2705 Confound control: Partial correlations, matched controls</li> </ul> <p>See our analysis recipes.</p>"},{"location":"design/ai4h_alignment/#del108-topic-description-document-for-neurology","title":"DEL10.8: Topic Description Document for Neurology","text":"<p>Reference: ITU-T FG-AI4H-DEL10.8 (2023)</p> <p>Purpose: Define neurology-specific evaluation standards for the TG-Neuro topic group</p> <p>Our Implementation:</p>"},{"location":"design/ai4h_alignment/#benchmark-structure-following-tdd-template","title":"Benchmark Structure (Following TDD Template)","text":"<p>Each neurology benchmark follows the TDD structure:</p> <pre><code># Example: bm_ad_classification.yaml\nbenchmark_id: AD-CLASSIFICATION\nname: Alzheimer's Disease Classification using Brain MRI\n\n# 1. Health Topic (TDD Section 2)\nhealth_topic: Alzheimer's Disease\nhealth_domain: Neurology\n\n# 2. Scope (TDD Section 3)\nscope:\n  clinical_context: \"Automated screening for AD to assist radiological workflow\"\n  population: Adults age 55+\n  inclusion_criteria:\n    - Confirmed AD diagnosis (NINCDS-ADRDA criteria) OR cognitively normal\n    - T1-weighted MRI available\n  exclusion_criteria:\n    - Other neurological disorders\n    - MRI contraindications\n\n# 3. Input Data (TDD Section 4)\ninputs:\n  dataset:\n    modality: sMRI\n    sequence: T1-weighted\n    resolution: 1mm isotropic\n    preprocessing: FreeSurfer 7.x\n\n# 4. Evaluation Metrics (TDD Section 5)\nmetrics:\n  primary: AUROC\n  secondary:\n    - Accuracy\n    - Sensitivity (recall)\n    - Specificity\n    - Positive Predictive Value\n  stratification:\n    - age_group\n    - sex\n    - apoe_genotype\n\n# 5. Clinical Relevance (TDD Section 6)\nclinical_relevance:\n  use_case: Radiological decision support\n  impact: Early detection, treatment planning\n</code></pre>"},{"location":"design/ai4h_alignment/#stratified-evaluation","title":"Stratified Evaluation","text":"<p>DEL10.8 emphasizes evaluation across patient subgroups. Our framework automatically computes stratified metrics:</p> <pre><code># From fmbench/runners.py\ndef compute_stratified_metrics(y_true, y_pred, groups):\n    \"\"\"\n    Compute metrics for each subgroup.\n\n    Aligns with DEL10.8 Section 5.3: Subgroup analysis.\n    \"\"\"\n    stratified = {}\n\n    for group_name in np.unique(groups):\n        mask = groups == group_name\n\n        if mask.sum() &gt; 10:  # Minimum sample size\n            stratified[group_name] = {\n                'accuracy': accuracy_score(y_true[mask], y_pred[mask]),\n                'n_samples': mask.sum(),\n            }\n\n    return stratified\n</code></pre> <p>Example output:</p> <pre><code>metrics:\n  accuracy: 0.85\n  auroc: 0.91\n  stratified:\n    by_age_group:\n      '55-65': {accuracy: 0.88, n_samples: 120}\n      '65-75': {accuracy: 0.85, n_samples: 200}\n      '75+': {accuracy: 0.80, n_samples: 80}\n    by_sex:\n      M: {accuracy: 0.84, n_samples: 180}\n      F: {accuracy: 0.86, n_samples: 220}\n</code></pre>"},{"location":"design/ai4h_alignment/#extensions-beyond-ai4h-standards","title":"Extensions Beyond AI4H Standards","text":"<p>While we align with AI4H deliverables, we extend the framework to address foundation model-specific challenges:</p>"},{"location":"design/ai4h_alignment/#1-robustness-testing","title":"1. Robustness Testing","text":"<p>Motivation: Foundation models must handle real-world data variability (noise, artifacts, missing data).</p> <p>Our Framework:  - Inspired by brainaug-lab methodology - Tests model resilience to controlled perturbations - Produces rAUC (Reverse Area Under Curve) scores</p> <p>Probes: - Channel dropout (missing sensors) - Gaussian noise (SNR variation) - Line noise (50/60 Hz artifacts) - Channel permutation (equivariance test) - Temporal shift (timing jitter)</p> <pre><code># Run robustness evaluation\npython -m fmbench run-robustness \\\n    --model configs/model_brainlm.yaml \\\n    --data toy_data/neuro/robustness \\\n    --out results/robustness_eval\n</code></pre> <p>See: Robustness documentation</p>"},{"location":"design/ai4h_alignment/#2-multi-modal-evaluation","title":"2. Multi-Modal Evaluation","text":"<p>Challenge: Many foundation models integrate multiple data types (e.g., imaging + genomics).</p> <p>Our Approach: - CCA-based cross-modal alignment testing - Multi-modal fusion benchmarks - Modality-specific and joint performance metrics</p> <p>See: CCA &amp; Permutation Testing</p>"},{"location":"design/ai4h_alignment/#3-interpretability-explainability","title":"3. Interpretability &amp; Explainability","text":"<p>Planned Features (aligned with DEL3 Section 8): - Attention map visualization - Feature attribution (SHAP, Integrated Gradients) - Embedding space interpretability</p>"},{"location":"design/ai4h_alignment/#compliance-checklist","title":"Compliance Checklist","text":"<p>Use this checklist to verify AI4H alignment for new benchmarks:</p> <ul> <li>[ ] Terminology: Uses standardized AI4H terms (DEL0.1)</li> <li>[ ] Health Topic: Clearly defined clinical context (DEL10.8 Section 2)</li> <li>[ ] Input Specs: Documented data format and preprocessing (DEL3 Section 4.2)</li> <li>[ ] Metrics: Primary and secondary metrics defined (DEL3 Section 6)</li> <li>[ ] Baselines: Comparison to reference implementations (DEL3 Section 7)</li> <li>[ ] Stratification: Performance across relevant subgroups (DEL10.8 Section 5.3)</li> <li>[ ] Clinical Relevance: Justification for clinical use case (DEL10.8 Section 6)</li> <li>[ ] Reproducibility: Code, data, and results publicly available</li> </ul>"},{"location":"design/ai4h_alignment/#governance-contribution","title":"Governance &amp; Contribution","text":""},{"location":"design/ai4h_alignment/#adding-new-benchmarks","title":"Adding New Benchmarks","text":"<p>To propose a new benchmark aligned with AI4H standards:</p> <ol> <li>Define the Health Topic (following DEL10.8 template)</li> <li>Specify Input/Output (following DEL3 Section 4)</li> <li>Choose Metrics (primary + secondary, with clinical justification)</li> <li>Implement Reference Baselines (see Prediction Baselines)</li> <li>Document Clinical Relevance</li> <li>Submit PR with benchmark YAML + documentation</li> </ol>"},{"location":"design/ai4h_alignment/#citing-ai4h-deliverables","title":"Citing AI4H Deliverables","text":"<p>When publishing results from this benchmark hub, please cite:</p> <pre><code>@techreport{itu_ai4h_del3_2023,\n  title = {AI4H requirement specifications},\n  author = {{ITU-T Focus Group on AI for Health}},\n  year = {2023},\n  institution = {International Telecommunication Union},\n  number = {FG-AI4H-DEL3},\n  url = {https://www.itu.int/dms_pub/itu-t/opb/fg/T-FG-AI4H-2023-11-PDF-E.pdf}\n}\n\n@techreport{itu_ai4h_del10_8_2023,\n  title = {Topic Description Document for the Topic Group on AI for neurological disorders (TG-Neuro)},\n  author = {{ITU-T Focus Group on AI for Health}},\n  year = {2023},\n  institution = {International Telecommunication Union},\n  number = {FG-AI4H-DEL10.8},\n  url = {https://www.itu.int/dms_pub/itu-t/opb/fg/T-FG-AI4H-2023-20-PDF-E.pdf}\n}\n</code></pre>"},{"location":"design/ai4h_alignment/#references","title":"References","text":"<ol> <li> <p>ITU-T Focus Group on AI for Health. (2022). Common unified terms in artificial intelligence for health (DEL0.1). PDF</p> </li> <li> <p>ITU-T Focus Group on AI for Health. (2023). AI4H requirement specifications (DEL3). PDF</p> </li> <li> <p>ITU-T Focus Group on AI for Health. (2023). Topic Description Document for the Topic Group on AI for neurological disorders (TG-Neuro) (DEL10.8). PDF</p> </li> <li> <p>Wiegand, T., et al. (2019). WHO and ITU establish benchmarking process for artificial intelligence in health. The Lancet, 394(10192), 9-11.</p> </li> </ol>"},{"location":"design/ai4h_alignment/#contact-feedback","title":"Contact &amp; Feedback","text":"<p>For questions about AI4H alignment or to suggest improvements:</p> <ul> <li>GitHub Issues: Report an issue</li> <li>ITU FG-AI4H: Official website</li> </ul> <p>\u00a9 ITU 2025. AI4H deliverables are available under the Creative Commons Attribution-Non Commercial-Share Alike 3.0 IGO licence (CC BY-NC-SA 3.0 IGO).</p>"},{"location":"integration/analysis_recipes/cca_permutation/","title":"CCA &amp; Permutation Testing","text":""},{"location":"integration/analysis_recipes/cca_permutation/#overview","title":"Overview","text":"<p>Canonical Correlation Analysis (CCA) with permutation testing is a standard protocol for evaluating the relationship between multimodal brain data representations. This recipe provides a reproducible approach for testing whether learned representations capture meaningful biological signal beyond chance.</p>"},{"location":"integration/analysis_recipes/cca_permutation/#background","title":"Background","text":"<p>Canonical Correlation Analysis (CCA) identifies linear combinations of two sets of variables that maximize their correlation. In the context of foundation models:</p> <ul> <li>Use Case 1: Compare model embeddings from different modalities (e.g., fMRI vs genetics)</li> <li>Use Case 2: Test if model representations align with behavioral or clinical outcomes</li> <li>Use Case 3: Validate cross-modal alignment in multi-modal foundation models</li> </ul> <p>Permutation Testing provides statistical significance by: 1. Computing CCA on real data 2. Shuffling one modality randomly N times 3. Computing CCA on each permutation 4. Comparing real correlation against null distribution</p>"},{"location":"integration/analysis_recipes/cca_permutation/#protocol","title":"Protocol","text":""},{"location":"integration/analysis_recipes/cca_permutation/#1-data-preparation","title":"1. Data Preparation","text":"<pre><code>import numpy as np\nfrom sklearn.cross_decomposition import CCA\n\n# Load your model embeddings\n# X: (n_samples, n_features_1) - e.g., fMRI embeddings\n# Y: (n_samples, n_features_2) - e.g., genomic embeddings\n\nX = model_fmri.encode(fmri_data)  # Shape: (N, D1)\nY = model_genomics.encode(genomic_data)  # Shape: (N, D2)\n\n# Standardize\nX = (X - X.mean(axis=0)) / X.std(axis=0)\nY = (Y - Y.mean(axis=0)) / Y.std(axis=0)\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#2-run-cca","title":"2. Run CCA","text":"<pre><code># Number of canonical components to compute\nn_components = min(X.shape[1], Y.shape[1], 20)\n\ncca = CCA(n_components=n_components)\nX_c, Y_c = cca.fit_transform(X, Y)\n\n# Compute canonical correlations\ncanonical_corrs = [np.corrcoef(X_c[:, i], Y_c[:, i])[0, 1] \n                   for i in range(n_components)]\n\nprint(f\"Top 5 canonical correlations: {canonical_corrs[:5]}\")\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#3-permutation-test","title":"3. Permutation Test","text":"<pre><code>from tqdm import tqdm\n\nn_permutations = 1000\nperm_corrs = []\n\nfor _ in tqdm(range(n_permutations)):\n    # Shuffle one modality\n    idx = np.random.permutation(len(Y))\n    Y_perm = Y[idx]\n\n    # Run CCA on permuted data\n    cca_perm = CCA(n_components=n_components)\n    X_c_perm, Y_c_perm = cca_perm.fit_transform(X, Y_perm)\n\n    # Store top canonical correlation\n    perm_corr = np.corrcoef(X_c_perm[:, 0], Y_c_perm[:, 0])[0, 1]\n    perm_corrs.append(perm_corr)\n\nperm_corrs = np.array(perm_corrs)\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#4-compute-p-value","title":"4. Compute P-Value","text":"<pre><code># P-value: fraction of permutations with correlation &gt;= observed\nreal_corr = canonical_corrs[0]\np_value = (perm_corrs &gt;= real_corr).mean()\n\nprint(f\"Real CCA correlation: {real_corr:.4f}\")\nprint(f\"Mean permuted correlation: {perm_corrs.mean():.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\n# Effect size\neffect_size = (real_corr - perm_corrs.mean()) / perm_corrs.std()\nprint(f\"Effect size (Z-score): {effect_size:.2f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#5-visualization","title":"5. Visualization","text":"<pre><code>import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Histogram of permutation null distribution\naxes[0].hist(perm_corrs, bins=50, alpha=0.7, label='Permuted')\naxes[0].axvline(real_corr, color='red', linestyle='--', \n                label=f'Observed (p={p_value:.3f})')\naxes[0].set_xlabel('Canonical Correlation')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('CCA Permutation Test')\naxes[0].legend()\n\n# Scree plot of canonical correlations\naxes[1].plot(range(1, len(canonical_corrs)+1), canonical_corrs, \n             marker='o', label='Real Data')\naxes[1].axhline(perm_corrs.mean(), color='gray', linestyle='--', \n                label='Permuted Mean')\naxes[1].set_xlabel('Canonical Component')\naxes[1].set_ylabel('Correlation')\naxes[1].set_title('Canonical Correlations (Scree Plot)')\naxes[1].legend()\n\nplt.tight_layout()\nplt.savefig('cca_permutation_results.png', dpi=150)\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#interpretation-guidelines","title":"Interpretation Guidelines","text":""},{"location":"integration/analysis_recipes/cca_permutation/#statistical-significance","title":"Statistical Significance","text":"<ul> <li>p &lt; 0.05: Significant alignment between modalities</li> <li>p &lt; 0.01: Strong evidence of cross-modal relationship</li> <li>p &lt; 0.001: Very strong evidence</li> </ul>"},{"location":"integration/analysis_recipes/cca_permutation/#effect-size","title":"Effect Size","text":"<ul> <li>Z &gt; 2: Small to moderate effect</li> <li>Z &gt; 3: Moderate to large effect  </li> <li>Z &gt; 5: Large effect</li> </ul>"},{"location":"integration/analysis_recipes/cca_permutation/#multiple-comparisons","title":"Multiple Comparisons","text":"<p>When testing multiple canonical components, apply correction:</p> <pre><code>from statsmodels.stats.multitest import multipletests\n\n# Compute p-value for each component\np_values = [(perm_corrs &gt;= corr).mean() for corr in canonical_corrs]\n\n# Bonferroni correction\n_, p_corrected, _, _ = multipletests(p_values, method='bonferroni')\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#example-use-cases","title":"Example Use Cases","text":""},{"location":"integration/analysis_recipes/cca_permutation/#use-case-1-multi-modal-foundation-model-validation","title":"Use Case 1: Multi-Modal Foundation Model Validation","text":"<p>Test if a multi-modal brain model's fMRI and genetics branches learn aligned representations:</p> <pre><code># Extract embeddings from each branch\nfmri_emb = model.encode_fmri(fmri_data)\ngene_emb = model.encode_genetics(genetics_data)\n\n# Run CCA + permutation test\n# \u2192 If p &lt; 0.05, model successfully learns cross-modal alignment\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#use-case-2-behavioral-prediction-alignment","title":"Use Case 2: Behavioral Prediction Alignment","text":"<p>Test if brain embeddings correlate with behavioral scores:</p> <pre><code># Brain embeddings (N x D)\nbrain_emb = model.encode(fmri_data)\n\n# Behavioral scores (N x K)\nbehavior_scores = load_behavioral_data()\n\n# Run CCA\n# \u2192 High correlation suggests embeddings capture behaviorally relevant features\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#itu-ai4h-alignment","title":"ITU AI4H Alignment","text":"<p>This protocol aligns with:</p> <ul> <li>DEL3 Section 5.2: Validation and verification requirements</li> <li>DEL10.8: Neurology-specific evaluation protocols</li> <li>DEL0.1: Statistical significance standards for AI4H benchmarks</li> </ul>"},{"location":"integration/analysis_recipes/cca_permutation/#references","title":"References","text":"<ol> <li>Hotelling, H. (1936). Relations between two sets of variates. Biometrika, 28(3/4), 321-377.</li> <li>Witten, D. M., Tibshirani, R., &amp; Hastie, T. (2009). A penalized matrix decomposition. Biostatistics, 10(3), 515-534.</li> <li>Wang, C. et al. (2020). Variational CCA for multimodal representation learning. ICML.</li> </ol>"},{"location":"integration/analysis_recipes/cca_permutation/#related-protocols","title":"Related Protocols","text":"<ul> <li>Prediction Baselines</li> <li>Partial Correlations</li> </ul>"},{"location":"integration/analysis_recipes/partial_correlations/","title":"Partial Correlations","text":""},{"location":"integration/analysis_recipes/partial_correlations/#overview","title":"Overview","text":"<p>Partial correlation analysis controls for confounding variables when evaluating relationships between model representations and outcomes. This is essential for neurogenomic studies where age, sex, site, and other covariates can introduce spurious correlations.</p>"},{"location":"integration/analysis_recipes/partial_correlations/#background","title":"Background","text":"<p>Partial Correlation measures the relationship between two variables while controlling for one or more confounding variables.</p> <p>Given: - X: Feature of interest (e.g., brain embedding dimension) - Y: Outcome (e.g., cognitive score) - Z: Confounders (e.g., age, sex, scanner site)</p> <p>The partial correlation between X and Y controlling for Z is the correlation between the residuals of: 1. X regressed on Z 2. Y regressed on Z</p>"},{"location":"integration/analysis_recipes/partial_correlations/#why-partial-correlations-matter","title":"Why Partial Correlations Matter","text":""},{"location":"integration/analysis_recipes/partial_correlations/#common-confounders-in-neurogenomics","title":"Common Confounders in Neurogenomics","text":""},{"location":"integration/analysis_recipes/partial_correlations/#neuroimaging","title":"Neuroimaging","text":"<ul> <li>Age: Affects brain structure and function</li> <li>Sex: Systematic differences in brain anatomy</li> <li>Scanner site: Multi-site studies have acquisition differences</li> <li>Head motion: Correlates with many clinical variables</li> <li>Total intracranial volume (TIV): Affects structural measurements</li> </ul>"},{"location":"integration/analysis_recipes/partial_correlations/#genomics","title":"Genomics","text":"<ul> <li>Batch effects: Technical variation between sequencing runs</li> <li>Cell composition: Proportion of cell types in bulk data</li> <li>Sequencing depth: Coverage differences across samples</li> <li>Population stratification: Ancestry-related genetic variation</li> </ul>"},{"location":"integration/analysis_recipes/partial_correlations/#protocol","title":"Protocol","text":""},{"location":"integration/analysis_recipes/partial_correlations/#1-basic-partial-correlation","title":"1. Basic Partial Correlation","text":"<pre><code>import numpy as np\nfrom scipy.stats import pearsonr\nfrom sklearn.linear_model import LinearRegression\n\ndef partial_correlation(X, Y, Z):\n    \"\"\"\n    Compute partial correlation between X and Y controlling for Z.\n\n    Args:\n        X: (n_samples,) or (n_samples, 1)\n        Y: (n_samples,) or (n_samples, 1)  \n        Z: (n_samples, n_confounds)\n\n    Returns:\n        r: Partial correlation coefficient\n        p: P-value\n    \"\"\"\n    # Reshape if needed\n    X = np.asarray(X).reshape(-1, 1) if X.ndim == 1 else X\n    Y = np.asarray(Y).reshape(-1, 1) if Y.ndim == 1 else Y\n    Z = np.asarray(Z).reshape(-1, 1) if Z.ndim == 1 else Z\n\n    # Regress out confounds from X\n    lr_x = LinearRegression()\n    lr_x.fit(Z, X)\n    X_resid = X - lr_x.predict(Z)\n\n    # Regress out confounds from Y\n    lr_y = LinearRegression()\n    lr_y.fit(Z, Y)\n    Y_resid = Y - lr_y.predict(Z)\n\n    # Correlation of residuals\n    r, p = pearsonr(X_resid.ravel(), Y_resid.ravel())\n\n    return r, p\n\n# Example usage\nfrom sklearn.datasets import make_regression\n\n# Simulated data\nn_samples = 200\nX, Y = make_regression(n_samples=n_samples, n_features=1, noise=10, random_state=42)\nX = X.ravel()\n\n# Confounders (age, sex)\nage = np.random.randn(n_samples)\nsex = np.random.randint(0, 2, n_samples)\nZ = np.column_stack([age, sex])\n\n# Compute partial correlation\nr_partial, p_partial = partial_correlation(X, Y, Z)\nprint(f\"Partial correlation: r={r_partial:.3f}, p={p_partial:.4f}\")\n\n# Compare to raw correlation (without controlling)\nr_raw, p_raw = pearsonr(X, Y)\nprint(f\"Raw correlation: r={r_raw:.3f}, p={p_raw:.4f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#2-multiple-partial-correlations","title":"2. Multiple Partial Correlations","text":"<p>When testing many features (e.g., all embedding dimensions):</p> <pre><code>from statsmodels.stats.multitest import multipletests\n\ndef partial_correlation_matrix(X, Y, Z):\n    \"\"\"\n    Compute partial correlations for multiple features.\n\n    Args:\n        X: (n_samples, n_features) - features to test\n        Y: (n_samples,) - outcome\n        Z: (n_samples, n_confounds) - confounders\n\n    Returns:\n        correlations: (n_features,) - partial correlation coefficients\n        p_values: (n_features,) - uncorrected p-values\n        p_corrected: (n_features,) - FDR-corrected p-values\n    \"\"\"\n    n_features = X.shape[1]\n    correlations = np.zeros(n_features)\n    p_values = np.zeros(n_features)\n\n    for i in range(n_features):\n        r, p = partial_correlation(X[:, i], Y, Z)\n        correlations[i] = r\n        p_values[i] = p\n\n    # FDR correction\n    _, p_corrected, _, _ = multipletests(p_values, method='fdr_bh')\n\n    return correlations, p_values, p_corrected\n\n# Example: Test all embedding dimensions\nembeddings = model.encode(brain_data)  # Shape: (n_samples, embedding_dim)\ncognitive_score = load_cognitive_scores()  # Shape: (n_samples,)\n\nconfounds = np.column_stack([age, sex, site_indicator])\n\ncorrs, p_raw, p_fdr = partial_correlation_matrix(\n    embeddings, \n    cognitive_score, \n    confounds\n)\n\n# Find significant dimensions (FDR &lt; 0.05)\nsig_dims = np.where(p_fdr &lt; 0.05)[0]\nprint(f\"Significant dimensions: {sig_dims}\")\nprint(f\"Correlations: {corrs[sig_dims]}\")\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#3-partial-correlation-with-standardization","title":"3. Partial Correlation with Standardization","text":"<p>For better interpretability, standardize all variables:</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\ndef partial_correlation_standardized(X, Y, Z):\n    \"\"\"Partial correlation with standardized variables.\"\"\"\n    scaler = StandardScaler()\n\n    X_std = scaler.fit_transform(X.reshape(-1, 1)).ravel()\n    Y_std = scaler.fit_transform(Y.reshape(-1, 1)).ravel()\n    Z_std = scaler.fit_transform(Z)\n\n    return partial_correlation(X_std, Y_std, Z_std)\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#4-using-pingouin-library","title":"4. Using pingouin Library","text":"<p>For more advanced partial correlation analysis:</p> <pre><code>import pingouin as pg\n\n# Create DataFrame\nimport pandas as pd\ndf = pd.DataFrame({\n    'brain_feature': X,\n    'cognitive_score': Y,\n    'age': age,\n    'sex': sex\n})\n\n# Partial correlation\nresult = pg.partial_corr(\n    data=df,\n    x='brain_feature',\n    y='cognitive_score',\n    covar=['age', 'sex'],\n    method='pearson'\n)\n\nprint(result)\n# Output: n, r, CI95%, p-val\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#domain-specific-applications","title":"Domain-Specific Applications","text":""},{"location":"integration/analysis_recipes/partial_correlations/#neuroimaging-example-fmri-connectivity-and-behavior","title":"Neuroimaging Example: fMRI Connectivity and Behavior","text":"<pre><code># Load data\nconnectivity = load_fmri_connectivity()  # (n_subjects, n_connections)\nbehavior = load_behavior_score()  # (n_subjects,)\nage = load_age()  # (n_subjects,)\nsex = load_sex()  # (n_subjects,)\nsite = load_site()  # (n_subjects,)\nmotion = load_head_motion()  # (n_subjects,)\n\n# Confound matrix\nZ = np.column_stack([age, sex, site, motion])\n\n# Test each connection\nn_connections = connectivity.shape[1]\nresults = []\n\nfor i in range(n_connections):\n    r, p = partial_correlation(connectivity[:, i], behavior, Z)\n    results.append({'connection_id': i, 'r': r, 'p': p})\n\nresults_df = pd.DataFrame(results)\n\n# FDR correction\n_, results_df['p_fdr'], _, _ = multipletests(\n    results_df['p'], \n    method='fdr_bh'\n)\n\n# Significant connections\nsig_connections = results_df[results_df['p_fdr'] &lt; 0.05]\nprint(f\"Found {len(sig_connections)} significant connections\")\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#genomics-example-gene-expression-and-disease","title":"Genomics Example: Gene Expression and Disease","text":"<pre><code># Load single-cell data\ngene_expression = load_gene_expression()  # (n_cells, n_genes)\ndisease_score = load_disease_phenotype()  # (n_cells,)\n\n# Confounders\nbatch = load_batch_id()  # (n_cells,)\nsequencing_depth = load_total_counts()  # (n_cells,)\ncell_cycle = load_cell_cycle_score()  # (n_cells,)\n\nZ_genomics = np.column_stack([batch, sequencing_depth, cell_cycle])\n\n# Find disease-associated genes (controlling for technical factors)\ncorrs, p_raw, p_fdr = partial_correlation_matrix(\n    gene_expression,\n    disease_score,\n    Z_genomics\n)\n\n# Top disease-associated genes\ntop_genes_idx = np.argsort(np.abs(corrs))[-20:]\nprint(f\"Top disease-associated genes: {gene_names[top_genes_idx]}\")\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#visualization","title":"Visualization","text":""},{"location":"integration/analysis_recipes/partial_correlations/#1-comparison-plot-raw-vs-partial-correlations","title":"1. Comparison Plot: Raw vs Partial Correlations","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Compute both raw and partial correlations\nraw_corrs = []\npartial_corrs = []\n\nfor i in range(X.shape[1]):\n    r_raw, _ = pearsonr(X[:, i], Y)\n    r_partial, _ = partial_correlation(X[:, i], Y, Z)\n    raw_corrs.append(r_raw)\n    partial_corrs.append(r_partial)\n\nraw_corrs = np.array(raw_corrs)\npartial_corrs = np.array(partial_corrs)\n\n# Scatter plot\nfig, ax = plt.subplots(figsize=(6, 6))\nax.scatter(raw_corrs, partial_corrs, alpha=0.6)\nax.plot([-1, 1], [-1, 1], 'k--', label='Identity')\nax.set_xlabel('Raw Correlation')\nax.set_ylabel('Partial Correlation\\n(controlling for confounds)')\nax.set_title('Effect of Confound Correction')\nax.legend()\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.savefig('raw_vs_partial_correlation.png', dpi=150)\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#2-manhattan-plot-for-genome-wide-analysis","title":"2. Manhattan Plot for Genome-Wide Analysis","text":"<pre><code>def manhattan_plot(p_values, chromosome_positions=None):\n    \"\"\"Create Manhattan plot for partial correlation p-values.\"\"\"\n    fig, ax = plt.subplots(figsize=(14, 4))\n\n    # -log10(p-value) for y-axis\n    neg_log_p = -np.log10(p_values)\n\n    # Color by chromosome (if provided)\n    if chromosome_positions is not None:\n        colors = ['blue', 'orange']\n        for chrom in np.unique(chromosome_positions):\n            mask = chromosome_positions == chrom\n            color = colors[chrom % 2]\n            ax.scatter(np.where(mask)[0], neg_log_p[mask], \n                      c=color, s=5, alpha=0.7)\n    else:\n        ax.scatter(range(len(p_values)), neg_log_p, s=5, alpha=0.7)\n\n    # Significance threshold line\n    ax.axhline(-np.log10(0.05), color='red', linestyle='--', \n               label='p=0.05')\n    ax.axhline(-np.log10(0.05 / len(p_values)), color='green', \n               linestyle='--', label='Bonferroni')\n\n    ax.set_xlabel('Feature Index')\n    ax.set_ylabel('-log\u2081\u2080(p-value)')\n    ax.set_title('Partial Correlation Significance')\n    ax.legend()\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#interpretation-guidelines","title":"Interpretation Guidelines","text":""},{"location":"integration/analysis_recipes/partial_correlations/#when-to-use-partial-correlations","title":"When to Use Partial Correlations","text":"<p>\u2705 Use when: - Known confounders exist (age, sex, site) - Multi-site studies - Evaluating specific hypotheses while controlling for nuisance variables</p> <p>\u274c Don't use when: - No clear confounders - Confounders are part of the scientific question - Confounders are colliders (can introduce bias)</p>"},{"location":"integration/analysis_recipes/partial_correlations/#effect-size-interpretation","title":"Effect Size Interpretation","text":"<p>| |r| | Interpretation | |------|----------------| | &lt; 0.1 | Negligible | | 0.1 - 0.3 | Small | | 0.3 - 0.5 | Moderate | | &gt; 0.5 | Large |</p>"},{"location":"integration/analysis_recipes/partial_correlations/#statistical-power","title":"Statistical Power","text":"<p>Required sample size for 80% power:</p> Effect Size (r) n (\u03b1=0.05) 0.1 ~780 0.2 ~195 0.3 ~85 0.5 ~30"},{"location":"integration/analysis_recipes/partial_correlations/#itu-ai4h-alignment","title":"ITU AI4H Alignment","text":"<p>This protocol aligns with:</p> <ul> <li>DEL3 Section 5.3: Confound control in validation</li> <li>DEL10.8 Section 4.2: Covariate adjustment in neurology benchmarks</li> <li>DEL0.1: Statistical terminology standards</li> </ul>"},{"location":"integration/analysis_recipes/partial_correlations/#best-practices","title":"Best Practices","text":"<ol> <li>Pre-register confounders: Decide which confounders to control before analysis</li> <li>Report both raw and partial: Show effect of confound correction</li> <li>Visualize confound effects: Plot raw vs. partial correlations</li> <li>Use appropriate corrections: FDR or Bonferroni for multiple comparisons</li> <li>Check assumptions: Linearity, homoscedasticity, normality</li> </ol>"},{"location":"integration/analysis_recipes/partial_correlations/#references","title":"References","text":"<ol> <li>Fisher, R. A. (1924). The distribution of the partial correlation coefficient. Metron, 3, 329-332.</li> <li>Baba, K., et al. (2004). Partial correlation and conditional correlation as measures of conditional independence. Australian &amp; New Zealand Journal of Statistics, 46(4), 657-664.</li> <li>Vallat, R. (2018). Pingouin: statistics in Python. JOSS, 3(31), 1026.</li> </ol>"},{"location":"integration/analysis_recipes/partial_correlations/#related-protocols","title":"Related Protocols","text":"<ul> <li>CCA &amp; Permutation Testing</li> <li>Prediction Baselines</li> </ul>"},{"location":"integration/analysis_recipes/prediction_baselines/","title":"Prediction Baselines","text":""},{"location":"integration/analysis_recipes/prediction_baselines/#overview","title":"Overview","text":"<p>Establishing strong baselines is critical for fair evaluation of foundation models. This protocol defines standardized baseline approaches for common prediction tasks in neurogenomics, aligned with ITU AI4H standards.</p>"},{"location":"integration/analysis_recipes/prediction_baselines/#baseline-types","title":"Baseline Types","text":""},{"location":"integration/analysis_recipes/prediction_baselines/#1-random-baseline","title":"1. Random Baseline","text":"<p>When to use: Always. Provides lower bound for model performance.</p> <pre><code>import numpy as np\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\n# For binary classification\ny_true = np.array([0, 1, 0, 1, ...])  # True labels\ny_pred_random = np.random.randint(0, 2, size=len(y_true))\n\nacc_random = accuracy_score(y_true, y_pred_random)\nprint(f\"Random baseline accuracy: {acc_random:.3f}\")  # ~0.500\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#2-majority-class-baseline","title":"2. Majority Class Baseline","text":"<p>When to use: Classification tasks with class imbalance.</p> <pre><code>from sklearn.dummy import DummyClassifier\n\n# Majority class classifier\nmajority_clf = DummyClassifier(strategy='most_frequent')\nmajority_clf.fit(X_train, y_train)\ny_pred = majority_clf.predict(X_test)\n\nacc_majority = accuracy_score(y_test, y_pred)\nprint(f\"Majority class baseline: {acc_majority:.3f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#3-logistic-regression-linear","title":"3. Logistic Regression (Linear)","text":"<p>When to use: Standard baseline for classification tasks.</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train logistic regression\nlr = LogisticRegression(max_iter=1000, random_state=42)\nlr.fit(X_train_scaled, y_train)\n\n# Evaluate\ny_pred = lr.predict(X_test_scaled)\ny_prob = lr.predict_proba(X_test_scaled)[:, 1]\n\nacc_lr = accuracy_score(y_test, y_pred)\nauc_lr = roc_auc_score(y_test, y_prob)\n\nprint(f\"Logistic Regression - Accuracy: {acc_lr:.3f}, AUC: {auc_lr:.3f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#4-ridge-regression-linear-regularized","title":"4. Ridge Regression (Linear, Regularized)","text":"<p>When to use: High-dimensional data with multicollinearity (e.g., fMRI, genomics).</p> <pre><code>from sklearn.linear_model import Ridge, RidgeCV\n\n# Cross-validated ridge\nalphas = np.logspace(-3, 3, 50)\nridge_cv = RidgeCV(alphas=alphas, cv=5)\nridge_cv.fit(X_train_scaled, y_train)\n\ny_pred = ridge_cv.predict(X_test_scaled)\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Ridge Regression - MSE: {mse:.3f}, R\u00b2: {r2:.3f}\")\nprint(f\"Optimal alpha: {ridge_cv.alpha_:.3f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#5-random-forest-non-linear","title":"5. Random Forest (Non-linear)","text":"<p>When to use: Benchmark for capturing non-linear relationships.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,\n    min_samples_split=10,\n    random_state=42,\n    n_jobs=-1\n)\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\ny_prob = rf.predict_proba(X_test)[:, 1]\n\nacc_rf = accuracy_score(y_test, y_pred)\nauc_rf = roc_auc_score(y_test, y_prob)\n\nprint(f\"Random Forest - Accuracy: {acc_rf:.3f}, AUC: {auc_rf:.3f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#6-k-nearest-neighbors-instance-based","title":"6. k-Nearest Neighbors (Instance-based)","text":"<p>When to use: Baseline for representation quality (in embedding space).</p> <pre><code>from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_scaled, y_train)\n\ny_pred = knn.predict(X_test_scaled)\nacc_knn = accuracy_score(y_test, y_pred)\n\nprint(f\"k-NN baseline: {acc_knn:.3f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#domain-specific-baselines","title":"Domain-Specific Baselines","text":""},{"location":"integration/analysis_recipes/prediction_baselines/#neuroimaging-fmrismri","title":"Neuroimaging (fMRI/sMRI)","text":""},{"location":"integration/analysis_recipes/prediction_baselines/#regional-mean-baseline","title":"Regional Mean Baseline","text":"<p>Use mean signal per brain region as features:</p> <pre><code># Assuming parcellated brain data\n# X_raw: (n_samples, n_voxels)\n# parcellation: (n_voxels,) - region labels\n\nn_regions = len(np.unique(parcellation))\nX_regional = np.zeros((len(X_raw), n_regions))\n\nfor i, region_id in enumerate(np.unique(parcellation)):\n    mask = parcellation == region_id\n    X_regional[:, i] = X_raw[:, mask].mean(axis=1)\n\n# Use X_regional for prediction\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#functional-connectivity-baseline","title":"Functional Connectivity Baseline","text":"<p>Use pairwise correlations as features:</p> <pre><code>from nilearn.connectome import ConnectivityMeasure\n\nconn_measure = ConnectivityMeasure(kind='correlation')\nconnectivity_matrices = conn_measure.fit_transform(timeseries_data)\n\n# Flatten upper triangle as features\nn_samples = len(connectivity_matrices)\nn_regions = connectivity_matrices.shape[1]\nn_features = n_regions * (n_regions - 1) // 2\n\nX_conn = np.zeros((n_samples, n_features))\nfor i, conn_mat in enumerate(connectivity_matrices):\n    X_conn[i] = conn_mat[np.triu_indices(n_regions, k=1)]\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#genomics-scrna-seq","title":"Genomics (scRNA-seq)","text":""},{"location":"integration/analysis_recipes/prediction_baselines/#highly-variable-genes-hvg-baseline","title":"Highly Variable Genes (HVG) Baseline","text":"<p>Use only the most variable genes:</p> <pre><code>from sklearn.feature_selection import VarianceThreshold\n\n# Select top 2000 most variable genes\ngene_vars = X_train.var(axis=0)\ntop_genes_idx = np.argsort(gene_vars)[-2000:]\n\nX_train_hvg = X_train[:, top_genes_idx]\nX_test_hvg = X_test[:, top_genes_idx]\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#cell-type-marker-genes","title":"Cell-Type Marker Genes","text":"<p>Use known marker genes for classification:</p> <pre><code># Pre-defined marker gene list\nmarker_genes = ['CD3D', 'CD79A', 'CST3', 'NKG7', ...]  # Example\nmarker_idx = [gene_names.index(g) for g in marker_genes if g in gene_names]\n\nX_train_markers = X_train[:, marker_idx]\nX_test_markers = X_test[:, marker_idx]\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#baseline-evaluation-protocol","title":"Baseline Evaluation Protocol","text":""},{"location":"integration/analysis_recipes/prediction_baselines/#1-define-metrics","title":"1. Define Metrics","text":"<pre><code>from sklearn.metrics import (\n    accuracy_score, \n    balanced_accuracy_score,\n    f1_score, \n    roc_auc_score,\n    precision_recall_fscore_support\n)\n\ndef evaluate_classifier(y_true, y_pred, y_prob=None):\n    \"\"\"Comprehensive classification metrics.\"\"\"\n    metrics = {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n        'f1_score': f1_score(y_true, y_pred, average='weighted'),\n    }\n\n    if y_prob is not None:\n        metrics['auroc'] = roc_auc_score(y_true, y_prob)\n\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        y_true, y_pred, average='weighted'\n    )\n    metrics.update({\n        'precision': precision,\n        'recall': recall,\n    })\n\n    return metrics\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#2-cross-validation","title":"2. Cross-Validation","text":"<pre><code>from sklearn.model_selection import cross_validate\n\n# 5-fold cross-validation\ncv_results = cross_validate(\n    estimator=lr,\n    X=X_train_scaled,\n    y=y_train,\n    cv=5,\n    scoring=['accuracy', 'roc_auc', 'f1'],\n    return_train_score=True,\n    n_jobs=-1\n)\n\nprint(f\"CV Accuracy: {cv_results['test_accuracy'].mean():.3f} \"\n      f\"\u00b1 {cv_results['test_accuracy'].std():.3f}\")\nprint(f\"CV AUC: {cv_results['test_roc_auc'].mean():.3f} \"\n      f\"\u00b1 {cv_results['test_roc_auc'].std():.3f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#3-baseline-comparison-table","title":"3. Baseline Comparison Table","text":"<pre><code>import pandas as pd\n\nbaselines = {\n    'Random': (acc_random, None),\n    'Majority Class': (acc_majority, None),\n    'Logistic Regression': (acc_lr, auc_lr),\n    'Ridge': (acc_ridge, auc_ridge),\n    'Random Forest': (acc_rf, auc_rf),\n    'k-NN': (acc_knn, auc_knn),\n}\n\nresults_df = pd.DataFrame([\n    {'Method': name, 'Accuracy': acc, 'AUC': auc}\n    for name, (acc, auc) in baselines.items()\n])\n\nprint(results_df.to_markdown(index=False, floatfmt='.3f'))\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#expected-baseline-performance","title":"Expected Baseline Performance","text":""},{"location":"integration/analysis_recipes/prediction_baselines/#neuroimaging-classification-eg-ad-vs-cn","title":"Neuroimaging Classification (e.g., AD vs CN)","text":"Baseline Typical Accuracy Typical AUC Random 0.50 0.50 Majority 0.50-0.60 - Logistic Regression 0.65-0.75 0.70-0.80 Ridge 0.65-0.75 0.70-0.80 Random Forest 0.70-0.80 0.75-0.85"},{"location":"integration/analysis_recipes/prediction_baselines/#single-cell-classification-cell-type","title":"Single-Cell Classification (Cell Type)","text":"Baseline Typical Accuracy Typical F1 Random 0.10-0.20 0.10-0.20 Marker Genes + Logistic 0.75-0.85 0.70-0.80 HVG + Random Forest 0.80-0.90 0.75-0.85"},{"location":"integration/analysis_recipes/prediction_baselines/#itu-ai4h-alignment","title":"ITU AI4H Alignment","text":"<p>This protocol aligns with:</p> <ul> <li>DEL3 Section 6.3: Performance benchmarking requirements</li> <li>DEL3 Section 7.1: Baseline comparison standards</li> <li>DEL0.1: Terminology for baseline models (\"reference implementation\")</li> </ul>"},{"location":"integration/analysis_recipes/prediction_baselines/#best-practices","title":"Best Practices","text":"<ol> <li>Always report random baseline to establish floor performance</li> <li>Use cross-validation for robust baseline estimates</li> <li>Match preprocessing between baselines and foundation models</li> <li>Report confidence intervals (e.g., via bootstrapping)</li> <li>Document hyperparameters for reproducibility</li> </ol>"},{"location":"integration/analysis_recipes/prediction_baselines/#related-protocols","title":"Related Protocols","text":"<ul> <li>CCA &amp; Permutation Testing</li> <li>Partial Correlations</li> </ul>"},{"location":"integration/modality_features/fmri/","title":"fMRI Data Specifications","text":""},{"location":"integration/modality_features/fmri/#overview","title":"Overview","text":"<p>Functional MRI (fMRI) measures brain activity by detecting changes in blood oxygenation (BOLD signal). This page defines the standardized data formats and preprocessing requirements for fMRI data in this benchmark hub.</p>"},{"location":"integration/modality_features/fmri/#data-format-requirements","title":"Data Format Requirements","text":""},{"location":"integration/modality_features/fmri/#1-raw-fmri-time-series","title":"1. Raw fMRI Time Series","text":"<p>Format: NIfTI (<code>.nii</code> or <code>.nii.gz</code>) or NumPy array Shape: <code>(n_samples, n_timepoints, n_voxels)</code> or <code>(n_samples, n_voxels, n_timepoints)</code> Data Type: <code>float32</code> or <code>float64</code></p> <pre><code>import numpy as np\nimport nibabel as nib\n\n# Load NIfTI file\nimg = nib.load('subject_001_bold.nii.gz')\ndata = img.get_fdata()  # Shape: (x, y, z, time)\n\n# Reshape to 2D: (time, voxels)\nn_timepoints = data.shape[-1]\ntimeseries = data.reshape(-1, n_timepoints).T  # (time, voxels)\n</code></pre>"},{"location":"integration/modality_features/fmri/#2-preprocessed-time-series-recommended","title":"2. Preprocessed Time Series (Recommended)","text":"<p>Minimum preprocessing steps: 1. \u2705 Motion correction 2. \u2705 Slice timing correction 3. \u2705 Spatial normalization to standard space (MNI152) 4. \u2705 Nuisance regression (motion parameters, CSF, white matter) 5. \u2705 Bandpass filtering (0.01 - 0.1 Hz typical for resting-state)</p> <p>Optional: - Spatial smoothing (6-8mm FWHM) - Global signal regression (controversial) - Denoising (ICA-AROMA, CompCor)</p>"},{"location":"integration/modality_features/fmri/#3-parcellatedroi-time-series","title":"3. Parcellated/ROI Time Series","text":"<p>Format: NumPy array or CSV Shape: <code>(n_samples, n_timepoints, n_regions)</code></p> <pre><code># Example: 400 parcels (Schaefer atlas), 200 timepoints\nroi_timeseries = np.load('subject_timeseries.npy')  \n# Shape: (1, 200, 400)\n</code></pre> <p>Recommended atlases: - Schaefer 2018 (100-1000 parcels, 7 or 17 networks) - AAL3 (170 regions) - Gordon (333 parcels) - Harvard-Oxford (cortical + subcortical) - Glasser MMP (360 parcels)</p>"},{"location":"integration/modality_features/fmri/#4-connectivity-matrices","title":"4. Connectivity Matrices","text":"<p>Format: NumPy array Shape: <code>(n_samples, n_regions, n_regions)</code> or <code>(n_samples, n_features)</code> (vectorized)</p> <pre><code>from nilearn.connectome import ConnectivityMeasure\n\n# Compute functional connectivity\nconn_measure = ConnectivityMeasure(kind='correlation')\nconnectivity = conn_measure.fit_transform([roi_timeseries])\n# Shape: (1, n_regions, n_regions)\n\n# Vectorize upper triangle (for ML)\nfrom sklearn.utils import check_array\nimport numpy as np\n\ndef vectorize_connectivity(conn_mat):\n    \"\"\"Extract upper triangle as feature vector.\"\"\"\n    n_regions = conn_mat.shape[0]\n    triu_idx = np.triu_indices(n_regions, k=1)\n    return conn_mat[triu_idx]\n\nfeatures = vectorize_connectivity(connectivity[0])\n# Shape: (n_regions * (n_regions-1) / 2,)\n</code></pre>"},{"location":"integration/modality_features/fmri/#metadata-requirements","title":"Metadata Requirements","text":"<p>Each fMRI dataset should include metadata:</p> <pre><code>dataset_id: ukb_fmri_tensor\nname: UK Biobank fMRI Tensors\nmodality: fmri\ntask: resting_state  # or 'task_based'\nn_subjects: 40000\nn_timepoints: 490\nn_voxels: 91282  # or n_regions if parcellated\ntr: 0.735  # Repetition time in seconds\npreprocessing: fmriprep_20.2.0\natlas: Schaefer2018_400  # if parcellated\nbandpass: [0.01, 0.1]  # Hz\nsmoothing: 6mm  # FWHM, or null\nstandard_space: MNI152NLin6Asym\n</code></pre>"},{"location":"integration/modality_features/fmri/#quality-control-metrics","title":"Quality Control Metrics","text":""},{"location":"integration/modality_features/fmri/#1-motion-parameters","title":"1. Motion Parameters","text":"<p>Framewise Displacement (FD): Measure of head motion</p> <pre><code>def framewise_displacement(motion_params):\n    \"\"\"\n    Calculate framewise displacement (Power et al. 2012).\n\n    Args:\n        motion_params: (n_timepoints, 6) - 3 translations + 3 rotations\n\n    Returns:\n        fd: (n_timepoints-1,) - framewise displacement\n    \"\"\"\n    # Translations in mm\n    trans = motion_params[:, :3]\n\n    # Rotations converted to mm (50mm sphere radius)\n    rot = motion_params[:, 3:] * 50  # radians to mm\n\n    # Absolute derivatives\n    dtrans = np.abs(np.diff(trans, axis=0))\n    drot = np.abs(np.diff(rot, axis=0))\n\n    # Sum\n    fd = np.sum(dtrans, axis=1) + np.sum(drot, axis=1)\n\n    return fd\n\n# Recommended threshold: FD &lt; 0.5mm for resting-state\nmean_fd = fd.mean()\nprint(f\"Mean FD: {mean_fd:.3f} mm\")\n\n# Exclude high-motion timepoints (scrubbing)\nlow_motion_frames = fd &lt; 0.5\nclean_timeseries = timeseries[low_motion_frames]\n</code></pre>"},{"location":"integration/modality_features/fmri/#2-temporal-snr-tsnr","title":"2. Temporal SNR (tSNR)","text":"<pre><code>def temporal_snr(timeseries):\n    \"\"\"\n    Temporal signal-to-noise ratio.\n\n    Args:\n        timeseries: (n_timepoints, n_voxels)\n\n    Returns:\n        tsnr: (n_voxels,) - temporal SNR per voxel\n    \"\"\"\n    mean_signal = timeseries.mean(axis=0)\n    std_signal = timeseries.std(axis=0)\n\n    tsnr = mean_signal / (std_signal + 1e-8)\n\n    return tsnr\n\ntsnr = temporal_snr(timeseries)\nprint(f\"Mean tSNR: {tsnr.mean():.2f}\")\n\n# Typical values: 50-100 for 3T, higher for 7T\n</code></pre>"},{"location":"integration/modality_features/fmri/#3-data-completeness","title":"3. Data Completeness","text":"<pre><code># Check for missing data\nn_nan = np.isnan(timeseries).sum()\ncompleteness = 1 - (n_nan / timeseries.size)\nprint(f\"Data completeness: {completeness*100:.1f}%\")\n\n# Minimum recommended: 95% completeness\nassert completeness &gt; 0.95, \"Too much missing data\"\n</code></pre>"},{"location":"integration/modality_features/fmri/#data-augmentation-for-robustness-testing","title":"Data Augmentation for Robustness Testing","text":"<p>See our robustness testing documentation for details on perturbations:</p> <pre><code>from fmbench.robustness import (\n    ChannelDropout,\n    GaussianNoise,\n    LineNoise,\n    TemporalShift\n)\n\n# Example: Add Gaussian noise\nnoise_probe = GaussianNoise(snr_db=10)\nnoisy_data = noise_probe.apply(timeseries)\n</code></pre>"},{"location":"integration/modality_features/fmri/#example-data-loading","title":"Example Data Loading","text":""},{"location":"integration/modality_features/fmri/#from-nifti","title":"From NIfTI","text":"<pre><code>import nibabel as nib\nfrom nilearn.maskers import NiftiMasker\n\n# Load functional image\nfunc_img = nib.load('subject_001_bold.nii.gz')\n\n# Apply brain mask and extract timeseries\nmasker = NiftiMasker(\n    standardize=True,\n    detrend=True,\n    low_pass=0.1,\n    high_pass=0.01,\n    t_r=2.0\n)\ntimeseries = masker.fit_transform(func_img)\n# Shape: (n_timepoints, n_voxels)\n</code></pre>"},{"location":"integration/modality_features/fmri/#from-parcellated-csv","title":"From Parcellated CSV","text":"<pre><code>import pandas as pd\n\n# Load ROI timeseries\ndf = pd.read_csv('subject_001_schaefer400.csv')\n# Columns: timepoint, region_1, region_2, ..., region_400\n\ntimeseries = df.iloc[:, 1:].values  # Exclude timepoint column\n# Shape: (n_timepoints, 400)\n</code></pre>"},{"location":"integration/modality_features/fmri/#from-hcp-style-data","title":"From HCP-style Data","text":"<pre><code># HCP stores timeseries as CIFTI (cortical surface + subcortical)\nfrom nibabel import cifti2\n\ncifti_img = cifti2.load('subject_REST_LR.dtseries.nii')\ntimeseries = cifti_img.get_fdata()\n# Shape: (n_timepoints, 91282) - 91k grayordinates\n</code></pre>"},{"location":"integration/modality_features/fmri/#benchmark-tasks","title":"Benchmark Tasks","text":""},{"location":"integration/modality_features/fmri/#1-classification","title":"1. Classification","text":"<p>Typical tasks: - Disease vs. Control (AD, ADHD, ASD, schizophrenia) - Cognitive state classification - Task decoding</p> <p>Input: <code>(n_samples, n_timepoints, n_regions)</code> or connectivity matrices Output: Class labels <code>(n_samples,)</code></p>"},{"location":"integration/modality_features/fmri/#2-reconstruction","title":"2. Reconstruction","text":"<p>Typical tasks: - Masked autoencoding (predict masked timepoints/regions) - Denoising - Super-resolution (spatial or temporal)</p> <p>Input: Masked/noisy timeseries Output: Clean timeseries</p>"},{"location":"integration/modality_features/fmri/#3-regression","title":"3. Regression","text":"<p>Typical tasks: - Cognitive score prediction - Age prediction - Symptom severity prediction</p> <p>Input: Timeseries Output: Continuous values <code>(n_samples,)</code></p>"},{"location":"integration/modality_features/fmri/#itu-ai4h-alignment","title":"ITU AI4H Alignment","text":"<p>This specification aligns with:</p> <ul> <li>DEL10.8 Section 3.1: Input data specifications for neurology</li> <li>DEL3 Section 4.2: Data format requirements</li> <li>DEL0.1: Standardized terminology (BOLD, fMRI, parcellation)</li> </ul>"},{"location":"integration/modality_features/fmri/#tools-libraries","title":"Tools &amp; Libraries","text":""},{"location":"integration/modality_features/fmri/#preprocessing","title":"Preprocessing","text":"<ul> <li>fMRIPrep: Robust preprocessing pipeline</li> <li>CONN Toolbox: Connectivity preprocessing</li> <li>DPARSF: Data Processing Assistant for Resting-State fMRI</li> </ul>"},{"location":"integration/modality_features/fmri/#analysis","title":"Analysis","text":"<ul> <li>Nilearn: Machine learning for neuroimaging</li> <li>NiBabel: Read/write neuroimaging formats</li> <li>BrainIAK: Brain Imaging Analysis Kit</li> </ul>"},{"location":"integration/modality_features/fmri/#parcellation","title":"Parcellation","text":"<ul> <li>Schaefer2018: <code>nilearn.datasets.fetch_atlas_schaefer_2018()</code></li> <li>AAL3: <code>nilearn.datasets.fetch_atlas_aal()</code></li> </ul>"},{"location":"integration/modality_features/fmri/#references","title":"References","text":"<ol> <li>Esteban, O., et al. (2019). fMRIPrep: a robust preprocessing pipeline for fMRI data. Nature Methods, 16(1), 111-116.</li> <li>Power, J. D., et al. (2012). Spurious but systematic correlations in functional connectivity MRI. NeuroImage, 59(3), 2142-2154.</li> <li>Schaefer, A., et al. (2018). Local-Global Parcellation of the Human Cerebral Cortex. Cerebral Cortex, 28(9), 3095-3114.</li> </ol>"},{"location":"integration/modality_features/fmri/#related-documentation","title":"Related Documentation","text":"<ul> <li>sMRI Specifications</li> <li>Genomics Specifications</li> <li>Robustness Testing</li> </ul>"},{"location":"integration/modality_features/genomics/","title":"Genomics Data Specifications","text":""},{"location":"integration/modality_features/genomics/#overview","title":"Overview","text":"<p>This page defines standardized data formats and preprocessing requirements for genomics data in the benchmark hub, covering single-cell RNA-seq (scRNA-seq), bulk RNA-seq, DNA sequences, and variant data.</p>"},{"location":"integration/modality_features/genomics/#data-modalities","title":"Data Modalities","text":""},{"location":"integration/modality_features/genomics/#1-single-cell-rna-seq-scrna-seq","title":"1. Single-Cell RNA-seq (scRNA-seq)","text":"<p>Format: AnnData (<code>.h5ad</code>), Loom (<code>.loom</code>), or CSV/TSV matrices Shape: <code>(n_cells, n_genes)</code> Data Type: Raw counts (integer) or normalized (float)</p> <pre><code>import scanpy as sc\nimport numpy as np\n\n# Load single-cell data\nadata = sc.read_h5ad('pbmc_68k.h5ad')\n\n# Inspect\nprint(f\"Shape: {adata.shape}\")  # (n_cells, n_genes)\nprint(f\"Genes: {adata.var_names[:5]}\")\nprint(f\"Cells: {adata.obs_names[:5]}\")\n\n# Access count matrix\nX = adata.X  # Sparse or dense matrix (n_cells, n_genes)\n</code></pre>"},{"location":"integration/modality_features/genomics/#required-metadata-adataobs","title":"Required Metadata (adata.obs)","text":"<pre><code># Cell-level metadata\nadata.obs['cell_type']  # Cell type annotations\nadata.obs['donor_id']  # Subject/donor identifier\nadata.obs['batch']  # Batch/experiment ID\nadata.obs['n_genes']  # Number of detected genes\nadata.obs['n_counts']  # Total UMI counts\nadata.obs['percent_mito']  # % mitochondrial gene expression\n</code></pre>"},{"location":"integration/modality_features/genomics/#gene-metadata-adatavar","title":"Gene Metadata (adata.var)","text":"<pre><code># Gene-level metadata\nadata.var['gene_ids']  # Ensembl IDs\nadata.var['gene_symbols']  # HGNC symbols\nadata.var['highly_variable']  # Boolean for HVG selection\nadata.var['mean_counts']  # Mean expression\nadata.var['dispersions']  # Dispersion (variability)\n</code></pre>"},{"location":"integration/modality_features/genomics/#2-bulk-rna-seq","title":"2. Bulk RNA-seq","text":"<p>Format: NumPy array, CSV, or TSV Shape: <code>(n_samples, n_genes)</code> Data Type: Raw counts, TPM, or FPKM</p> <pre><code>import pandas as pd\n\n# Load bulk RNA-seq data\ngene_expression = pd.read_csv('bulk_rnaseq_tpm.csv', index_col=0)\n# Rows: genes, Columns: samples\n\n# Transpose to (samples, genes) for ML\nX = gene_expression.T.values  # (n_samples, n_genes)\n</code></pre>"},{"location":"integration/modality_features/genomics/#3-dna-sequences","title":"3. DNA Sequences","text":"<p>Format: FASTA (<code>.fasta</code>, <code>.fa</code>), plain text, or tokenized arrays Alphabet: <code>{A, C, G, T, N}</code> (N = ambiguous)</p> <pre><code>from Bio import SeqIO\n\n# Load FASTA file\nsequences = []\nfor record in SeqIO.parse(\"sequences.fasta\", \"fasta\"):\n    sequences.append(str(record.seq))\n\n# Tokenize sequences (for transformer models)\nvocab = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 4, '&lt;PAD&gt;': 5}\n\ndef tokenize_sequence(seq, vocab, max_len=512):\n    \"\"\"Convert DNA sequence to token IDs.\"\"\"\n    tokens = [vocab.get(base, vocab['N']) for base in seq.upper()]\n\n    # Pad or truncate\n    if len(tokens) &lt; max_len:\n        tokens += [vocab['&lt;PAD&gt;']] * (max_len - len(tokens))\n    else:\n        tokens = tokens[:max_len]\n\n    return np.array(tokens)\n\ntokenized = [tokenize_sequence(seq, vocab) for seq in sequences]\nX_dna = np.array(tokenized)  # (n_sequences, max_len)\n</code></pre>"},{"location":"integration/modality_features/genomics/#4-variant-data-vcf","title":"4. Variant Data (VCF)","text":"<p>Format: VCF (Variant Call Format) Use: SNP arrays, whole-genome sequencing (WGS), whole-exome sequencing (WES)</p> <pre><code>import pandas as pd\nimport allel\n\n# Load VCF file\nvcf_path = 'cohort_variants.vcf.gz'\ncallset = allel.read_vcf(vcf_path)\n\n# Extract genotypes\ngenotypes = callset['calldata/GT']  # (n_variants, n_samples, ploidy=2)\n\n# Convert to dosage (0, 1, 2 for diploid)\ndosage = genotypes.sum(axis=-1)  # (n_variants, n_samples)\n\n# Transpose for ML: (samples, variants)\nX_geno = dosage.T\n</code></pre>"},{"location":"integration/modality_features/genomics/#preprocessing-workflows","title":"Preprocessing Workflows","text":""},{"location":"integration/modality_features/genomics/#scrna-seq-standard-preprocessing","title":"scRNA-seq Standard Preprocessing","text":"<pre><code>import scanpy as sc\n\n# Load raw counts\nadata = sc.read_h5ad('raw_counts.h5ad')\n\n# 1. Quality control\nsc.pp.filter_cells(adata, min_genes=200)  # Remove low-quality cells\nsc.pp.filter_genes(adata, min_cells=3)  # Remove rare genes\n\n# Calculate QC metrics\nadata.var['mt'] = adata.var_names.str.startswith('MT-')\nsc.pp.calculate_qc_metrics(\n    adata, \n    qc_vars=['mt'], \n    percent_top=None, \n    log1p=False, \n    inplace=True\n)\n\n# Filter cells by QC\nadata = adata[adata.obs.n_genes_by_counts &lt; 2500, :]\nadata = adata[adata.obs.pct_counts_mt &lt; 5, :]\n\n# 2. Normalization\nsc.pp.normalize_total(adata, target_sum=1e4)  # CPM-like\nsc.pp.log1p(adata)  # Log-transform\n\n# 3. Feature selection\nsc.pp.highly_variable_genes(adata, n_top_genes=2000)\nadata = adata[:, adata.var.highly_variable]  # Keep only HVGs\n\n# 4. Scaling (for PCA/visualization, not always for ML)\nsc.pp.scale(adata, max_value=10)\n\n# Extract preprocessed data\nX_processed = adata.X  # (n_cells, 2000)\n</code></pre>"},{"location":"integration/modality_features/genomics/#bulk-rna-seq-preprocessing","title":"Bulk RNA-seq Preprocessing","text":"<pre><code>from sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Load TPM or FPKM data\nX = gene_expression.values  # (n_samples, n_genes)\n\n# 1. Log-transform (if not already)\nX_log = np.log1p(X)\n\n# 2. Filter low-expression genes\ngene_means = X_log.mean(axis=0)\nhigh_expr_genes = gene_means &gt; 1.0  # Threshold\nX_filtered = X_log[:, high_expr_genes]\n\n# 3. Z-score normalization (optional)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_filtered)\n</code></pre>"},{"location":"integration/modality_features/genomics/#dna-sequence-preprocessing","title":"DNA Sequence Preprocessing","text":"<pre><code># K-mer encoding (alternative to tokenization)\ndef generate_kmers(sequence, k=3):\n    \"\"\"Generate k-mer features from DNA sequence.\"\"\"\n    kmers = []\n    for i in range(len(sequence) - k + 1):\n        kmers.append(sequence[i:i+k])\n    return kmers\n\n# Build k-mer vocabulary\nfrom collections import Counter\n\nall_kmers = []\nfor seq in sequences:\n    all_kmers.extend(generate_kmers(seq, k=3))\n\nkmer_vocab = {kmer: i for i, kmer in enumerate(set(all_kmers))}\n\n# Encode sequence as k-mer counts\ndef encode_kmers(sequence, kmer_vocab, k=3):\n    \"\"\"Encode sequence as k-mer count vector.\"\"\"\n    kmers = generate_kmers(sequence, k)\n    counts = Counter(kmers)\n\n    vec = np.zeros(len(kmer_vocab))\n    for kmer, count in counts.items():\n        if kmer in kmer_vocab:\n            vec[kmer_vocab[kmer]] = count\n\n    return vec\n</code></pre>"},{"location":"integration/modality_features/genomics/#data-augmentation-for-scrna-seq","title":"Data Augmentation for scRNA-seq","text":""},{"location":"integration/modality_features/genomics/#1-poisson-sampling-simulates-sequencing-depth-variation","title":"1. Poisson Sampling (Simulates sequencing depth variation)","text":"<pre><code>def poisson_augment(counts, factor=0.8):\n    \"\"\"\n    Simulate different sequencing depths via Poisson sampling.\n\n    Args:\n        counts: (n_cells, n_genes) - raw count matrix\n        factor: Downsampling factor (0 &lt; factor &lt; 1)\n\n    Returns:\n        augmented: Downsampled count matrix\n    \"\"\"\n    total_counts = counts.sum(axis=1, keepdims=True)\n    target_counts = total_counts * factor\n\n    # Multinomial sampling\n    probs = counts / (total_counts + 1e-8)\n    augmented = np.array([\n        np.random.multinomial(int(tc), p)\n        for tc, p in zip(target_counts, probs)\n    ])\n\n    return augmented\n</code></pre>"},{"location":"integration/modality_features/genomics/#2-cell-mixtures-simulates-doublets","title":"2. Cell Mixtures (Simulates doublets)","text":"<pre><code>def create_doublet(cell1, cell2):\n    \"\"\"Simulate doublet by averaging two cells.\"\"\"\n    return (cell1 + cell2) / 2\n</code></pre>"},{"location":"integration/modality_features/genomics/#benchmark-tasks","title":"Benchmark Tasks","text":""},{"location":"integration/modality_features/genomics/#1-cell-type-annotation-scrna-seq","title":"1. Cell Type Annotation (scRNA-seq)","text":"<p>Input: Gene expression matrix <code>(n_cells, n_genes)</code> Output: Cell type labels <code>(n_cells,)</code></p> <pre><code># Example: PBMC cell type classification\nX_train = adata_train.X  # (n_train, n_genes)\ny_train = adata_train.obs['cell_type'].values  # (n_train,)\n\n# Labels: B cells, T cells, Monocytes, NK cells, etc.\n</code></pre>"},{"location":"integration/modality_features/genomics/#2-gene-expression-prediction","title":"2. Gene Expression Prediction","text":"<p>Input: Partial gene expression or other modalities Output: Full gene expression profiles</p>"},{"location":"integration/modality_features/genomics/#3-variant-effect-prediction","title":"3. Variant Effect Prediction","text":"<p>Input: DNA sequences with variants Output: Pathogenicity scores or functional effects</p>"},{"location":"integration/modality_features/genomics/#4-sequence-classification","title":"4. Sequence Classification","text":"<p>Input: DNA sequences Output: Labels (e.g., promoter vs non-promoter, coding vs non-coding)</p>"},{"location":"integration/modality_features/genomics/#quality-control-metrics","title":"Quality Control Metrics","text":""},{"location":"integration/modality_features/genomics/#scrna-seq-qc","title":"scRNA-seq QC","text":"<pre><code># Check key QC metrics\nprint(f\"Mean genes/cell: {adata.obs['n_genes'].mean():.0f}\")\nprint(f\"Mean UMIs/cell: {adata.obs['n_counts'].mean():.0f}\")\nprint(f\"Mean % mito: {adata.obs['percent_mito'].mean():.2f}%\")\n\n# Recommended thresholds:\n# - n_genes: 200 - 5000\n# - percent_mito: &lt; 5-10%\n# - n_counts: &gt; 500\n</code></pre>"},{"location":"integration/modality_features/genomics/#variant-data-qc","title":"Variant Data QC","text":"<pre><code># Check missing rate\nmissing_rate = (genotypes == -1).mean()\nprint(f\"Missing genotype rate: {missing_rate*100:.2f}%\")\n\n# Filter variants by minor allele frequency (MAF)\nfrom allel import GenotypeArray\n\ngt = GenotypeArray(genotypes)\nallele_counts = gt.count_alleles()\nmaf = allele_counts[:, 1] / allele_counts.sum(axis=1)\n\n# Keep MAF &gt; 0.01\nkeep_variants = maf &gt; 0.01\n</code></pre>"},{"location":"integration/modality_features/genomics/#data-formats","title":"Data Formats","text":""},{"location":"integration/modality_features/genomics/#anndata-structure","title":"AnnData Structure","text":"<pre><code># Recommended AnnData structure for benchmarking\nadata.X  # Main data matrix (can be sparse)\nadata.obs  # Cell/sample metadata (pandas DataFrame)\nadata.var  # Gene/feature metadata (pandas DataFrame)\nadata.layers  # Alternative representations (raw, scaled, etc.)\nadata.obsm  # Multi-dimensional annotations (PCA, UMAP)\nadata.varm  # Multi-dimensional gene annotations\nadata.uns  # Unstructured metadata (dataset info)\n</code></pre>"},{"location":"integration/modality_features/genomics/#example-dataset-metadata","title":"Example Dataset Metadata","text":"<pre><code>dataset_id: pbmc_68k\nname: PBMC 68k (10x Genomics)\nmodality: scRNA-seq\nn_cells: 68579\nn_genes: 20387\ntechnology: 10x Chromium v2\norganism: Homo sapiens\ntissue: Peripheral blood mononuclear cells (PBMC)\npreprocessing: Scanpy 1.9.1\nnormalization: log1p(CPM)\nfeature_selection: top_2000_HVG\nreference: Zheng et al. (2017)\n</code></pre>"},{"location":"integration/modality_features/genomics/#foundation-model-input-formats","title":"Foundation Model Input Formats","text":""},{"location":"integration/modality_features/genomics/#for-geneformer-and-similar-models","title":"For Geneformer and Similar Models","text":"<pre><code># Geneformer expects rank-ordered gene tokens\ndef prepare_geneformer_input(adata, n_genes=2048):\n    \"\"\"\n    Convert AnnData to Geneformer input format.\n\n    Returns ranked gene indices per cell.\n    \"\"\"\n    # Rank genes by expression within each cell\n    cell_inputs = []\n\n    for i in range(adata.n_obs):\n        cell_expr = adata.X[i].toarray().ravel()\n\n        # Rank genes (descending)\n        ranked_genes = np.argsort(cell_expr)[::-1][:n_genes]\n\n        cell_inputs.append(ranked_genes)\n\n    return np.array(cell_inputs)  # (n_cells, n_genes)\n</code></pre>"},{"location":"integration/modality_features/genomics/#for-dna-sequence-models-dnabert-hyenadna-etc","title":"For DNA Sequence Models (DNABERT, HyenaDNA, etc.)","text":"<pre><code># K-mer tokenization for DNABERT\ndef kmer_tokenize(sequence, k=6, stride=1):\n    \"\"\"\n    Tokenize DNA sequence into k-mers.\n\n    Args:\n        sequence: DNA string\n        k: K-mer size\n        stride: Step size\n\n    Returns:\n        tokens: List of k-mer strings\n    \"\"\"\n    tokens = []\n    for i in range(0, len(sequence) - k + 1, stride):\n        tokens.append(sequence[i:i+k])\n\n    return tokens\n\n# Example\nseq = \"ATCGATCGATCG\"\ntokens = kmer_tokenize(seq, k=6, stride=3)\nprint(tokens)  # ['ATCGAT', 'GATCGA', 'TCGATC']\n</code></pre>"},{"location":"integration/modality_features/genomics/#itu-ai4h-alignment","title":"ITU AI4H Alignment","text":"<p>This specification aligns with:</p> <ul> <li>DEL3 Section 4.2: Data format requirements for AI4H benchmarks</li> <li>DEL0.1: Genomics terminology standardization</li> <li>FAIR principles: Findable, Accessible, Interoperable, Reusable data</li> </ul>"},{"location":"integration/modality_features/genomics/#tools-libraries","title":"Tools &amp; Libraries","text":""},{"location":"integration/modality_features/genomics/#scrna-seq","title":"scRNA-seq","text":"<ul> <li>Scanpy: Single-cell analysis in Python</li> <li>Seurat: R toolkit for scRNA-seq</li> <li>scVI-tools: Probabilistic models for single-cell omics</li> </ul>"},{"location":"integration/modality_features/genomics/#genomics","title":"Genomics","text":"<ul> <li>BioPython: Sequence analysis</li> <li>PyVCF / scikit-allel: Variant data handling</li> <li>pybedtools: Genomic interval operations</li> </ul>"},{"location":"integration/modality_features/genomics/#foundation-model-libraries","title":"Foundation Model Libraries","text":"<ul> <li>Geneformer: Transformer for scRNA-seq</li> <li>DNABERT: BERT for DNA sequences</li> <li>Nucleotide Transformer: Multi-species genomic foundation model</li> </ul>"},{"location":"integration/modality_features/genomics/#references","title":"References","text":"<ol> <li>Wolf, F. A., et al. (2018). SCANPY: large-scale single-cell gene expression data analysis. Genome Biology, 19(1), 15.</li> <li>Theodoris, C. V., et al. (2023). Transfer learning enables predictions in network biology. Nature, 618, 616-624. (Geneformer)</li> <li>Ji, Y., et al. (2021). DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome. Bioinformatics, 37(15), 2112-2120.</li> </ol>"},{"location":"integration/modality_features/genomics/#related-documentation","title":"Related Documentation","text":"<ul> <li>fMRI Specifications</li> <li>sMRI Specifications</li> <li>Prediction Baselines</li> </ul>"},{"location":"integration/modality_features/smri/","title":"sMRI Data Specifications","text":""},{"location":"integration/modality_features/smri/#overview","title":"Overview","text":"<p>Structural MRI (sMRI) captures anatomical brain structure, typically using T1-weighted or T2-weighted sequences. This page defines standardized data formats and preprocessing requirements for sMRI data in this benchmark hub.</p>"},{"location":"integration/modality_features/smri/#data-format-requirements","title":"Data Format Requirements","text":""},{"location":"integration/modality_features/smri/#1-raw-structural-mri","title":"1. Raw Structural MRI","text":"<p>Format: NIfTI (<code>.nii</code> or <code>.nii.gz</code>) Typical Shape: <code>(256, 256, 256)</code> at 1mm\u00b3 isotropic resolution Data Type: <code>float32</code> or <code>int16</code> Contrast: T1-weighted (most common), T2-weighted, FLAIR, or multi-contrast</p> <pre><code>import nibabel as nib\nimport numpy as np\n\n# Load T1-weighted image\nimg = nib.load('subject_001_T1w.nii.gz')\ndata = img.get_fdata()  # Shape: (x, y, z)\n\nprint(f\"Image shape: {data.shape}\")\nprint(f\"Voxel size: {img.header.get_zooms()[:3]} mm\")\n</code></pre>"},{"location":"integration/modality_features/smri/#2-preprocessed-structural-images","title":"2. Preprocessed Structural Images","text":"<p>Minimum preprocessing: 1. \u2705 Skull stripping / brain extraction 2. \u2705 Bias field correction (inhomogeneity correction) 3. \u2705 Spatial normalization to standard space (MNI152) 4. \u2705 Segmentation (GM, WM, CSF)</p> <p>Optional: - Denoising - Resolution standardization - Intensity normalization</p> <pre><code># Example: Load preprocessed brain-extracted image\nbrain_img = nib.load('subject_001_T1w_brain.nii.gz')\nbrain_data = brain_img.get_fdata()\n</code></pre>"},{"location":"integration/modality_features/smri/#3-derived-features","title":"3. Derived Features","text":""},{"location":"integration/modality_features/smri/#a-tissue-segmentation-maps","title":"A. Tissue Segmentation Maps","text":"<p>Format: NIfTI (probability maps or binary masks) Channels: Gray matter (GM), White matter (WM), CSF</p> <pre><code># Load tissue probability maps from FreeSurfer/FSL/SPM\ngm_prob = nib.load('subject_001_GM_prob.nii.gz').get_fdata()\nwm_prob = nib.load('subject_001_WM_prob.nii.gz').get_fdata()\ncsf_prob = nib.load('subject_001_CSF_prob.nii.gz').get_fdata()\n\n# Stack as multi-channel input (for CNNs)\ntissue_stack = np.stack([gm_prob, wm_prob, csf_prob], axis=-1)\n# Shape: (x, y, z, 3)\n</code></pre>"},{"location":"integration/modality_features/smri/#b-freesurfer-derived-metrics","title":"B. FreeSurfer-Derived Metrics","text":"<p>Format: CSV or NumPy array Features: Regional volumes, cortical thickness, surface area</p> <pre><code>import pandas as pd\n\n# Load FreeSurfer stats\nfs_stats = pd.read_csv('subject_001_aparc_stats.csv')\n# Columns: region, volume_mm3, thickness_mm, surface_area_mm2\n\n# Extract feature vector\nfeatures = fs_stats[['volume_mm3', 'thickness_mm']].values.flatten()\n# Shape: (n_regions * 2,)\n</code></pre> <p>Common FreeSurfer outputs: - <code>aseg.stats</code>: Subcortical volumes (hippocampus, amygdala, etc.) - <code>aparc.stats</code>: Cortical parcellation (Desikan-Killiany atlas, 68 regions) - <code>aparc.a2009s.stats</code>: Destrieux atlas (148 regions)</p>"},{"location":"integration/modality_features/smri/#c-voxel-based-morphometry-vbm","title":"C. Voxel-Based Morphometry (VBM)","text":"<p>Format: NIfTI (modulated GM/WM maps) Use case: Voxel-wise comparison of tissue density</p> <pre><code># VBM: Modulated, smoothed gray matter\nvbm_gm = nib.load('subject_001_VBM_GM.nii.gz').get_fdata()\n# Typically smoothed with 8mm FWHM Gaussian kernel\n</code></pre>"},{"location":"integration/modality_features/smri/#metadata-requirements","title":"Metadata Requirements","text":"<pre><code>dataset_id: adni_smri\nname: ADNI Structural MRI\nmodality: sMRI\nsequence: T1-weighted\nn_subjects: 1200\nresolution: [1.0, 1.0, 1.0]  # mm (x, y, z)\nfield_strength: 3T  # or 1.5T, 7T\nmanufacturer: Siemens  # or GE, Philips\npreprocessing: FreeSurfer 7.2.0  # or CAT12, FSL, ANTs\nstandard_space: MNI152NLin2009cAsym\nsegmentation_method: FreeSurfer\natlas: Desikan-Killiany  # or Destrieux, AAL, etc.\n</code></pre>"},{"location":"integration/modality_features/smri/#quality-control-metrics","title":"Quality Control Metrics","text":""},{"location":"integration/modality_features/smri/#1-image-quality-assessment","title":"1. Image Quality Assessment","text":"<pre><code>def compute_snr(img_data, brain_mask):\n    \"\"\"\n    Signal-to-noise ratio for structural MRI.\n\n    Args:\n        img_data: (x, y, z) - T1w image\n        brain_mask: (x, y, z) - binary brain mask\n\n    Returns:\n        snr: Signal-to-noise ratio\n    \"\"\"\n    brain_signal = img_data[brain_mask &gt; 0]\n\n    # Estimate noise from background (air)\n    background = img_data[brain_mask == 0]\n    noise_std = background.std()\n\n    snr = brain_signal.mean() / (noise_std + 1e-8)\n\n    return snr\n\n# Typical SNR for 3T: 20-40\nsnr = compute_snr(data, brain_mask)\nprint(f\"SNR: {snr:.2f}\")\n</code></pre>"},{"location":"integration/modality_features/smri/#2-contrast-to-noise-ratio-cnr","title":"2. Contrast-to-Noise Ratio (CNR)","text":"<pre><code>def compute_cnr(img_data, gm_mask, wm_mask):\n    \"\"\"\n    Contrast-to-noise ratio between gray and white matter.\n    \"\"\"\n    gm_signal = img_data[gm_mask &gt; 0].mean()\n    wm_signal = img_data[wm_mask &gt; 0].mean()\n\n    noise_std = img_data[gm_mask &gt; 0].std()\n\n    cnr = abs(gm_signal - wm_signal) / (noise_std + 1e-8)\n\n    return cnr\n\ncnr = compute_cnr(data, gm_mask, wm_mask)\nprint(f\"CNR (GM-WM): {cnr:.2f}\")\n</code></pre>"},{"location":"integration/modality_features/smri/#3-freesurfer-qc-metrics","title":"3. FreeSurfer QC Metrics","text":"<pre><code># Check FreeSurfer reconstruction quality\ndef check_freesurfer_qc(subjects_dir, subject_id):\n    \"\"\"\n    Check FreeSurfer quality control metrics.\n    \"\"\"\n    # Euler number (lower is better, typically &lt; -50)\n    euler_path = f\"{subjects_dir}/{subject_id}/stats/lh.aparc.stats\"\n\n    # Mean cortical thickness (typical: 2.0-3.0 mm)\n    # Total brain volume (typical: 1000-1500 cm\u00b3)\n\n    # Flag for manual inspection if outliers\n    pass\n</code></pre>"},{"location":"integration/modality_features/smri/#data-normalization","title":"Data Normalization","text":""},{"location":"integration/modality_features/smri/#1-intensity-normalization","title":"1. Intensity Normalization","text":"<pre><code>from sklearn.preprocessing import StandardScaler\n\ndef normalize_intensity(img_data, brain_mask):\n    \"\"\"\n    Z-score normalization within brain mask.\n    \"\"\"\n    brain_voxels = img_data[brain_mask &gt; 0]\n\n    mean = brain_voxels.mean()\n    std = brain_voxels.std()\n\n    img_normalized = (img_data - mean) / (std + 1e-8)\n    img_normalized[brain_mask == 0] = 0  # Keep background at 0\n\n    return img_normalized\n\nnormalized = normalize_intensity(data, brain_mask)\n</code></pre>"},{"location":"integration/modality_features/smri/#2-total-intracranial-volume-tiv-correction","title":"2. Total Intracranial Volume (TIV) Correction","text":"<pre><code>def tiv_correction(regional_volumes, tiv):\n    \"\"\"\n    Correct regional volumes for head size.\n\n    Args:\n        regional_volumes: (n_regions,) - volumes in mm\u00b3\n        tiv: Total intracranial volume in mm\u00b3\n\n    Returns:\n        corrected_volumes: TIV-corrected volumes\n    \"\"\"\n    # Method 1: Proportional scaling\n    corrected = (regional_volumes / tiv) * 1500000  # Scale to 1.5L\n\n    # Method 2: Residuals after regression (preferred)\n    from sklearn.linear_model import LinearRegression\n    lr = LinearRegression()\n    lr.fit(tiv.reshape(-1, 1), regional_volumes.reshape(-1, 1))\n    residuals = regional_volumes - lr.predict(tiv.reshape(-1, 1)).ravel()\n\n    return residuals\n\n# Load TIV from FreeSurfer\n# Typically in aseg.stats: \"Estimated Total Intracranial Volume\"\ntiv = 1450000  # mm\u00b3\n\nvolumes_corrected = tiv_correction(regional_volumes, tiv)\n</code></pre>"},{"location":"integration/modality_features/smri/#data-augmentation","title":"Data Augmentation","text":""},{"location":"integration/modality_features/smri/#for-deep-learning-models","title":"For Deep Learning Models","text":"<pre><code>import torchio as tio\n\n# Define augmentation pipeline\ntransforms = tio.Compose([\n    tio.RandomAffine(\n        scales=(0.9, 1.1),\n        degrees=10,\n        translation=5,\n        p=0.75\n    ),\n    tio.RandomFlip(axes=('LR',), p=0.5),  # Left-Right flip\n    tio.RandomNoise(std=(0, 0.1), p=0.25),\n    tio.RandomBiasField(coefficients=0.5, p=0.25),\n])\n\n# Apply to image\nsubject = tio.Subject(\n    t1=tio.ScalarImage('subject_001_T1w.nii.gz'),\n)\naugmented = transforms(subject)\naugmented_data = augmented.t1.data  # Shape: (1, x, y, z)\n</code></pre>"},{"location":"integration/modality_features/smri/#benchmark-tasks","title":"Benchmark Tasks","text":""},{"location":"integration/modality_features/smri/#1-classification","title":"1. Classification","text":"<p>Typical tasks: - Alzheimer's Disease (AD) vs. Cognitively Normal (CN) - Brain tumor classification - Age group classification - Sex classification</p> <p>Input: T1w images <code>(x, y, z)</code> or derived features Output: Class labels</p> <pre><code># Example: Binary classification (AD vs CN)\nX_train = load_t1w_images(train_ids)  # (n_train, x, y, z)\ny_train = load_labels(train_ids)  # (n_train,) - {0, 1}\n</code></pre>"},{"location":"integration/modality_features/smri/#2-regression","title":"2. Regression","text":"<p>Typical tasks: - Age prediction (brain age) - Cognitive score prediction (MMSE, MoCA) - Disease severity prediction</p> <p>Input: T1w images Output: Continuous values</p> <pre><code># Example: Brain age prediction\nX = load_t1w_images(subject_ids)  # (n, x, y, z)\ny_age = load_ages(subject_ids)  # (n,) - chronological age in years\n</code></pre>"},{"location":"integration/modality_features/smri/#3-segmentation","title":"3. Segmentation","text":"<p>Typical tasks: - Tissue segmentation (GM, WM, CSF) - Lesion segmentation (MS, stroke, tumor) - Hippocampal subfield segmentation</p> <p>Input: T1w images Output: Segmentation masks <code>(x, y, z)</code> or <code>(x, y, z, n_classes)</code></p>"},{"location":"integration/modality_features/smri/#4-reconstructiondenoising","title":"4. Reconstruction/Denoising","text":"<p>Input: Low-quality or partial images Output: High-quality reconstructed images</p>"},{"location":"integration/modality_features/smri/#example-data-loading","title":"Example Data Loading","text":""},{"location":"integration/modality_features/smri/#using-nilearn","title":"Using Nilearn","text":"<pre><code>from nilearn import datasets, plotting\nfrom nilearn.image import resample_to_img\n\n# Load example T1w image\nt1_img = nib.load('subject_001_T1w.nii.gz')\n\n# Resample to standard resolution (e.g., 2mm isotropic)\nfrom nilearn.image import resample_img\nt1_resampled = resample_img(\n    t1_img,\n    target_affine=np.diag([2, 2, 2]),\n    interpolation='continuous'\n)\n\n# Visualize\nplotting.plot_anat(t1_resampled)\n</code></pre>"},{"location":"integration/modality_features/smri/#loading-freesurfer-derived-features","title":"Loading FreeSurfer-Derived Features","text":"<pre><code>def load_freesurfer_features(subjects_dir, subject_id):\n    \"\"\"\n    Load FreeSurfer morphometric features.\n\n    Returns:\n        features: Dictionary with volumes, thickness, etc.\n    \"\"\"\n    import re\n\n    # Parse aparc.stats\n    stats_file = f\"{subjects_dir}/{subject_id}/stats/lh.aparc.stats\"\n\n    features = {}\n    with open(stats_file, 'r') as f:\n        for line in f:\n            if line.startswith('#'):\n                continue\n            parts = line.split()\n            if len(parts) &gt;= 5:\n                region = parts[0]\n                features[f\"lh_{region}_volume\"] = float(parts[3])\n                features[f\"lh_{region}_thickness\"] = float(parts[4])\n\n    # Repeat for right hemisphere\n    # ... (similar code for rh.aparc.stats)\n\n    return features\n</code></pre>"},{"location":"integration/modality_features/smri/#itu-ai4h-alignment","title":"ITU AI4H Alignment","text":"<p>This specification aligns with:</p> <ul> <li>DEL10.8 Section 3.1: Input data specifications for neurology</li> <li>DEL3 Section 4.2: Data format and quality requirements</li> <li>DEL0.1: Standardized neuroimaging terminology</li> </ul>"},{"location":"integration/modality_features/smri/#tools-libraries","title":"Tools &amp; Libraries","text":""},{"location":"integration/modality_features/smri/#preprocessing","title":"Preprocessing","text":"<ul> <li>FreeSurfer: Comprehensive cortical reconstruction</li> <li>FSL: FMRIB Software Library (BET, FAST, FLIRT/FNIRT)</li> <li>SPM: Statistical Parametric Mapping</li> <li>ANTs: Advanced Normalization Tools</li> <li>CAT12: Computational Anatomy Toolbox</li> </ul>"},{"location":"integration/modality_features/smri/#python-libraries","title":"Python Libraries","text":"<ul> <li>NiBabel: Read/write neuroimaging formats</li> <li>Nilearn: Machine learning for neuroimaging</li> <li>TorchIO: Medical image augmentation</li> <li>MONAI: Medical imaging deep learning</li> </ul>"},{"location":"integration/modality_features/smri/#references","title":"References","text":"<ol> <li>Fischl, B. (2012). FreeSurfer. NeuroImage, 62(2), 774-781.</li> <li>Ashburner, J., &amp; Friston, K. J. (2000). Voxel-based morphometry. NeuroImage, 11(6), 805-821.</li> <li>Klein, A., et al. (2009). Evaluation of volume-based and surface-based brain image registration methods. NeuroImage, 51(1), 214-220.</li> </ol>"},{"location":"integration/modality_features/smri/#related-documentation","title":"Related Documentation","text":"<ul> <li>fMRI Specifications</li> <li>Genomics Specifications</li> <li>Prediction Baselines</li> </ul>"},{"location":"leaderboards/","title":"\ud83c\udfc6 Foundation Model Leaderboards","text":"<p>Benchmark Hub Stats</p> <p>\ud83c\udfaf 7 Benchmarks | \ud83e\udd16 21 Models Evaluated | \ud83d\udcca 38 Total Evaluations</p> <p>Welcome to the AI4H-Inspired FM Benchmark Hub! Rankings below show all submitted models from best to developing, helping you find the right model for your use case.</p>"},{"location":"leaderboards/#quick-navigation","title":"\ud83e\udded Quick Navigation","text":"<ul> <li>\ud83c\udf10 Cross-Domain</li> <li>\ud83e\uddec Genomics</li> <li>\ud83e\udde0 Neurology</li> </ul>"},{"location":"leaderboards/#cross-domain","title":"\ud83c\udf10 Cross-Domain","text":""},{"location":"leaderboards/#clinical-report-generation-quality","title":"\ud83c\udf10 Clinical Report Generation Quality","text":"<p>\u270d\ufe0f Task: Generation | \ud83c\udfe5 Health Topic: Automated Clinical Reporting</p> <p>Clinical Relevance</p> <p>Foundation models increasingly generate clinical reports, radiology  interpretations, and patient summaries. Quality metrics must capture both linguistic fluency and clinical accuracy/safety.</p>"},{"location":"leaderboards/#leaderboard","title":"\ud83c\udfc6 Leaderboard","text":"<pre><code>         \ud83e\udd47          \n     [Me-LLaMA]    \n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \n \ud83e\udd48 \u2502         \u2502 \ud83e\udd49  \n[M3FM]\u2502         \u2502[OpenFlamin]\n\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\n</code></pre> <p>All 6 models ranked by report_quality_score:</p> Rank Model Score Performance Dataset Date \ud83e\udd47 Me-LLaMA \ud83d\udc51 0.8750 mimic_cxr_reports 2024-02-05 \ud83e\udd48 M3FM \ud83c\udf1f 0.8600 mimic_cxr_reports 2024-01-28 \ud83e\udd49 OpenFlamingo \u2728 0.8400 mimic_cxr_reports 2024-01-20 \ud83c\udfc5 TITAN 0.8100 mimic_cxr_reports 2024-01-25 \ud83c\udfc5 Med-Flamingo 0.7800 mimic_cxr_reports 2024-01-18 \ud83c\udf96\ufe0f RadBERT 0.6900 mimic_cxr_reports 2024-01-12"},{"location":"leaderboards/#ranking-explanation","title":"\ud83d\udcd6 Ranking Explanation","text":"<p>Why These Rankings?</p> <p>\ud83e\udd47 Me-LLaMA leads with report_quality_score=0.8750</p> <ul> <li>Gap to \ud83e\udd48 M3FM: +0.0150 (1.7% better)</li> <li>Score range across all models: 0.1850</li> </ul> \ud83d\udcd0 How are models scored?  ---  ### \ud83c\udfaf Primary Metric: `report_quality_score`  &gt; Composite score of linguistic fluency + clinical accuracy (0.0-1.0)  ---  ### \ud83d\udcca Metric Priority  | Priority | Metric | What it measures | |:---:|:---|:---| | 1 | `report_quality_score` | Composite clinical + linguistic quality | | 2 | `clinical_accuracy` | Correctness of medical content | | 3 | `bertscore` | Semantic similarity | | 4 | `hallucination_rate` | Safety (lower = better) |  ---  ### \ud83c\udfe5 Clinical Readiness Tiers  | Score | Tier | Deployment | Guidance | |:---:|:---:|:---:|:---| | \u2265 0.90 | \u2b50 Excellent | Production | Clinical decision support ready | | 0.80-0.89 | \u2705 Good | Pilot | Needs prospective validation | | 0.70-0.79 | \ud83d\udd36 Fair | Research | Not for patient-facing use | | &lt; 0.70 | \ud83d\udcc8 Developing | Development | Significant improvement needed |  ---  ### \ud83d\udccf Ranking Rules  1. Ranked by **primary metric** (higher = better) 2. Ties broken by secondary metrics 3. Best run per model used 4. 4 decimal precision  ---  ### \u2696\ufe0f Fairness Analysis  Models evaluated across:  | Category | Strata | |:---|:---| | \ud83d\udc64 Demographics | Age, sex, ethnicity | | \ud83d\udd2c Technical | Scanner, acquisition | | \ud83c\udfe5 Clinical | Disease stage, site |  &gt; \u26a0\ufe0f Gaps &gt;10% flagged for review  ---  *Aligned with [ITU/WHO AI4H DEL3](https://www.itu.int/pub/T-FG-AI4H) standards.*"},{"location":"leaderboards/#complete-metrics-comparison","title":"\ud83d\udccb Complete Metrics Comparison","text":"Rank Model report_quality_score clinical_accuracy bertscore bleu finding_recall hallucination_rate finding_precision flesch_kincaid \ud83e\udd47 Me-LLaMA 0.8750 0.9200 0.9000 43.5000 0.8800 0.0400 0.9300 9.5000 \ud83e\udd48 M3FM 0.8600 0.9100 0.8900 41.2000 0.8700 0.0450 0.9200 9.8000 \ud83e\udd49 OpenFlamingo 0.8400 0.8900 0.8700 38.5000 0.8500 0.0600 0.9100 10.2000 \ud83c\udfc5 TITAN 0.8100 0.8600 0.8500 35.2000 0.8200 0.0700 0.8800 10.8000 \ud83c\udfc5 Med-Flamingo 0.7800 0.8200 0.8200 32.5000 0.7900 0.0900 0.8500 11.5000 \ud83c\udf96\ufe0f RadBERT 0.6900 0.7200 0.7400 24.2000 0.6800 0.1500 0.7500 13.2000 <p>Legend</p> <p>\ud83d\udcca Primary metric: report_quality_score (bold) | \u2b50 Excellent (\u22650.9) | \u2705 Good (\u22650.8) | \ud83d\udd36 Fair (\u22650.7) | \ud83d\udcc8 Developing (&lt;0.7)</p>"},{"location":"leaderboards/#granular-performance-breakdown","title":"\ud83d\udcca Granular Performance Breakdown","text":"<p>Expand sections below to see how models perform across different conditions:</p> \ud83d\udcc4 OpenFlamingo by Report Type  | Report Type | clinical_accuracy | finding_recall | bertscore | N | |---|---|---|---|---| | \ud83e\udd47 chest_xray | 0.9100 | 0.8700 | 0.8800 | 2000 | | \ud83e\udd48 brain_mri | 0.8800 | 0.8400 | 0.8600 | 600 | | ct_abdomen | 0.8600 | 0.8200 | 0.8500 | 800 |   \ud83d\udcca OpenFlamingo by Complexity  | Complexity | clinical_accuracy | hallucination_rate | N | |---|---|---|---| | \ud83e\udd47 simple | 0.9400 | 0.0300 | 1500 | | \ud83e\udd48 moderate | 0.8800 | 0.0600 | 1200 | | complex | 0.8200 | 0.1000 | 700 |   \ud83d\udcc4 Med-Flamingo by Report Type  | Report Type | clinical_accuracy | finding_recall | bertscore | N | |---|---|---|---|---| | \ud83e\udd47 chest_xray | 0.8500 | 0.8100 | 0.8300 | 2000 | | \ud83e\udd48 brain_mri | 0.8000 | 0.7600 | 0.8000 | 600 | | ct_abdomen | 0.7800 | 0.7400 | 0.7900 | 800 |   <p>Ranked by report_quality_score (higher is better). Last updated from 6 evaluation(s).</p>"},{"location":"leaderboards/#foundation-model-robustness-evaluation","title":"\ud83c\udf10 Foundation Model Robustness Evaluation","text":"<p>\ud83d\udee1\ufe0f Task: Robustness Assessment | \ud83c\udfe5 Health Topic: Model Reliability and Artifact Resilience</p> <p>Clinical Relevance</p> <p>Clinical deployment of AI models requires robustness to real-world data variability including sensor noise, signal artifacts, and acquisition differences. This benchmark evaluates model stability under controlled perturbations that simulate common data quality issues.</p>"},{"location":"leaderboards/#leaderboard_1","title":"\ud83c\udfc6 Leaderboard","text":"<pre><code>         \ud83e\udd47          \n     [geneformer]    \n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \n \ud83e\udd48 \u2502         \u2502 \ud83e\udd49  \n[Brain-JEPA]\u2502         \u2502[BrainHarmo]\n\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\n</code></pre> <p>All 7 models ranked by robustness_score:</p> Rank Model Score Performance Dataset Date \ud83e\udd47 geneformer \ud83d\udc51 0.9995 \u2b50 Excellent - 2025-11-27 \ud83e\udd48 Brain-JEPA \ud83c\udf1f 0.8650 \u2705 Good DS-TOY-NEURO-ROBUSTNESS 2024-01-20 \ud83e\udd49 BrainHarmony \u2728 0.8450 \u2705 Good DS-TOY-NEURO-ROBUSTNESS 2024-01-18 \ud83c\udfc5 Geneformer 0.8350 \u2705 Good DS-TOY-GENOMICS 2024-01-10 \ud83c\udfc5 BrainLM 0.8250 \u2705 Good DS-TOY-NEURO-ROBUSTNESS 2024-01-16 \ud83c\udf96\ufe0f HyenaDNA 0.7950 \ud83d\udd36 Fair DS-TOY-GENOMICS 2024-01-12 \ud83c\udf96\ufe0f Baseline (Random/Majority) 0.7810 \ud83d\udd36 Fair - 2025-11-27"},{"location":"leaderboards/#ranking-explanation_1","title":"\ud83d\udcd6 Ranking Explanation","text":"<p>Why These Rankings?</p> <p>\ud83e\udd47 geneformer leads with robustness_score=0.9995</p> <ul> <li>Gap to \ud83e\udd48 Brain-JEPA: +0.1345 (15.5% better)</li> <li>Score range across all models: 0.2185</li> <li>Performance distribution: \u2b50 1 excellent, \u2705 4 good, \ud83d\udd36 2 fair</li> </ul> \ud83d\udcd0 How are models scored?  ---  ### \ud83c\udfaf Primary Metric: `robustness_score`  &gt; Average performance retention under data perturbations (0.0-1.0)  ---  ### \ud83d\udcca Metric Priority  | Priority | Metric | What it measures | |:---:|:---|:---| | 1 | `robustness_score` | Overall perturbation resilience | | 2 | `dropout_rAUC` | Performance under missing data | | 3 | `noise_rAUC` | Performance under noise | | 4 | `perm_equivariance` | Input reordering consistency |  ---  ### \ud83c\udfe5 Clinical Readiness Tiers  | Score | Tier | Deployment | Guidance | |:---:|:---:|:---:|:---| | \u2265 0.90 | \u2b50 Excellent | Production | Clinical decision support ready | | 0.80-0.89 | \u2705 Good | Pilot | Needs prospective validation | | 0.70-0.79 | \ud83d\udd36 Fair | Research | Not for patient-facing use | | &lt; 0.70 | \ud83d\udcc8 Developing | Development | Significant improvement needed |  ---  ### \ud83d\udccf Ranking Rules  1. Ranked by **primary metric** (higher = better) 2. Ties broken by secondary metrics 3. Best run per model used 4. 4 decimal precision  ---  ### \u2696\ufe0f Fairness Analysis  Models evaluated across:  | Category | Strata | |:---|:---| | \ud83d\udc64 Demographics | Age, sex, ethnicity | | \ud83d\udd2c Technical | Scanner, acquisition | | \ud83c\udfe5 Clinical | Disease stage, site |  &gt; \u26a0\ufe0f Gaps &gt;10% flagged for review  ---  *Aligned with [ITU/WHO AI4H DEL3](https://www.itu.int/pub/T-FG-AI4H) standards.*"},{"location":"leaderboards/#complete-metrics-comparison_1","title":"\ud83d\udccb Complete Metrics Comparison","text":"Rank Model robustness_score dropout_rAUC expression_rAUC line_noise_rAUC masking_rAUC noise_rAUC perm_equivariance shift_rAUC \ud83e\udd47 \u2b50 geneformer 0.9995 0.9995 - 0.9995 - 0.9995 0.9995 0.9995 \ud83e\udd48 \u2705 Brain-JEPA 0.8650 0.8800 - 0.8400 - 0.8500 0.8900 0.8650 \ud83e\udd49 \u2705 BrainHarmony 0.8450 0.8600 - 0.8200 - 0.8300 0.8700 0.8450 \ud83c\udfc5 \u2705 Geneformer 0.8350 0.8500 0.8100 - 0.8600 0.8200 0.8350 - \ud83c\udfc5 \u2705 BrainLM 0.8250 0.8400 - 0.8000 - 0.8100 0.8500 0.8250 \ud83c\udf96\ufe0f \ud83d\udd36 HyenaDNA 0.7950 0.8100 0.7700 - 0.8200 0.7800 0.8000 - \ud83c\udf96\ufe0f \ud83d\udd36 Baseline (Random/Majority) 0.7810 0.7760 - 0.7737 - 0.7867 0.7819 0.7874 <p>Legend</p> <p>\ud83d\udcca Primary metric: robustness_score (bold) | \u2b50 Excellent (\u22650.9) | \u2705 Good (\u22650.8) | \ud83d\udd36 Fair (\u22650.7) | \ud83d\udcc8 Developing (&lt;0.7)</p> <p>Ranked by robustness_score (higher is better). Last updated from 11 evaluation(s).</p>"},{"location":"leaderboards/#genomics","title":"\ud83e\uddec Genomics","text":""},{"location":"leaderboards/#cell-type-annotation","title":"\ud83e\uddec Cell Type Annotation","text":"<p>\ud83c\udfaf Task: Classification | \ud83c\udfe5 Health Topic: Single-cell Transcriptomics</p> <p>Predicting cell types from single-cell RNA-seq data.</p> <p>Clinical Relevance</p> <p>Automated characterization of immune cell populations.</p>"},{"location":"leaderboards/#leaderboard_2","title":"\ud83c\udfc6 Leaderboard","text":"<pre><code>         \ud83e\udd47          \n     [Evo 2]    \n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \n \ud83e\udd48 \u2502         \u2502 \ud83e\udd49  \n[Geneformer]\u2502         \u2502[SWIFT]\n\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\n</code></pre> <p>All 6 models ranked by Accuracy:</p> Rank Model Score Performance Dataset Date \ud83e\udd47 Evo 2 \ud83d\udc51 0.9250 \u2b50 Excellent PBMC 68k 2024-02-01 \ud83e\udd48 Geneformer \ud83c\udf1f 0.9100 \u2b50 Excellent PBMC 68k 2023-11-01 \ud83e\udd49 SWIFT \u2728 0.8950 \u2705 Good PBMC 68k 2024-01-15 \ud83c\udfc5 Caduceus 0.8850 \u2705 Good PBMC 68k 2024-01-12 \ud83c\udfc5 HyenaDNA 0.8700 \u2705 Good PBMC 68k 2024-01-08 \ud83c\udf96\ufe0f DNABERT-2 0.8500 \u2705 Good PBMC 68k 2024-01-05"},{"location":"leaderboards/#ranking-explanation_2","title":"\ud83d\udcd6 Ranking Explanation","text":"<p>Why These Rankings?</p> <p>\ud83e\udd47 Evo 2 leads with Accuracy=0.9250</p> <ul> <li>Gap to \ud83e\udd48 Geneformer: +0.0150 (1.6% better)</li> <li>Score range across all models: 0.0750</li> <li>Performance distribution: \u2b50 2 excellent, \u2705 4 good</li> </ul> \ud83d\udcd0 How are models scored?  ---  ### \ud83c\udfaf Primary Metric: `Accuracy`  &gt; Proportion of correct predictions (0.0-1.0)  ---  ### \ud83d\udcca Metric Priority  | Priority | Metric | What it measures | |:---:|:---|:---| | 1 | `AUROC` | Discrimination (best for imbalanced data) | | 2 | `Accuracy` | Overall correctness | | 3 | `F1-Score` | Precision-recall balance | | 4 | `Sensitivity` | True positive rate |  ---  ### \ud83c\udfe5 Clinical Readiness Tiers  | Score | Tier | Deployment | Guidance | |:---:|:---:|:---:|:---| | \u2265 0.90 | \u2b50 Excellent | Production | Clinical decision support ready | | 0.80-0.89 | \u2705 Good | Pilot | Needs prospective validation | | 0.70-0.79 | \ud83d\udd36 Fair | Research | Not for patient-facing use | | &lt; 0.70 | \ud83d\udcc8 Developing | Development | Significant improvement needed |  ---  ### \ud83d\udccf Ranking Rules  1. Ranked by **primary metric** (higher = better) 2. Ties broken by secondary metrics 3. Best run per model used 4. 4 decimal precision  ---  ### \u2696\ufe0f Fairness Analysis  Models evaluated across:  | Category | Strata | |:---|:---| | \ud83d\udc64 Demographics | Age, sex, ethnicity | | \ud83d\udd2c Technical | Scanner, acquisition | | \ud83c\udfe5 Clinical | Disease stage, site |  &gt; \u26a0\ufe0f Gaps &gt;10% flagged for review  ---  *Aligned with [ITU/WHO AI4H DEL3](https://www.itu.int/pub/T-FG-AI4H) standards.*"},{"location":"leaderboards/#complete-metrics-comparison_2","title":"\ud83d\udccb Complete Metrics Comparison","text":"Rank Model Accuracy F1-Score \ud83e\udd47 \u2b50 Evo 2 0.9250 0.8900 \ud83e\udd48 \u2b50 Geneformer 0.9100 0.8500 \ud83e\udd49 \u2705 SWIFT 0.8950 0.8550 \ud83c\udfc5 \u2705 Caduceus 0.8850 0.8400 \ud83c\udfc5 \u2705 HyenaDNA 0.8700 0.8200 \ud83c\udf96\ufe0f \u2705 DNABERT-2 0.8500 0.8000 <p>Legend</p> <p>\ud83d\udcca Primary metric: Accuracy (bold) | \u2b50 Excellent (\u22650.9) | \u2705 Good (\u22650.8) | \ud83d\udd36 Fair (\u22650.7) | \ud83d\udcc8 Developing (&lt;0.7)</p> <p>Ranked by Accuracy (higher is better). Last updated from 6 evaluation(s).</p>"},{"location":"leaderboards/#neurology","title":"\ud83e\udde0 Neurology","text":""},{"location":"leaderboards/#alzheimers-disease-classification-using-brain-mri","title":"\ud83e\udde0 Alzheimer's Disease Classification using Brain MRI","text":"<p>\ud83c\udfaf Task: Classification | \ud83c\udfe5 Health Topic: Alzheimer's Disease</p> <p>Binary classification of AD vs CN using structural MRI data.</p> <p>Clinical Relevance</p> <p>Automated screening for AD to assist radiological workflow.</p>"},{"location":"leaderboards/#leaderboard_3","title":"\ud83c\udfc6 Leaderboard","text":"<pre><code>         \ud83e\udd47          \n     [Brain-JEPA]    \n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \n \ud83e\udd48 \u2502         \u2502 \ud83e\udd49  \n[UNI]\u2502         \u2502[BrainLM]\n\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\n</code></pre> <p>All 3 models ranked by AUROC:</p> Rank Model Score Performance Dataset Date \ud83e\udd47 Brain-JEPA \ud83d\udc51 0.9350 \u2b50 Excellent ADNI 2024-01-20 \ud83e\udd48 UNI \ud83c\udf1f 0.9200 \u2b50 Excellent Alzheimer's Disease Neuroimaging Initiative (ADNI) 2023-10-27 \ud83e\udd49 BrainLM \u2728 0.9100 \u2b50 Excellent ADNI 2024-01-15"},{"location":"leaderboards/#ranking-explanation_3","title":"\ud83d\udcd6 Ranking Explanation","text":"<p>Why These Rankings?</p> <p>\ud83e\udd47 Brain-JEPA leads with AUROC=0.9350</p> <ul> <li>Gap to \ud83e\udd48 UNI: +0.0150 (1.6% better)</li> <li>Score range across all models: 0.0250</li> <li>Performance distribution: \u2b50 3 excellent</li> </ul> \ud83d\udcd0 How are models scored?  ---  ### \ud83c\udfaf Primary Metric: `AUROC`  &gt; Area Under ROC Curve - measures discrimination ability (0.5 = random, 1.0 = perfect)  ---  ### \ud83d\udcca Metric Priority  | Priority | Metric | What it measures | |:---:|:---|:---| | 1 | `AUROC` | Discrimination (best for imbalanced data) | | 2 | `Accuracy` | Overall correctness | | 3 | `F1-Score` | Precision-recall balance | | 4 | `Sensitivity` | True positive rate |  ---  ### \ud83c\udfe5 Clinical Readiness Tiers  | Score | Tier | Deployment | Guidance | |:---:|:---:|:---:|:---| | \u2265 0.90 | \u2b50 Excellent | Production | Clinical decision support ready | | 0.80-0.89 | \u2705 Good | Pilot | Needs prospective validation | | 0.70-0.79 | \ud83d\udd36 Fair | Research | Not for patient-facing use | | &lt; 0.70 | \ud83d\udcc8 Developing | Development | Significant improvement needed |  ---  ### \ud83d\udccf Ranking Rules  1. Ranked by **primary metric** (higher = better) 2. Ties broken by secondary metrics 3. Best run per model used 4. 4 decimal precision  ---  ### \u2696\ufe0f Fairness Analysis  Models evaluated across:  | Category | Strata | |:---|:---| | \ud83d\udc64 Demographics | Age, sex, ethnicity | | \ud83d\udd2c Technical | Scanner, acquisition | | \ud83c\udfe5 Clinical | Disease stage, site |  &gt; \u26a0\ufe0f Gaps &gt;10% flagged for review  ---  *Aligned with [ITU/WHO AI4H DEL3](https://www.itu.int/pub/T-FG-AI4H) standards.*"},{"location":"leaderboards/#complete-metrics-comparison_3","title":"\ud83d\udccb Complete Metrics Comparison","text":"Rank Model AUROC Accuracy \ud83e\udd47 \u2b50 Brain-JEPA 0.9350 0.8950 \ud83e\udd48 \u2b50 UNI 0.9200 0.8800 \ud83e\udd49 \u2b50 BrainLM 0.9100 0.8700 <p>Legend</p> <p>\ud83d\udcca Primary metric: AUROC (bold) | \u2b50 Excellent (\u22650.9) | \u2705 Good (\u22650.8) | \ud83d\udd36 Fair (\u22650.7) | \ud83d\udcc8 Developing (&lt;0.7)</p> <p>Ranked by AUROC (higher is better). Last updated from 3 evaluation(s).</p>"},{"location":"leaderboards/#brain-time-series-modeling","title":"\ud83e\udde0 Brain Time-Series Modeling","text":"<p>\ud83d\udd04 Task: Reconstruction | \ud83c\udfe5 Health Topic: Functional Brain Connectivity</p> <p>Evaluating ability to reconstruct masked fMRI voxel time-series.</p> <p>Clinical Relevance</p> <p>Foundation for understanding functional connectivity patterns.</p>"},{"location":"leaderboards/#leaderboard_4","title":"\ud83c\udfc6 Leaderboard","text":"<p>All 1 models ranked by Correlation:</p> Rank Model Score Performance Dataset Date \ud83e\udd47 BrainLM \ud83d\udc51 0.7800 UK Biobank fMRI tensors 2025-11-15 \ud83d\udcd0 How are models scored?  ---  ### \ud83c\udfaf Primary Metric: `Correlation`  &gt; Pearson correlation between predicted and actual values (-1 to 1)  ---  ### \ud83d\udcca Metric Priority  | Priority | Metric | What it measures | |:---:|:---|:---| | 1 | `AUROC` | Discrimination (best for imbalanced data) | | 2 | `Accuracy` | Overall correctness | | 3 | `F1-Score` | Precision-recall balance | | 4 | `Sensitivity` | True positive rate |  ---  ### \ud83c\udfe5 Clinical Readiness Tiers  | Score | Tier | Deployment | Guidance | |:---:|:---:|:---:|:---| | \u2265 0.90 | \u2b50 Excellent | Production | Clinical decision support ready | | 0.80-0.89 | \u2705 Good | Pilot | Needs prospective validation | | 0.70-0.79 | \ud83d\udd36 Fair | Research | Not for patient-facing use | | &lt; 0.70 | \ud83d\udcc8 Developing | Development | Significant improvement needed |  ---  ### \ud83d\udccf Ranking Rules  1. Ranked by **primary metric** (higher = better) 2. Ties broken by secondary metrics 3. Best run per model used 4. 4 decimal precision  ---  ### \u2696\ufe0f Fairness Analysis  Models evaluated across:  | Category | Strata | |:---|:---| | \ud83d\udc64 Demographics | Age, sex, ethnicity | | \ud83d\udd2c Technical | Scanner, acquisition | | \ud83c\udfe5 Clinical | Disease stage, site |  &gt; \u26a0\ufe0f Gaps &gt;10% flagged for review  ---  *Aligned with [ITU/WHO AI4H DEL3](https://www.itu.int/pub/T-FG-AI4H) standards.*"},{"location":"leaderboards/#complete-metrics-comparison_4","title":"\ud83d\udccb Complete Metrics Comparison","text":"Rank Model Correlation MSE \ud83e\udd47 BrainLM 0.7800 0.4500 <p>Legend</p> <p>\ud83d\udcca Primary metric: Correlation (bold) | \u2b50 Excellent (\u22650.9) | \u2705 Good (\u22650.8) | \ud83d\udd36 Fair (\u22650.7) | \ud83d\udcc8 Developing (&lt;0.7)</p> <p>Ranked by Correlation (higher is better). Last updated from 1 evaluation(s).</p>"},{"location":"leaderboards/#toy-classification-benchmark","title":"\ud83e\udde0 Toy Classification Benchmark","text":"<p>\ud83c\udfaf Task: Classification | \ud83c\udfe5 Health Topic: N/A</p> <p>A toy benchmark for testing the pipeline.</p>"},{"location":"leaderboards/#leaderboard_5","title":"\ud83c\udfc6 Leaderboard","text":"<p>All 2 models ranked by AUROC:</p> Rank Model Score Performance Dataset Date \ud83e\udd47 Baseline (Random/Majority) \ud83d\udc51 0.5597 \ud83d\udcc8 Developing Toy fMRI Classification 2025-11-27 \ud83e\udd48 BrainLM \ud83c\udf1f 0.5193 \ud83d\udcc8 Developing Toy fMRI Classification 2025-11-27"},{"location":"leaderboards/#ranking-explanation_4","title":"\ud83d\udcd6 Ranking Explanation","text":"<p>Why These Rankings?</p> <p>\ud83e\udd47 Baseline (Random/Majority) leads with AUROC=0.5597</p> <ul> <li>Gap to \ud83e\udd48 BrainLM: +0.0404 (7.8% better)</li> <li>Performance distribution: \ud83d\udcc8 2 developing</li> </ul> \ud83d\udcd0 How are models scored?  ---  ### \ud83c\udfaf Primary Metric: `AUROC`  &gt; Area Under ROC Curve - measures discrimination ability (0.5 = random, 1.0 = perfect)  ---  ### \ud83d\udcca Metric Priority  | Priority | Metric | What it measures | |:---:|:---|:---| | 1 | `AUROC` | Discrimination (best for imbalanced data) | | 2 | `Accuracy` | Overall correctness | | 3 | `F1-Score` | Precision-recall balance | | 4 | `Sensitivity` | True positive rate |  ---  ### \ud83c\udfe5 Clinical Readiness Tiers  | Score | Tier | Deployment | Guidance | |:---:|:---:|:---:|:---| | \u2265 0.90 | \u2b50 Excellent | Production | Clinical decision support ready | | 0.80-0.89 | \u2705 Good | Pilot | Needs prospective validation | | 0.70-0.79 | \ud83d\udd36 Fair | Research | Not for patient-facing use | | &lt; 0.70 | \ud83d\udcc8 Developing | Development | Significant improvement needed |  ---  ### \ud83d\udccf Ranking Rules  1. Ranked by **primary metric** (higher = better) 2. Ties broken by secondary metrics 3. Best run per model used 4. 4 decimal precision  ---  ### \u2696\ufe0f Fairness Analysis  Models evaluated across:  | Category | Strata | |:---|:---| | \ud83d\udc64 Demographics | Age, sex, ethnicity | | \ud83d\udd2c Technical | Scanner, acquisition | | \ud83c\udfe5 Clinical | Disease stage, site |  &gt; \u26a0\ufe0f Gaps &gt;10% flagged for review  ---  *Aligned with [ITU/WHO AI4H DEL3](https://www.itu.int/pub/T-FG-AI4H) standards.*"},{"location":"leaderboards/#complete-metrics-comparison_5","title":"\ud83d\udccb Complete Metrics Comparison","text":"Rank Model AUROC Accuracy F1-Score \ud83e\udd47 \ud83d\udcc8 Baseline (Random/Majority) 0.5597 0.5750 0.5732 \ud83e\udd48 \ud83d\udcc8 BrainLM 0.5193 0.5100 0.5100 <p>Legend</p> <p>\ud83d\udcca Primary metric: AUROC (bold) | \u2b50 Excellent (\u22650.9) | \u2705 Good (\u22650.8) | \ud83d\udd36 Fair (\u22650.7) | \ud83d\udcc8 Developing (&lt;0.7)</p>"},{"location":"leaderboards/#granular-performance-breakdown_1","title":"\ud83d\udcca Granular Performance Breakdown","text":"<p>Expand sections below to see how models perform across different conditions:</p> \ud83c\udfe5 Baseline (Random/Majority) by Site  | Site | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 SiteB | 0.5780 | 0.6286 | 0.6081 | 70 | | \ud83e\udd48 SiteA | 0.5663 | 0.5714 | 0.5675 | 63 | | SiteC | 0.5107 | 0.5224 | 0.5214 | 67 |   \ud83d\udc64 Baseline (Random/Majority) by Sex  | Sex | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 M | 0.5545 | 0.5938 | 0.5772 | 96 | | F | 0.5487 | 0.5577 | 0.5562 | 104 |   \ud83d\udcc5 Baseline (Random/Majority) by Age Group  | Age Group | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 age_80-100 | 0.6100 | 0.6000 | 0.5960 | 20 | | \ud83e\udd48 age_60-80 | 0.5846 | 0.6094 | 0.6093 | 64 | | \ud83e\udd49 age_20-40 | 0.5679 | 0.5862 | 0.5735 | 58 | | age_40-60 | 0.5209 | 0.5172 | 0.5149 | 58 |   \ud83d\udd2c BrainLM by Scanner  | Scanner | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 Philips | 0.6226 | 0.5479 | 0.5476 | 73 | | \ud83e\udd48 Siemens | 0.5099 | 0.5088 | 0.5086 | 57 | | GE | 0.4087 | 0.4714 | 0.4687 | 70 |   \ud83c\udfe5 BrainLM by Site  | Site | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 SiteA | 0.5791 | 0.5417 | 0.5394 | 72 | | \ud83e\udd48 SiteC | 0.4944 | 0.5070 | 0.5035 | 71 | | SiteB | 0.4643 | 0.4737 | 0.4722 | 57 |   \ud83e\ude7a BrainLM by Disease Stage  | Disease Stage | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 MCI | 0.6372 | 0.6143 | 0.6136 | 70 | | \ud83e\udd48 AD | 0.4649 | 0.4500 | 0.4462 | 60 | | CN | 0.4286 | 0.4571 | 0.4571 | 70 |   \ud83d\udc64 BrainLM by Sex  | Sex | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 F | 0.5513 | 0.5217 | 0.5208 | 92 | | M | 0.4881 | 0.5000 | 0.4985 | 108 |   \ud83d\udcc5 BrainLM by Age Group  | Age Group | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 age_40-60 | 0.5730 | 0.5538 | 0.5430 | 65 | | \ud83e\udd48 age_80-100 | 0.5667 | 0.4545 | 0.4500 | 11 | | \ud83e\udd49 age_60-80 | 0.5357 | 0.5286 | 0.5238 | 70 | | age_20-40 | 0.4528 | 0.4444 | 0.4444 | 54 |   \ud83c\udf0d BrainLM by Ethnicity  | Ethnicity | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 Asian | 0.6667 | 0.5862 | 0.5817 | 29 | | \ud83e\udd48 Black | 0.6165 | 0.6316 | 0.6306 | 38 | | \ud83e\udd49 White | 0.4717 | 0.4545 | 0.4522 | 77 | | Hispanic | 0.4630 | 0.5116 | 0.5106 | 43 | | Other | 0.2857 | 0.3077 | 0.3077 | 13 |   <p>Ranked by AUROC (higher is better). Last updated from 6 evaluation(s).</p>"},{"location":"leaderboards/#fmri-foundation-model-benchmark-granular","title":"\ud83e\udde0 fMRI Foundation Model Benchmark (Granular)","text":"<p>\ud83d\udccb Task: Classification/Reconstruction | \ud83c\udfe5 Health Topic: Functional Brain Imaging Analysis</p> <p>Clinical Relevance</p> <p>Foundation models for fMRI must generalize across diverse acquisition  parameters, scanner manufacturers, and preprocessing pipelines. This benchmark provides granular rankings to identify optimal model-data matches.</p>"},{"location":"leaderboards/#leaderboard_6","title":"\ud83c\udfc6 Leaderboard","text":"<pre><code>         \ud83e\udd47          \n     [Brain-JEPA]    \n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \n \ud83e\udd48 \u2502         \u2502 \ud83e\udd49  \n[BrainLM]\u2502         \u2502[BrainBERT]\n\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\n</code></pre> <p>All 5 models ranked by AUROC:</p> Rank Model Score Performance Dataset Date \ud83e\udd47 Brain-JEPA \ud83d\udc51 0.9250 \u2b50 Excellent hcp_1200 2024-01-22 \ud83e\udd48 BrainLM \ud83c\udf1f 0.9100 \u2b50 Excellent hcp_1200 2024-01-15 \ud83e\udd49 BrainBERT \u2728 0.8700 \u2705 Good hcp_1200 2024-01-10 \ud83c\udfc5 BrainMT 0.8500 \u2705 Good hcp_1200 2024-01-18 \ud83c\udfc5 NeuroClips 0.8300 \u2705 Good hcp_1200 2024-01-05"},{"location":"leaderboards/#ranking-explanation_5","title":"\ud83d\udcd6 Ranking Explanation","text":"<p>Why These Rankings?</p> <p>\ud83e\udd47 Brain-JEPA leads with AUROC=0.9250</p> <ul> <li>Gap to \ud83e\udd48 BrainLM: +0.0150 (1.6% better)</li> <li>Score range across all models: 0.0950</li> <li>Performance distribution: \u2b50 2 excellent, \u2705 3 good</li> </ul> \ud83d\udcd0 How are models scored?  ---  ### \ud83c\udfaf Primary Metric: `AUROC`  &gt; Area Under ROC Curve - measures discrimination ability (0.5 = random, 1.0 = perfect)  ---  ### \ud83d\udcca Metric Priority  | Priority | Metric | What it measures | |:---:|:---|:---| | 1 | `AUROC` | Discrimination (best for imbalanced data) | | 2 | `Accuracy` | Overall correctness | | 3 | `F1-Score` | Precision-recall balance | | 4 | `Sensitivity` | True positive rate |  ---  ### \ud83c\udfe5 Clinical Readiness Tiers  | Score | Tier | Deployment | Guidance | |:---:|:---:|:---:|:---| | \u2265 0.90 | \u2b50 Excellent | Production | Clinical decision support ready | | 0.80-0.89 | \u2705 Good | Pilot | Needs prospective validation | | 0.70-0.79 | \ud83d\udd36 Fair | Research | Not for patient-facing use | | &lt; 0.70 | \ud83d\udcc8 Developing | Development | Significant improvement needed |  ---  ### \ud83d\udccf Ranking Rules  1. Ranked by **primary metric** (higher = better) 2. Ties broken by secondary metrics 3. Best run per model used 4. 4 decimal precision  ---  ### \u2696\ufe0f Fairness Analysis  Models evaluated across:  | Category | Strata | |:---|:---| | \ud83d\udc64 Demographics | Age, sex, ethnicity | | \ud83d\udd2c Technical | Scanner, acquisition | | \ud83c\udfe5 Clinical | Disease stage, site |  &gt; \u26a0\ufe0f Gaps &gt;10% flagged for review  ---  *Aligned with [ITU/WHO AI4H DEL3](https://www.itu.int/pub/T-FG-AI4H) standards.*"},{"location":"leaderboards/#complete-metrics-comparison_6","title":"\ud83d\udccb Complete Metrics Comparison","text":"Rank Model AUROC Accuracy F1-Score Correlation MSE \ud83e\udd47 \u2b50 Brain-JEPA 0.9250 0.8900 0.8800 0.8300 0.3900 \ud83e\udd48 \u2b50 BrainLM 0.9100 0.8700 0.8600 0.8100 0.4200 \ud83e\udd49 \u2705 BrainBERT 0.8700 0.8200 0.8100 0.7600 0.5100 \ud83c\udfc5 \u2705 BrainMT 0.8500 0.8100 0.8000 0.7400 0.5500 \ud83c\udfc5 \u2705 NeuroClips 0.8300 0.7900 0.7800 0.7200 0.5800 <p>Legend</p> <p>\ud83d\udcca Primary metric: AUROC (bold) | \u2b50 Excellent (\u22650.9) | \u2705 Good (\u22650.8) | \ud83d\udd36 Fair (\u22650.7) | \ud83d\udcc8 Developing (&lt;0.7)</p>"},{"location":"leaderboards/#granular-performance-breakdown_2","title":"\ud83d\udcca Granular Performance Breakdown","text":"<p>Expand sections below to see how models perform across different conditions:</p> \ud83d\udd2c Brain-JEPA by Scanner  | Scanner | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 Siemens | 0.9400 | 0.9100 | 0.9000 | 450 | | \ud83e\udd48 Philips | 0.9200 | 0.8800 | 0.8700 | 370 | | GE | 0.9100 | 0.8700 | 0.8600 | 380 |   \ud83d\udce1 Brain-JEPA by Acquisition Type  | Acquisition Type | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 resting_state | 0.9350 | 0.9000 | 0.8900 | 600 | | task_based | 0.9100 | 0.8700 | 0.8600 | 400 |   \ud83d\udd2c BrainLM by Scanner  | Scanner | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 Siemens | 0.9300 | 0.8900 | 0.8800 | 450 | | \ud83e\udd48 Philips | 0.9000 | 0.8600 | 0.8500 | 370 | | GE | 0.8800 | 0.8400 | 0.8300 | 380 |   \ud83c\udfe5 BrainLM by Site  | Site | AUROC | Accuracy | N | |---|---|---|---| | \ud83e\udd47 WashU | 0.9300 | 0.8900 | 220 | | \ud83e\udd48 MGH | 0.9200 | 0.8800 | 200 | | \ud83e\udd49 Oxford | 0.9100 | 0.8700 | 200 | | UCLA | 0.9000 | 0.8600 | 180 | | UMinn | 0.8900 | 0.8500 | 200 |   \ud83d\udce1 BrainLM by Acquisition Type  | Acquisition Type | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 resting_state | 0.9200 | 0.8800 | 0.8700 | 600 | | \ud83e\udd48 language | 0.9100 | 0.8700 | - | 100 | | \ud83e\udd49 working_memory | 0.9000 | 0.8600 | - | 150 | | task_based | 0.8900 | 0.8500 | 0.8400 | 400 | | motor | 0.8800 | 0.8400 | - | 150 |   \u2699\ufe0f BrainLM by Preprocessing  | Preprocessing | AUROC | Accuracy | N | |---|---|---|---| | \ud83e\udd47 fmriprep | 0.9200 | 0.8800 | 500 | | \ud83e\udd48 hcp | 0.9100 | 0.8700 | 400 | | minimal | 0.8500 | 0.8100 | 300 |   \ud83e\uddf2 BrainLM by Field Strength  | Field Strength | AUROC | Accuracy | N | |---|---|---|---| | \ud83e\udd47 7T | 0.9400 | 0.9100 | 100 | | 3T | 0.9100 | 0.8700 | 900 |   \ud83d\udd2c BrainBERT by Scanner  | Scanner | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 Siemens | 0.8900 | 0.8400 | 0.8300 | 450 | | \ud83e\udd48 GE | 0.8600 | 0.8100 | 0.8000 | 380 | | Philips | 0.8500 | 0.8000 | 0.7900 | 370 |   \ud83d\udce1 BrainBERT by Acquisition Type  | Acquisition Type | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 resting_state | 0.8800 | 0.8300 | 0.8200 | 600 | | task_based | 0.8500 | 0.8000 | 0.7900 | 400 |   \ud83d\udd2c BrainMT by Scanner  | Scanner | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 Siemens | 0.8700 | 0.8300 | 0.8200 | 450 | | \ud83e\udd48 GE | 0.8400 | 0.8000 | 0.7900 | 380 | | Philips | 0.8300 | 0.7900 | 0.7800 | 370 |   \ud83d\udd2c NeuroClips by Scanner  | Scanner | AUROC | Accuracy | F1-Score | N | |---|---|---|---|---| | \ud83e\udd47 Siemens | 0.8500 | 0.8100 | 0.8000 | 450 | | \ud83e\udd48 GE | 0.8200 | 0.7800 | 0.7700 | 380 | | Philips | 0.8100 | 0.7700 | 0.7600 | 370 |   <p>Ranked by AUROC (higher is better). Last updated from 5 evaluation(s).</p>"},{"location":"leaderboards/#get-your-model-on-the-leaderboard","title":"\ud83d\ude80 Get Your Model on the Leaderboard","text":"<p>Want to see your Foundation Model ranked here?</p> <ol> <li>\ud83d\udce5 Download the benchmark suite and run locally</li> <li>\ud83e\uddea Evaluate your model: <code>python -m fmbench run --help</code></li> <li>\ud83d\udce4 Submit your results via GitHub Issue</li> </ol> <p>\ud83d\udca1 Propose new evaluation protocols via Issue</p> <p>Curated Benchmark Hub</p> <p>All submissions are reviewed before being added. See Submission Guide for details.</p> <p>Aligned with ITU/WHO FG-AI4H standards for healthcare AI evaluation.</p>"},{"location":"models/","title":"Foundation Models Catalog","text":"<p>This page provides an overview of the foundation models evaluated in this benchmark hub. Each model is defined by a YAML configuration file in the <code>models/</code> directory.</p>"},{"location":"models/#neurology-brain-imaging-models","title":"\ud83e\udde0 Neurology / Brain Imaging Models","text":""},{"location":"models/#brainlm","title":"BrainLM","text":"<p>Model ID: <code>brainlm</code> Modality: fMRI (Brain functional imaging) Architecture: ViT-MAE + Nystromformer encoder/decoder Parameters: 111M / 650M Repository: github.com/vandijklab/BrainLM</p> <p>Masked autoencoding language model for fMRI voxel time-series. Uses ViT-MAE scaffolding with custom BrainLM embeddings that mix voxel coordinates and patched time windows to learn denoised cortical dynamics.</p>"},{"location":"models/#brainjepa","title":"BrainJEPA","text":"<p>Model ID: <code>brainjepa</code> Modality: fMRI, EEG Repository: Brain-JEPA Repository</p> <p>Joint-Embedding Predictive Architecture for brain signals. Self-supervised learning approach that learns representations by predicting latent representations rather than raw pixels/signals.</p>"},{"location":"models/#brainmt","title":"BrainMT","text":"<p>Model ID: <code>brainmt</code> Modality: Multi-modal brain imaging Repository: BrainMT Repository</p> <p>Multi-modal brain transformer for integrating structural and functional brain imaging data.</p>"},{"location":"models/#brainharmony","title":"BrainHarmony","text":"<p>Model ID: <code>brainharmony</code> Modality: Multi-site neuroimaging Repository: BrainHarmony Repository</p> <p>Harmonization framework for multi-site neuroimaging studies, addressing scanner and acquisition protocol variability.</p>"},{"location":"models/#uni","title":"UNI","text":"<p>Model ID: <code>MOD-UNI</code> Modality: MRI (Structural brain imaging) Repository: github.com/insitro/uni</p> <p>Vision Transformer trained on massive MRI datasets for general-purpose brain imaging analysis.</p>"},{"location":"models/#genomics-single-cell-models","title":"\ud83e\uddec Genomics / Single-Cell Models","text":""},{"location":"models/#geneformer","title":"Geneformer","text":"<p>Model ID: <code>MOD-GENEFORMER</code> Modality: scRNA-seq (Single-cell transcriptomics) Repository: huggingface.co/ctheodoris/Geneformer</p> <p>Transformer model pretrained on 30 million single cell transcriptomes. Learns context-aware gene embeddings for cell type annotation, gene regulatory network inference, and therapeutic target discovery.</p>"},{"location":"models/#caduceus","title":"Caduceus","text":"<p>Model ID: <code>caduceus</code> Modality: DNA sequences Repository: Caduceus Repository</p> <p>Long-range DNA sequence model using efficient attention mechanisms for genomic variant interpretation.</p>"},{"location":"models/#dnabert-2","title":"DNABERT-2","text":"<p>Model ID: <code>dnabert2</code> Modality: DNA sequences Repository: DNABERT-2 Repository</p> <p>BERT-based model for DNA sequence understanding, supporting tasks like promoter prediction, splice site detection, and variant effect prediction.</p>"},{"location":"models/#evo2","title":"Evo2","text":"<p>Model ID: <code>evo2</code> Modality: DNA/RNA sequences Repository: Evo2 Repository</p> <p>Evolution-inspired foundation model for sequence analysis and generation.</p>"},{"location":"models/#hyenadna","title":"HyenaDNA","text":"<p>Model ID: <code>hyenadna</code> Modality: Long DNA sequences Repository: HyenaDNA Repository</p> <p>Efficient long-range sequence model using Hyena operators for genomic analysis at scale.</p>"},{"location":"models/#nucleotide-transformer-generator","title":"Nucleotide Transformer (Generator)","text":"<p>Model ID: <code>generator</code> Modality: Nucleotide sequences Repository: Generator Repository</p> <p>Transformer-based generative model for nucleotide sequence synthesis and analysis.</p>"},{"location":"models/#multi-modal-models","title":"\ud83d\udd2c Multi-Modal Models","text":""},{"location":"models/#m3fm","title":"M3FM","text":"<p>Model ID: <code>m3fm</code> Modality: Multi-modal (imaging + genomics) Repository: M3FM Repository</p> <p>Multi-modal foundation model integrating imaging, genomics, and clinical data.</p>"},{"location":"models/#flamingo","title":"Flamingo","text":"<p>Model ID: <code>flamingo</code> Modality: Vision-Language (Medical imaging + text) Repository: Flamingo Repository</p> <p>Vision-language model adapted for medical imaging and clinical text understanding.</p>"},{"location":"models/#vlm-dev-clinical","title":"VLM Dev Clinical","text":"<p>Model ID: <code>vlm_dev_clinical</code> Modality: Vision-Language (Clinical) Repository: Clinical VLM Development</p> <p>Vision-language model specifically developed for clinical applications.</p>"},{"location":"models/#specialized-models","title":"\ud83e\uddea Specialized Models","text":""},{"location":"models/#swift","title":"SWIFT","text":"<p>Model ID: <code>swift</code> Modality: Time-series analysis Repository: SWIFT Repository</p> <p>Specialized model for time-series analysis in biological signals.</p>"},{"location":"models/#tabpfn","title":"TabPFN","text":"<p>Model ID: <code>tabpfn</code> Modality: Tabular data Repository: TabPFN Repository</p> <p>Prior-data fitted network for tabular data classification, useful for clinical structured data.</p>"},{"location":"models/#titan","title":"Titan","text":"<p>Model ID: <code>titan</code> Modality: Large-scale biological data Repository: Titan Repository</p> <p>Large-scale foundation model for integrative biological data analysis.</p>"},{"location":"models/#me-llama","title":"ME-LLaMA","text":"<p>Model ID: <code>me_llama</code> Modality: Medical language Repository: ME-LLaMA Repository</p> <p>Medical-specialized language model based on LLaMA architecture.</p>"},{"location":"models/#llm-semantic-bridge","title":"LLM Semantic Bridge","text":"<p>Model ID: <code>llm_semantic_bridge</code> Modality: Multi-modal semantic alignment Repository: Semantic Bridge Development</p> <p>Model for bridging semantic representations across different medical data modalities.</p>"},{"location":"models/#model-configuration-format","title":"\ud83d\udcdd Model Configuration Format","text":"<p>Each model is defined in a YAML file with the following structure:</p> <pre><code>model_id: unique_identifier\nname: Human-Readable Name\nmodality: primary_modality\nupstream_repo: https://github.com/org/repo\nnotes: Description of the model architecture and capabilities\narch: Architecture details (optional)\nparams: Parameter count (optional)\n</code></pre>"},{"location":"models/#adding-your-model","title":"\ud83c\udfaf Adding Your Model","text":"<p>To add your foundation model to the benchmark:</p> <ol> <li>Create a model configuration YAML in <code>models/</code></li> <li>Implement the model interface (see <code>fmbench/models.py</code>)</li> <li>Run the benchmark suite(s) relevant to your model's modality</li> <li>Submit results for leaderboard inclusion</li> </ol> <p>See the contributing guide for more details.</p>"},{"location":"models/#model-performance","title":"\ud83d\udcca Model Performance","text":"<p>For detailed performance metrics and rankings, see the Leaderboards.</p>"}]}