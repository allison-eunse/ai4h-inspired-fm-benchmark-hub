{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AI4H-Inspired Foundation Model Benchmarks","text":"<p>Standardized, AI4H-aligned benchmarks for genetics and brain imaging foundation models \u2014 designed to be runnable locally, with public, comparable results.</p> <p>Run a toy benchmark Submit my results (<code>eval.yaml</code>) View leaderboards</p>"},{"location":"#what-you-can-run-right-now","title":"What you can run right now","text":"<pre><code>pip install -e .\npython -m fmbench generate-toy-data\npython -m fmbench run --suite SUITE-TOY-CLASS --model configs/model_dummy_classifier.yaml --out results/toy_run\n</code></pre> <p>You should get two concrete artifacts:</p> <ul> <li><code>results/toy_run/report.md</code>: a human-readable report</li> <li><code>results/toy_run/eval.yaml</code>: a machine-readable record for submission</li> </ul> <p>Example <code>eval.yaml</code> (what you submit):</p> <pre><code>eval_id: SUITE-TOY-CLASS-dummy_classifier-YYYYMMDD-HHMMSS\nbenchmark_id: BM-TOY-CLASS\nmodel_ids:\n  candidate: my_model_id\ndataset_id: DS-TOY-FMRI-CLASS\nrun_metadata:\n  runner: fmbench\n  suite_id: SUITE-TOY-CLASS\nmetrics:\n  AUROC: 0.82\n  Accuracy: 0.76\nstatus: Completed\n</code></pre>"},{"location":"#how-submissions-work-fully-automated","title":"\ud83d\udd04 How submissions work (fully automated)","text":"<p>Our leaderboard updates automatically when you submit results \u2014 no manual review delay.</p> <pre><code>flowchart LR\n    subgraph local [\"\ud83d\udda5\ufe0f Your Machine (Private)\"]\n        A[\"python -m fmbench run\"] --&gt; B[\"report.md\"]\n        A --&gt; C[\"eval.yaml\"]\n    end\n\n    subgraph github [\"\u2601\ufe0f GitHub (Public)\"]\n        C --&gt; D[\"Open Issue\\n+ paste YAML\"]\n        D --&gt; E[\"\ud83e\udd16 Bot extracts\\n&amp; validates\"]\n        E --&gt; F[\"Auto-commit\\nto evals/\"]\n        F --&gt; G[\"\ud83c\udfc6 Leaderboard\\nrebuilds\"]\n        G --&gt; H[\"\ud83d\udcc4 Docs deploy\\nto GitHub Pages\"]\n    end\n\n    style local fill:#e8f5e9,stroke:#4caf50\n    style github fill:#e3f2fd,stroke:#2196f3</code></pre> <ul> <li> <p> Your model stays private</p> <p>Weights, code, and training data never leave your machine. You only share metrics + metadata.</p> </li> <li> <p> Zero manual steps</p> <p>GitHub Actions validates your <code>eval.yaml</code>, commits it to the repo, and rebuilds the leaderboard automatically.</p> </li> <li> <p> Minutes, not days</p> <p>From submission to leaderboard appearance: ~2-3 minutes (not weeks of review).</p> </li> <li> <p> AI4H compliant</p> <p>Follows ITU/WHO FG-AI4H DEL3 standards for local evaluation with standardized reporting.</p> </li> </ul> <p>Submit your results \u2192</p>"},{"location":"#what-stays-private-vs-what-is-shared","title":"What stays private vs what is shared","text":"Item Shared publicly? Notes Benchmark code \u2705 This repository Toy datasets \u2705 <code>toy_data/</code> Metrics + run metadata \u2705 Submitted via <code>eval.yaml</code> Model weights \u274c Never leave your machine Model code \u274c Never leave your machine Training data \u274c Never leave your machine <p>This matches the AI4H DEL3 idea of local evaluation with standardized reporting. See AI4H Alignment.</p>"},{"location":"#data-disclaimer","title":"\u26a0\ufe0f Data disclaimer","text":"<p>This repo contains TOY DATA only</p> <p>Full-scale datasets are NOT included. We provide small subsamples for pipeline testing.</p> <ul> <li>Toy data (included): 100\u201327,000 samples for validating your integration</li> <li>Full genomics data: Download from HuggingFace</li> <li>Brain imaging data: Requires institutional access (UK Biobank, HCP, etc.)</li> </ul> Using Toy Data Using Full Data \u2705 Verify pipeline works \u2705 Get publishable metrics \u26a0\ufe0f High variance metrics \u2705 Stable, reproducible scores \u26a0\ufe0f Not for publication \u26a0\ufe0f Requires external download <p>See Data Sources for download links.</p>"},{"location":"#start-here-recommended-workflow","title":"Start here (recommended workflow)","text":"<ul> <li>Pick a suite: start with <code>SUITE-TOY-CLASS</code> (toy fMRI-like classification).</li> <li>Wrap your model locally: provide a small Python wrapper + a model config YAML.</li> <li>Run: <code>fmbench run</code> (and optionally <code>fmbench run-robustness</code>).</li> <li>Inspect outputs: <code>report.md</code>, then submit <code>eval.yaml</code>.</li> </ul> <p>Start Here / Researcher Workflow</p>"},{"location":"#robustness-testing","title":"Robustness testing","text":"<p>Test how your model handles noise, artifacts, and perturbations:</p> <pre><code>python -m fmbench run-robustness \\\n    --model configs/model_dummy_classifier.yaml \\\n    --data toy_data/neuro/robustness \\\n    --out results/robustness_eval\n</code></pre> <p>This produces rAUC (Reverse Area Under Curve) scores quantifying output stability under perturbations like channel dropout, Gaussian noise, and temporal shifts.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! You can:</p> <ul> <li>Submit benchmark results: Submission Guide</li> <li>Propose new protocols: Open a Discussion</li> <li>Add model adapters: See Models</li> </ul>"},{"location":"#documentation-map","title":"Documentation map","text":"<ul> <li>Leaderboards: Leaderboards</li> <li>Submit results: Submission Guide</li> <li>Models catalog: Models</li> <li>Data specifications: fMRI, sMRI, Genomics</li> <li>Protocols (recipes): CCA &amp; permutation, Prediction baselines, Partial correlations</li> <li>Design / standards: AI4H alignment</li> </ul>"},{"location":"credits/","title":"Credits &amp; Acknowledgments","text":"<p>This benchmark hub uses data and tools from multiple sources. We gratefully acknowledge the following contributions:</p>"},{"location":"credits/#data-sources","title":"Data Sources","text":""},{"location":"credits/#genomic-benchmarks-dna-sequence-classification","title":"Genomic Benchmarks (DNA Sequence Classification)","text":"<p>The DNA sequence benchmark datasets (enhancers, promoters, regulatory elements) are from the Genomic Benchmarks collection.</p> <p>GRE\u0160OV\u00c1, Katar\u00edna, et al. Genomic Benchmarks: A Collection of Datasets for Genomic Sequence Classification. bioRxiv, 2022.</p> <ul> <li>Paper: https://www.biorxiv.org/content/10.1101/2022.06.08.495248</li> <li>HuggingFace: https://huggingface.co/datasets/katielink/genomic-benchmarks</li> <li>License: Apache 2.0</li> </ul> <pre><code>@article{gresova2022genomic,\n  title={Genomic Benchmarks: A Collection of Datasets for Genomic Sequence Classification},\n  author={Gresova, Katarina and Martinek, Vlastimil and Cechak, David and Simecek, Petr and Alexiou, Panagiotis},\n  journal={bioRxiv},\n  year={2022},\n  publisher={Cold Spring Harbor Laboratory},\n  url={https://www.biorxiv.org/content/10.1101/2022.06.08.495248}\n}\n</code></pre>"},{"location":"credits/#nucleotide-transformer-benchmark","title":"Nucleotide Transformer Benchmark","text":"<p>Multi-task DNA benchmark from InstaDeepAI.</p> <p>DALLA-TORRE, Hugo, et al. The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics. bioRxiv, 2023.</p> <ul> <li>Paper: https://www.biorxiv.org/content/10.1101/2023.01.11.523679</li> <li>HuggingFace: https://huggingface.co/datasets/InstaDeepAI/nucleotide_transformer_downstream_tasks_revised</li> </ul> <pre><code>@article{dalla2023nucleotide,\n  title={The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics},\n  author={Dalla-Torre, Hugo and Gonzalez, Liam and Mendoza-Revilla, Javier and others},\n  journal={bioRxiv},\n  year={2023},\n  publisher={Cold Spring Harbor Laboratory}\n}\n</code></pre>"},{"location":"credits/#10x-genomics-pbmc-3k-dataset","title":"10x Genomics PBMC 3K Dataset","text":"<p>Single-cell RNA-seq data from peripheral blood mononuclear cells.</p> <p>10x Genomics. 3k PBMCs from a Healthy Donor. 10x Genomics Datasets, 2016.</p> <ul> <li>URL: https://www.10xgenomics.com/datasets/3-k-pbm-cs-from-a-healthy-donor-1-standard-1-1-0</li> <li>License: Free for research use</li> </ul>"},{"location":"credits/#encode-consortium","title":"ENCODE Consortium","text":"<p>Regulatory element annotations derived from ENCODE experimental data.</p> <p>The ENCODE Project Consortium. An integrated encyclopedia of DNA elements in the human genome. Nature, 489(7414), 57-74, 2012.</p> <ul> <li>URL: https://www.encodeproject.org/</li> </ul> <pre><code>@article{encode2012integrated,\n  title={An integrated encyclopedia of DNA elements in the human genome},\n  author={{ENCODE Project Consortium}},\n  journal={Nature},\n  volume={489},\n  number={7414},\n  pages={57--74},\n  year={2012},\n  publisher={Nature Publishing Group}\n}\n</code></pre>"},{"location":"credits/#roadmap-epigenomics","title":"Roadmap Epigenomics","text":"<p>Epigenomic data across human tissues and cell types.</p> <p>Roadmap Epigenomics Consortium. Integrative analysis of 111 reference human epigenomes. Nature, 518(7539), 317-330, 2015.</p> <ul> <li>URL: http://www.roadmapepigenomics.org/</li> </ul>"},{"location":"credits/#eukaryotic-promoter-database-epd","title":"Eukaryotic Promoter Database (EPD)","text":"<p>Curated promoter sequences.</p> <p>DREOS, Ren\u00e9, et al. The eukaryotic promoter database in its 30th year: focus on non-vertebrate organisms. Nucleic Acids Research, 45(D1), D51-D55, 2017.</p> <ul> <li>URL: https://epd.expasy.org/epd/</li> </ul>"},{"location":"credits/#tools-software","title":"Tools &amp; Software","text":""},{"location":"credits/#scanpy","title":"Scanpy","text":"<p>Used for single-cell RNA-seq data processing.</p> <p>WOLF, F. Alexander, ANGERER, Philipp, and THEIS, Fabian J. SCANPY: large-scale single-cell gene expression data analysis. Genome Biology, 19(1), 1-5, 2018.</p> <pre><code>@article{wolf2018scanpy,\n  title={SCANPY: large-scale single-cell gene expression data analysis},\n  author={Wolf, F Alexander and Angerer, Philipp and Theis, Fabian J},\n  journal={Genome biology},\n  volume={19},\n  number={1},\n  pages={1--5},\n  year={2018},\n  publisher={Springer}\n}\n</code></pre>"},{"location":"credits/#framework-alignment","title":"Framework Alignment","text":""},{"location":"credits/#ituwho-focus-group-on-ai-for-health-fg-ai4h","title":"ITU/WHO Focus Group on AI for Health (FG-AI4H)","text":"<p>This benchmark hub's methodology is derived from the public deliverables of FG-AI4H.</p> <ul> <li>DEL3: System Requirement Specifications</li> <li>DEL5.4: Test Data Specification  </li> <li>DEL7.x: Test Suite Guidelines</li> <li> <p>DEL10.8: Analytical Validation Protocols</p> </li> <li> <p>URL: https://www.itu.int/en/ITU-T/focusgroups/ai4h/</p> </li> </ul>"},{"location":"credits/#license","title":"License","text":"<p>This benchmark hub is licensed under the MIT License. Individual datasets retain their original licenses as noted above.</p>"},{"location":"data_sources/","title":"Free Data Sources for FM Benchmarking","text":"<p>This page lists the real, publicly available datasets included in this benchmark hub, with proper attribution to their original sources.</p>"},{"location":"data_sources/#dna-sequence-data-genomic-benchmarks","title":"\ud83e\uddec DNA Sequence Data (Genomic Benchmarks)","text":"<p>These datasets are from the Genomic Benchmarks collection and are ready to use in <code>toy_data/genomics/dna_sequences/</code>.</p> <p>Data Source</p> <p>DNA sequence benchmarks are from Genomic Benchmarks by Gre\u0161ov\u00e1 et al.</p>"},{"location":"data_sources/#enhancer-classification-cohn-et-al","title":"Enhancer Classification (Cohn et al.)","text":"<p>Real ChIP-seq derived enhancer sequences</p> Split Sequences Source Train 20,843 ChIP-seq experiments Test 6,948 ChIP-seq experiments <pre><code>toy_data/genomics/dna_sequences/enhancers_cohn/\n\u251c\u2500\u2500 train.tsv  (sequence, label)\n\u2514\u2500\u2500 test.tsv   (sequence, label)\n</code></pre> <p>Run benchmark:</p> <pre><code>python -m fmbench run --suite SUITE-DNA-CLASS --model configs/model_evo2.yaml --out results/\n</code></pre>"},{"location":"data_sources/#promoter-classification-epd-database","title":"Promoter Classification (EPD Database)","text":"<p>Real curated promoters from Eukaryotic Promoter Database</p> Split Sequences Source Train 27,097 EPD database Test 9,034 EPD database <pre><code>toy_data/genomics/dna_sequences/promoters_nontata/\n\u251c\u2500\u2500 train.tsv\n\u2514\u2500\u2500 test.tsv\n</code></pre>"},{"location":"data_sources/#regulatory-elements-ensembl","title":"Regulatory Elements (Ensembl)","text":"<p>Real annotations from ENCODE + Roadmap Epigenomics</p> Split Sequences Classes Notes Train 1,500 enhancer, promoter, open_chromatin Toy version (500/class) Test 57,713 enhancer, promoter, open_chromatin Full test set <p>Full Dataset</p> <p>The toy train set is a stratified subsample. Full training set (231,348 samples) available from Genomic Benchmarks.</p> <pre><code>toy_data/genomics/dna_sequences/regulatory_ensembl/\n\u251c\u2500\u2500 train.tsv\n\u2514\u2500\u2500 test.tsv\n</code></pre>"},{"location":"data_sources/#nucleotide-transformer-benchmark-suite","title":"Nucleotide Transformer Benchmark Suite","text":"<p>The standard multi-task benchmark for DNA FMs</p> Split Sequences Tasks Notes Train 1,500 18 tasks Toy version (500/class) Test 38,822 18 tasks Full test set <p>Full Dataset</p> <p>The toy train set is a stratified subsample. Full training set (493,242 samples) available from InstaDeepAI.</p> <p>Tasks include:</p> <ul> <li>Enhancer prediction</li> <li>Promoter prediction (TATA / non-TATA)</li> <li>Splice site prediction (donor / acceptor)</li> <li>Histone modifications (H3, H3K4me3, H3K36me3, etc.)</li> </ul> <pre><code>toy_data/genomics/dna_sequences/nucleotide_transformer/\n\u251c\u2500\u2500 train.tsv  (sequence, name, label, task)\n\u2514\u2500\u2500 test.tsv\n</code></pre> <p>Data Source</p> <p>From InstaDeepAI/nucleotide_transformer_downstream_tasks_revised. See Dalla-Torre et al. (2023).</p>"},{"location":"data_sources/#what-makes-this-data-real","title":"What makes this data \"real\"?","text":"Dataset Experimental Source Not Synthetic Because Enhancers (Cohn) ChIP-seq Binding sites measured in cells Promoters (EPD) TSS mapping Curated from experimental evidence Regulatory (Ensembl) ENCODE + Roadmap 1000s of ChIP-seq/DNase-seq experiments NT Benchmark Multiple Industry-standard benchmark from papers"},{"location":"data_sources/#single-cell-rna-seq-data","title":"\ud83e\uddec Single-Cell RNA-seq Data","text":""},{"location":"data_sources/#pbmc-3k-dataset","title":"PBMC 3K Dataset","text":"<p>Pre-processed with real cell type labels</p> Metric Value Cells 2,638 Genes 1,838 Classes 8 cell types <pre><code>toy_data/genomics/pbmc_classification/\n\u251c\u2500\u2500 X.npy      (expression matrix)\n\u251c\u2500\u2500 y.npy      (labels)\n\u251c\u2500\u2500 metadata.csv\n\u2514\u2500\u2500 label_map.csv\n</code></pre> <p>Data Source</p> <p>PBMC data from 10x Genomics - \"3k PBMCs from a Healthy Donor\" dataset. Processed using Scanpy with cell type annotations from Louvain clustering.</p> <p>Run benchmark:</p> <pre><code>python -m fmbench run --suite SUITE-GEN-CLASS-001 --model configs/model_geneformer.yaml --dataset DS-PBMC --out results/\n</code></pre>"},{"location":"data_sources/#additional-data-sources","title":"Additional Data Sources","text":""},{"location":"data_sources/#more-dna-sequence-data","title":"More DNA Sequence Data","text":"Source URL Data Type UCSC Genome Browser hgdownload.soe.ucsc.edu Reference genomes ENCODE Portal encodeproject.org ChIP-seq, ATAC-seq Roadmap Epigenomics roadmapepigenomics.org Epigenomic marks ClinVar ncbi.nlm.nih.gov/clinvar Pathogenic variants"},{"location":"data_sources/#more-single-cell-data","title":"More Single-Cell Data","text":"Source URL Data Type 10x Genomics 10xgenomics.com/datasets scRNA-seq CellxGene cellxgene.cziscience.com Annotated atlases Human Cell Atlas humancellatlas.org Multi-tissue atlases"},{"location":"data_sources/#brain-imaging-data","title":"Brain Imaging Data","text":"Source URL Registration OpenNeuro openneuro.org None required HCP humanconnectome.org Required UK Biobank ukbiobank.ac.uk Application required"},{"location":"data_sources/#download-more-data","title":"Download More Data","text":"<pre><code># Activate environment\nsource scripts/activate_fmbench.sh\n\n# Download additional genomic benchmarks\npython &lt;&lt; 'EOF'\nfrom datasets import load_dataset\n\n# Drosophila enhancers\nds = load_dataset(\"katarinagresova/Genomic_Benchmarks_drosophila_enhancers_stark\")\n\n# Mouse enhancers  \nds = load_dataset(\"katarinagresova/Genomic_Benchmarks_dummy_mouse_enhancers_ensembl\")\n\n# Coding vs intergenic\nds = load_dataset(\"katarinagresova/Genomic_Benchmarks_demo_coding_vs_intergenomic_seqs\")\nEOF\n</code></pre>"},{"location":"data_sources/#citations-acknowledgments","title":"\ud83d\udcda Citations &amp; Acknowledgments","text":"<p>If you use data from this benchmark hub, please cite the original sources:</p>"},{"location":"data_sources/#genomic-benchmarks-dna-sequence-data","title":"Genomic Benchmarks (DNA Sequence Data)","text":"<p>GRE\u0160OV\u00c1, Katar\u00edna, et al. Genomic Benchmarks: A Collection of Datasets for Genomic Sequence Classification. bioRxiv, 2022.</p> <p>URL: https://www.biorxiv.org/content/10.1101/2022.06.08.495248</p> <pre><code>@article{gresova2022genomic,\n  title={Genomic Benchmarks: A Collection of Datasets for Genomic Sequence Classification},\n  author={Gresova, Katarina and Martinek, Vlastimil and Cechak, David and Simecek, Petr and Alexiou, Panagiotis},\n  journal={bioRxiv},\n  year={2022},\n  publisher={Cold Spring Harbor Laboratory},\n  url={https://www.biorxiv.org/content/10.1101/2022.06.08.495248}\n}\n</code></pre>"},{"location":"data_sources/#nucleotide-transformer-nt-benchmark-data","title":"Nucleotide Transformer (NT Benchmark Data)","text":"<p>DALLA-TORRE, Hugo, et al. The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics. bioRxiv, 2023.</p> <p>URL: https://www.biorxiv.org/content/10.1101/2023.01.11.523679</p> <pre><code>@article{dalla2023nucleotide,\n  title={The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics},\n  author={Dalla-Torre, Hugo and Gonzalez, Liam and Mendoza-Revilla, Javier and others},\n  journal={bioRxiv},\n  year={2023},\n  publisher={Cold Spring Harbor Laboratory}\n}\n</code></pre>"},{"location":"data_sources/#10x-genomics-pbmc-3k-single-cell-rna-seq-data","title":"10x Genomics PBMC 3K (Single-Cell RNA-seq Data)","text":"<p>10x Genomics. 3k PBMCs from a Healthy Donor. 10x Genomics Datasets, 2016.</p> <p>URL: https://www.10xgenomics.com/datasets/3-k-pbm-cs-from-a-healthy-donor-1-standard-1-1-0</p>"},{"location":"data_sources/#scanpy-single-cell-analysis","title":"Scanpy (Single-Cell Analysis)","text":"<p>WOLF, F. Alexander, ANGERER, Philipp, and THEIS, Fabian J. SCANPY: large-scale single-cell gene expression data analysis. Genome Biology, 2018.</p> <pre><code>@article{wolf2018scanpy,\n  title={SCANPY: large-scale single-cell gene expression data analysis},\n  author={Wolf, F Alexander and Angerer, Philipp and Theis, Fabian J},\n  journal={Genome biology},\n  volume={19},\n  number={1},\n  pages={1--5},\n  year={2018},\n  publisher={Springer}\n}\n</code></pre>"},{"location":"data_sources/#encode-consortium-regulatory-data","title":"ENCODE Consortium (Regulatory Data)","text":"<p>The ENCODE Project Consortium. An integrated encyclopedia of DNA elements in the human genome. Nature, 2012.</p>"},{"location":"data_sources/#roadmap-epigenomics-regulatory-data","title":"Roadmap Epigenomics (Regulatory Data)","text":"<p>Roadmap Epigenomics Consortium. Integrative analysis of 111 reference human epigenomes. Nature, 2015.</p>"},{"location":"data_sources/#eukaryotic-promoter-database-promoter-data","title":"Eukaryotic Promoter Database (Promoter Data)","text":"<p>DREOS, Ren\u00e9, et al. The eukaryotic promoter database in its 30th year: focus on non-vertebrate organisms. Nucleic Acids Research, 2017.</p>"},{"location":"data_sources/#license","title":"License","text":"<ul> <li>Genomic Benchmarks: Apache 2.0</li> <li>10x Genomics PBMC: Free for research use</li> <li>ENCODE/Roadmap data: Public domain</li> </ul> <p>Back to Home View Benchmarks</p>"},{"location":"how_it_works/","title":"How It Works","text":"<p>This page explains the end-to-end flow from running a benchmark to appearing on the leaderboard.</p>"},{"location":"how_it_works/#the-big-picture","title":"The Big Picture","text":"<pre><code>flowchart LR\n    subgraph you [\"\ud83d\udc64 You\"]\n        A[\"Run benchmark\\nlocally\"]\n    end\n\n    subgraph artifacts [\"\ud83d\udce6 Artifacts\"]\n        B[\"eval.yaml\"]\n        C[\"report.md\"]\n    end\n\n    subgraph submit [\"\ud83d\udce4 Submit\"]\n        D[\"GitHub Issue\"]\n    end\n\n    subgraph automation [\"\ud83e\udd16 GitHub Actions\"]\n        E[\"Validate\"]\n        F[\"Commit\"]\n        G[\"Build\"]\n        H[\"Deploy\"]\n    end\n\n    subgraph result [\"\ud83c\udfc6 Result\"]\n        I[\"Live Leaderboard\"]\n    end\n\n    A --&gt; B\n    A --&gt; C\n    B --&gt; D\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style you fill:#e8f5e9,stroke:#4caf50\n    style artifacts fill:#fff3e0,stroke:#ff9800\n    style submit fill:#e1f5fe,stroke:#03a9f4\n    style automation fill:#f3e5f5,stroke:#9c27b0\n    style result fill:#fce4ec,stroke:#e91e63</code></pre>"},{"location":"how_it_works/#step-by-step","title":"Step-by-Step","text":""},{"location":"how_it_works/#1-run-locally-your-machine","title":"1\ufe0f\u20e3 Run locally (your machine)","text":"<pre><code>python -m fmbench run \\\n    --suite SUITE-GEN-CLASS-001 \\\n    --model configs/model_geneformer.yaml \\\n    --dataset DS-PBMC \\\n    --out results/my_run\n</code></pre> <p>This produces:</p> File What it is Share it? <code>eval.yaml</code> Machine-readable metrics + metadata \u2705 Yes (submit this) <code>report.md</code> Human-readable summary Optional <p>What stays private:</p> <ul> <li>Your model weights</li> <li>Your model code</li> <li>Your training data</li> <li>Your infrastructure details (unless you choose to share)</li> </ul>"},{"location":"how_it_works/#2-submit-via-github-issue","title":"2\ufe0f\u20e3 Submit via GitHub Issue","text":"<ol> <li>Go to New Submission Issue</li> <li>Fill in the template</li> <li>Paste your <code>eval.yaml</code> content in the YAML code block</li> <li>Click \"Submit new issue\"</li> </ol> <p>Pro tip</p> <p>You can also drag-and-drop your <code>eval.yaml</code> file as an attachment, but pasting the content directly enables automatic processing.</p>"},{"location":"how_it_works/#3-automated-validation-github-actions","title":"3\ufe0f\u20e3 Automated validation (GitHub Actions)","text":"<p>Within seconds of submitting, a GitHub Action:</p> <ol> <li>Extracts the YAML from your issue body</li> <li>Validates required fields:</li> <li><code>eval_id</code> \u2014 unique identifier</li> <li><code>benchmark_id</code> \u2014 must match an existing benchmark</li> <li><code>model_ids.candidate</code> \u2014 your model ID</li> <li><code>metrics</code> \u2014 at least one metric</li> <li><code>status</code> \u2014 must be <code>Completed</code></li> <li>Comments on your issue with the result</li> </ol> <p>If validation fails, you'll see an error message explaining what's wrong.</p>"},{"location":"how_it_works/#4-auto-commit-to-repository","title":"4\ufe0f\u20e3 Auto-commit to repository","text":"<p>If validation passes:</p> <ol> <li>Your <code>eval.yaml</code> is committed to <code>evals/</code></li> <li>The issue gets labeled <code>processed</code></li> <li>You get a confirmation comment with links</li> </ol>"},{"location":"how_it_works/#5-leaderboard-rebuild","title":"5\ufe0f\u20e3 Leaderboard rebuild","text":"<p>A second GitHub Action triggers:</p> <ol> <li>Reads all files in <code>evals/</code></li> <li>Aggregates metrics by benchmark and model</li> <li>Generates <code>docs/leaderboards/index.md</code></li> <li>Deploys to GitHub Pages</li> </ol>"},{"location":"how_it_works/#6-live-on-the-leaderboard","title":"6\ufe0f\u20e3 Live on the leaderboard! \ud83c\udf89","text":"<p>Your model appears on the live leaderboard within ~2-3 minutes.</p>"},{"location":"how_it_works/#what-triggers-automation","title":"What triggers automation?","text":"Event What happens Issue opened with <code>submission</code> label Extracts and validates eval.yaml Push to <code>evals/*.yaml</code> Rebuilds leaderboard Push to <code>benchmarks/*.yaml</code> Rebuilds leaderboard Daily at 00:00 UTC Scheduled leaderboard rebuild (consistency check) Manual trigger Workflow dispatch available"},{"location":"how_it_works/#validation-rules","title":"Validation rules","text":"<p>Your <code>eval.yaml</code> must include:</p> <pre><code>eval_id: SUITE-XXX-model_id-YYYYMMDD-HHMMSS  # Unique ID\nbenchmark_id: BM-XXX                          # Must exist in benchmarks/\nmodel_ids:\n  candidate: your_model_id                    # Your model's ID\ndataset_id: DS-XXX                            # Dataset used\nmetrics:\n  AUROC: 0.85                                 # At least one metric\n  Accuracy: 0.80\nstatus: Completed                             # Or Failed/Partial\n</code></pre> <p>Optional but recommended:</p> <pre><code>run_metadata:\n  date: \"2025-01-15\"\n  runner: fmbench\n  hardware: \"1x A100 40GB\"\n  runtime_seconds: 123.4\n</code></pre>"},{"location":"how_it_works/#transparency","title":"Transparency","text":"<p>Everything is public and auditable:</p> <ul> <li>Submissions: All issues are public</li> <li>Processing: GitHub Actions logs are public</li> <li>History: Git history shows all changes</li> <li>Standards: AI4H DEL3 compliance is documented</li> </ul> <p>This follows the ITU/WHO FG-AI4H principle of transparent, reproducible evaluation.</p>"},{"location":"how_it_works/#faq","title":"FAQ","text":"How long until my model appears on the leaderboard? <p>Typically 2-3 minutes after submitting a valid issue.</p> Can I update my submission? <p>Yes! Submit a new issue with the same <code>model_id</code> but a new <code>eval_id</code>. The leaderboard shows the best score per model.</p> What if validation fails? <p>You'll get a comment explaining the error. Fix your YAML and submit a new issue.</p> Can I submit without using fmbench? <p>Yes, as long as your <code>eval.yaml</code> has the required fields and uses a valid <code>benchmark_id</code>.</p> How do I propose a new benchmark? <p>Open a Discussion with your proposal.</p>"},{"location":"how_it_works/#technical-details","title":"Technical details","text":""},{"location":"how_it_works/#github-actions-workflows","title":"GitHub Actions workflows","text":"Workflow File Purpose Process Submission <code>.github/workflows/process-submission.yml</code> Extracts eval.yaml from issues Update Leaderboard <code>.github/workflows/update-leaderboard.yml</code> Rebuilds and deploys leaderboard CI <code>.github/workflows/ci.yml</code> Tests and docs deployment"},{"location":"how_it_works/#leaderboard-generation","title":"Leaderboard generation","text":"<p>The leaderboard is generated by <code>fmbench/leaderboard.py</code>:</p> <pre><code>python -m fmbench build-leaderboard\n</code></pre> <p>This reads all YAML files from: - <code>benchmarks/</code> \u2014 Benchmark definitions - <code>models/</code> \u2014 Model metadata - <code>datasets/</code> \u2014 Dataset specifications - <code>evals/</code> \u2014 Evaluation results</p> <p>And writes to: - <code>docs/leaderboards/index.md</code> \u2014 The rendered leaderboard</p> <p>Submit your results \u2192 View the leaderboard \u2192</p>"},{"location":"start_here/","title":"Start Here: Researcher Workflow","text":"<p>This page connects the docs into one simple flow:</p> <p>Domain \u2192 Suite \u2192 Run \u2192 Report \u2192 Submit \u2192 Leaderboard</p> <pre><code>flowchart LR\n  A[\"Pick a suite\"] --&gt; B[\"Wrap model\"]\n  B --&gt; C[\"Run locally\"]\n  C --&gt; D[\"Inspect report.md\"]\n  C --&gt; E[\"Submit eval.yaml\"]\n  E --&gt; F[\"Leaderboard entry\"]</code></pre>"},{"location":"start_here/#run-a-toy-benchmark","title":"Run a toy benchmark","text":"<p>Use the toy benchmark to verify your wrapper + evaluation pipeline end-to-end.</p> <pre><code>pip install -e .\npython -m fmbench generate-toy-data\npython -m fmbench run --suite SUITE-TOY-CLASS --model configs/model_dummy_classifier.yaml --out results/toy_run\n</code></pre> <p>Outputs:</p> <ul> <li><code>results/toy_run/report.md</code></li> <li><code>results/toy_run/eval.yaml</code></li> </ul>"},{"location":"start_here/#submit-results-the-fastest-path","title":"Submit results (the fastest path)","text":"<ol> <li>Run locally to produce <code>eval.yaml</code>.</li> <li>Open a submission issue and attach your <code>eval.yaml</code> (and optionally <code>report.md</code>).</li> </ol> <p>Open a submission issue Submission guide</p>"},{"location":"start_here/#scenarios-end-to-end-examples","title":"Scenarios (end-to-end examples)","text":""},{"location":"start_here/#suite-ids-you-can-rely-on","title":"Suite IDs you can rely on","text":"<p>These suite IDs are guaranteed to exist in this repo (see <code>python -m fmbench list-suites</code>):</p> Suite ID What it evaluates How you run it <code>SUITE-TOY-CLASS</code> Toy fMRI-like classification (pipeline sanity check) <code>python -m fmbench run --suite SUITE-TOY-CLASS ...</code> <code>SUITE-ROBUSTNESS-NEURO</code> Neuro robustness probes (dropout/noise/line noise/permutation/shift) <code>python -m fmbench run-robustness ...</code> <code>SUITE-GEN-CLASS-001</code> Genomics classification suite <code>python -m fmbench run --suite SUITE-GEN-CLASS-001 ...</code> <code>SUITE-NEURO-CLASS-001</code> Neurology MRI classification suite <code>python -m fmbench run --suite SUITE-NEURO-CLASS-001 ...</code>"},{"location":"start_here/#scenario-a-evaluate-my-fmri-like-classifier-on-toy-data","title":"Scenario A: Evaluate my fMRI(-like) classifier on toy data","text":"<ul> <li>Step 1 \u2014 Generate toy neuro data:</li> </ul> <pre><code>python -m fmbench generate-toy-data\n</code></pre> <ul> <li>Step 2 \u2014 Point <code>fmbench</code> at your model (via a YAML config)</li> </ul> <pre><code># my_model_config.yaml\nmodel_id: my_fmri_model\nname: \"My fMRI model\"\nversion: \"0.1.0\"\n\ntype: python_class\nimport_path: \"my_model:MyModelWrapper\"\n</code></pre> <ul> <li>Step 3 \u2014 Run the suite:</li> </ul> <pre><code>python -m fmbench run --suite SUITE-TOY-CLASS --model my_model_config.yaml --out results/my_fmri_model_toy\n</code></pre> <ul> <li>Step 4 \u2014 Inspect outputs: open <code>report.md</code>, then submit <code>eval.yaml</code>.</li> </ul>"},{"location":"start_here/#scenario-b-test-robustness-on-neuro-time-series-suite-robustness-neuro","title":"Scenario B: Test robustness on neuro time-series (<code>SUITE-ROBUSTNESS-NEURO</code>)","text":"<pre><code>python -m fmbench run-robustness \\\n  --model my_model_config.yaml \\\n  --data toy_data/neuro/robustness \\\n  --out results/my_fmri_model_robustness \\\n  --probes dropout,noise,line_noise,permutation,shift\n</code></pre> <p>The resulting <code>eval.yaml</code> includes robustness metrics like:</p> <ul> <li><code>dropout_rAUC</code>, <code>noise_rAUC</code>, <code>line_noise_rAUC</code></li> <li><code>perm_equivariance</code></li> <li><code>shift_rAUC</code></li> </ul>"},{"location":"start_here/#scenario-c-sanity-check-a-genomics-model-suite-gen-class-001","title":"Scenario C: Sanity-check a genomics model (<code>SUITE-GEN-CLASS-001</code>)","text":"<pre><code>python -m fmbench generate-toy-data\npython -m fmbench run --suite SUITE-GEN-CLASS-001 --model my_model_config.yaml --out results/my_genomics_toy\n</code></pre> <p>If you don\u2019t see the suite IDs above in <code>list-suites</code>, you\u2019re likely not running from the repo root. This should work:</p> <pre><code>python -m fmbench list-suites\n</code></pre>"},{"location":"start_here/#scenario-d-optional-neurology-fmri-classification-suite-neuro-class-001","title":"Scenario D (optional): Neurology fMRI classification (<code>SUITE-NEURO-CLASS-001</code>)","text":"<p>This suite evaluates fMRI foundation models on classification tasks using toy data.</p> <pre><code>python -m fmbench run --suite SUITE-NEURO-CLASS-001 --model my_model_config.yaml --out results/my_fmri_run\n</code></pre> <p>For full benchmarking, point to your own institutional data (UK Biobank, HCP, etc.).</p>"},{"location":"contributing/submission_guide/","title":"FM Benchmark Submission Guide","text":"<p>Welcome! This guide explains how to submit your Foundation Model's evaluation results to the leaderboard.</p>"},{"location":"contributing/submission_guide/#how-it-works-fully-automated","title":"\ud83d\udd04 How it works (fully automated)","text":"<p>Our submission pipeline is fully automated \u2014 your results appear on the leaderboard within minutes, not days.</p> <pre><code>flowchart TB\n    subgraph local [\"\ud83d\udda5\ufe0f Your Machine\"]\n        A[\"1. Run fmbench locally\"] --&gt; B[\"2. Get eval.yaml\"]\n    end\n\n    subgraph submit [\"\ud83d\udce4 Submit\"]\n        B --&gt; C[\"3. Open GitHub Issue\"]\n        C --&gt; D[\"4. Paste eval.yaml\"]\n    end\n\n    subgraph auto [\"\ud83e\udd16 Automated (GitHub Actions)\"]\n        D --&gt; E[\"5. Bot validates YAML\"]\n        E --&gt; F[\"6. Commits to evals/\"]\n        F --&gt; G[\"7. Rebuilds leaderboard\"]\n        G --&gt; H[\"8. Deploys to Pages\"]\n    end\n\n    H --&gt; I[\"\ud83c\udf89 Live on leaderboard!\"]\n\n    style local fill:#e8f5e9,stroke:#4caf50\n    style submit fill:#fff3e0,stroke:#ff9800\n    style auto fill:#e3f2fd,stroke:#2196f3</code></pre> <p>No manual review required</p> <p>The bot validates your submission automatically. If the YAML is valid, it's added immediately. If there's an error, you'll get feedback in the issue comments.</p>"},{"location":"contributing/submission_guide/#quick-start","title":"Quick Start","text":""},{"location":"contributing/submission_guide/#option-1-use-the-cli-github-issue-recommended","title":"Option 1: Use the CLI + GitHub Issue (Recommended)","text":"<pre><code># 1. Clone and install\ngit clone https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub.git\ncd ai4h-inspired-fm-benchmark-hub\npip install -e .\n\n# 2. Generate toy data for testing\npython -m fmbench generate-toy-data\n\n# 3. Run your model\npython -m fmbench run \\\n    --suite SUITE-TOY-CLASS \\\n    --model path/to/your_model_config.yaml \\\n    --out results/my_model_run\n\n# 4. Submit via GitHub Issue (attach eval.yaml)\n</code></pre> <p>Submit here:</p> <p>Open a pre-filled submission issue</p>"},{"location":"contributing/submission_guide/#option-2-manual-yaml-submission-pull-request","title":"Option 2: Manual YAML Submission (Pull Request)","text":"<p>Create evaluation YAML files manually and submit via PR.</p>"},{"location":"contributing/submission_guide/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"contributing/submission_guide/#1-create-your-model-configuration","title":"1. Create Your Model Configuration","text":"<p>Create a YAML file in <code>configs/</code> describing your model:</p> <pre><code># configs/model_my_awesome_fm.yaml\nmodel_id: my_awesome_fm\nname: \"My Awesome Foundation Model\"\nversion: \"1.0.0\"\n\n# How to load the model\ntype: python_class\nimport_path: \"my_package.models:MyModel\"\ninit_kwargs:\n  checkpoint: \"path/to/checkpoint.pt\"\n\n# Metadata\nmodality: [\"fMRI\", \"EEG\"]  # or \"genomics\", \"MRI\"\npaper: \"https://arxiv.org/abs/xxxx.xxxxx\"\ncode: \"https://github.com/my-org/my-model\"\nlicense: \"MIT\"\n</code></pre>"},{"location":"contributing/submission_guide/#2-run-evaluation","title":"2. Run Evaluation","text":""},{"location":"contributing/submission_guide/#classification-task","title":"Classification Task","text":"<pre><code>python -m fmbench run \\\n    --suite SUITE-NEURO-CLASS-001 \\\n    --model configs/model_my_awesome_fm.yaml \\\n    --out results/my_fm_neuro\n</code></pre>"},{"location":"contributing/submission_guide/#robustness-evaluation","title":"Robustness Evaluation","text":"<pre><code>python -m fmbench run-robustness \\\n    --model configs/model_my_awesome_fm.yaml \\\n    --data toy_data/neuro/robustness \\\n    --out results/my_fm_robustness\n</code></pre>"},{"location":"contributing/submission_guide/#3-review-your-results","title":"3. Review Your Results","text":"<p>Check the generated files:</p> <pre><code>results/my_fm_neuro/\n\u251c\u2500\u2500 eval.yaml      # Evaluation record (submit this!)\n\u2514\u2500\u2500 report.md      # Human-readable report\n</code></pre>"},{"location":"contributing/submission_guide/#what-to-submit-minimum-required","title":"What to submit (minimum required)","text":"<p>To get onto the leaderboard, you only need to submit one file: your <code>eval.yaml</code>.</p> <p>Minimum required fields (anything else is optional but encouraged):</p> <ul> <li><code>eval_id</code>: unique run identifier</li> <li><code>benchmark_id</code>: must match an existing benchmark in <code>benchmarks/</code></li> <li><code>model_ids.candidate</code>: your model ID (string)</li> <li><code>dataset_id</code>: dataset identifier used for the run</li> <li><code>run_metadata</code>: at least <code>runner</code>, plus hardware/runtime if available</li> <li><code>metrics</code>: task-appropriate metrics (e.g., <code>AUROC</code>, <code>Accuracy</code>, robustness rAUCs)</li> <li><code>status</code>: <code>Completed</code> (or explain failures)</li> </ul> <p>Minimal example:</p> <pre><code>eval_id: SUITE-TOY-CLASS-my_model-2025-12-18-120000\nbenchmark_id: BM-TOY-CLASS\nmodel_ids:\n  candidate: my_model_id\ndataset_id: DS-TOY-FMRI-CLASS\nrun_metadata:\n  date: \"2025-12-18\"\n  runner: fmbench\n  suite_id: SUITE-TOY-CLASS\n  hardware: \"1x A100 40GB\"\n  runtime_seconds: 123.4\nmetrics:\n  AUROC: 0.82\n  Accuracy: 0.76\nstatus: Completed\n</code></pre> <p>What a \u201cgood\u201d submission includes (recommended):</p> <ul> <li>Exact command + config used (<code>suite_id</code>, <code>model_config</code>, <code>output_dir</code>)</li> <li>Robustness metrics when applicable (<code>dropout_rAUC</code>, <code>noise_rAUC</code>, etc.)</li> <li>Stratified metrics under <code>metrics.stratified</code> (site/sex/age_group/etc.)</li> <li>Optional: attach <code>report.md</code> for quick human review</li> </ul>"},{"location":"contributing/submission_guide/#4-submit-via-pull-request","title":"4. Submit via Pull Request","text":"<ol> <li>Fork the repository</li> <li>Add your files:</li> <li><code>configs/model_your_model.yaml</code> - Model configuration</li> <li><code>evals/YOUR_EVAL_ID.yaml</code> - Evaluation results</li> <li>(Optional) <code>models/your_model.yaml</code> - Detailed model metadata</li> <li>Create PR with title: <code>[Submission] Your Model Name - Benchmark Name</code></li> </ol>"},{"location":"contributing/submission_guide/#evaluation-yaml-format","title":"Evaluation YAML Format","text":"<p>Your <code>evals/*.yaml</code> file should follow this format:</p> <pre><code># evals/SUITE-NEURO-CLASS-my_fm-20240101-120000.yaml\neval_id: SUITE-NEURO-CLASS-my_fm-20240101-120000\nbenchmark_id: BM-FMRI-GRANULAR  # Must match existing benchmark\n\nmodel_ids:\n  candidate: my_awesome_fm\n\ndataset_id: DS-TOY-FMRI-CLASS  # Or your dataset\n\nrun_metadata:\n  date: \"2024-01-01\"\n  runner: \"fmbench\"\n  hardware: \"NVIDIA A100 80GB\"\n  runtime_seconds: 3600\n\n# Overall metrics\nmetrics:\n  AUROC: 0.92\n  Accuracy: 0.88\n  F1-Score: 0.87\n\n  # Stratified metrics (for granular leaderboards)\n  stratified:\n    scanner:\n      Siemens:\n        AUROC: 0.94\n        Accuracy: 0.90\n        N: 150\n      GE:\n        AUROC: 0.89\n        Accuracy: 0.85\n        N: 120\n      Philips:\n        AUROC: 0.91\n        Accuracy: 0.87\n        N: 100\n\n    acquisition_type:\n      resting_state:\n        AUROC: 0.93\n        Accuracy: 0.89\n        N: 200\n      task_based:\n        AUROC: 0.90\n        Accuracy: 0.86\n        N: 170\n\n    site:\n      Site_A:\n        AUROC: 0.92\n        N: 100\n      Site_B:\n        AUROC: 0.91\n        N: 120\n\nstatus: Completed\n</code></pre>"},{"location":"contributing/submission_guide/#stratification-categories","title":"Stratification Categories","text":"<p>To appear in granular sub-leaderboards, include these stratified metrics:</p>"},{"location":"contributing/submission_guide/#for-fmrineuro","title":"For fMRI/Neuro","text":"Category Values <code>scanner</code> Siemens, GE, Philips <code>acquisition_type</code> resting_state, task_based, naturalistic <code>field_strength</code> 1.5T, 3T, 7T <code>preprocessing</code> fmriprep, hcp, conn, minimal <code>site</code> Your site names"},{"location":"contributing/submission_guide/#for-genomics","title":"For Genomics","text":"Category Values <code>sequencing_platform</code> Illumina, PacBio, ONT <code>cell_type</code> T-cell, B-cell, Monocyte, etc. <code>tissue</code> Blood, Brain, Liver, etc."},{"location":"contributing/submission_guide/#for-all-benchmarks","title":"For All Benchmarks","text":"Category Values <code>sex</code> M, F <code>age_group</code> age_20-40, age_40-60, age_60-80, age_80-100 <code>condition</code> Healthy, Clinical, etc. <code>ethnicity</code> Your categories"},{"location":"contributing/submission_guide/#report-quality-metrics-for-generation-tasks","title":"Report Quality Metrics (for Generation Tasks)","text":"<p>If your model generates reports/text, include these metrics:</p> <pre><code>metrics:\n  # Linguistic\n  bleu: 42.5\n  rouge_l: 0.68\n  bertscore: 0.89\n\n  # Clinical\n  clinical_accuracy: 0.92\n  finding_recall: 0.88\n  finding_precision: 0.91\n  hallucination_rate: 0.05  # Lower is better\n  omission_rate: 0.08       # Lower is better\n\n  # Aggregate\n  report_quality_score: 0.87\n</code></pre>"},{"location":"contributing/submission_guide/#robustness-metrics","title":"Robustness Metrics","text":"<p>For robustness evaluations, include:</p> <pre><code>metrics:\n  robustness_score: 0.78\n  dropout_rAUC: 0.82\n  noise_rAUC: 0.75\n  line_noise_rAUC: 0.80\n  perm_equivariance: 0.85\n  shift_sensitivity: 0.79\n\n  # For genomics\n  masking_rAUC: 0.77\n  expression_rAUC: 0.81\n</code></pre>"},{"location":"contributing/submission_guide/#pr-checklist","title":"PR Checklist","text":"<p>Before submitting, ensure:</p> <ul> <li>[ ] Model config YAML is valid</li> <li>[ ] Eval YAML has correct <code>benchmark_id</code></li> <li>[ ] Metrics include both overall and stratified results</li> <li>[ ] All required metadata is present</li> <li>[ ] Results are reproducible (include seed if applicable)</li> </ul>"},{"location":"contributing/submission_guide/#questions","title":"Questions?","text":"<ul> <li>Open an Issue</li> <li>Check existing Discussions</li> </ul> <p>Thank you for contributing to open FM benchmarking! \ud83c\udf89</p>"},{"location":"design/ai4h_alignment/","title":"AI4H Alignment","text":""},{"location":"design/ai4h_alignment/#overview","title":"Overview","text":"<p>This benchmark hub is explicitly designed to align with the ITU/WHO Focus Group on Artificial Intelligence for Health (FG-AI4H) standards and deliverables. This page documents how our framework implements and extends these standards for foundation model evaluation.</p>"},{"location":"design/ai4h_alignment/#quick-mapping-deliverables-concrete-implementation","title":"Quick mapping: deliverables \u2192 concrete implementation","text":"AI4H deliverable / concept What it means (short) Where it shows up in this hub DEL3 \u2014 System Requirement Specifications (SyRS) Define what an evaluation must do and how it\u2019s validated Benchmark definitions in <code>benchmarks/*.yaml</code> (functional requirements + metrics), and runner outputs (<code>report.md</code>, <code>eval.yaml</code>) produced by <code>fmbench</code> DEL0.1 \u2014 Common unified terms Shared vocabulary so runs are comparable Consistent IDs/terms across YAMLs: <code>benchmark_id</code>, <code>dataset_id</code>, <code>model_id</code>, <code>eval_id</code> used in <code>benchmarks/</code>, <code>datasets/</code>, <code>models/</code>, <code>evals/</code> and the docs DEL10.8 \u2014 Neurology TDD Neurology benchmark structure: topic, scope, inputs, metrics Neurology-style benchmark schemas reflected in <code>benchmarks/*.yaml</code> and supported by stratified metrics in <code>evals/*.yaml</code> (<code>metrics.stratified</code>) DEL7.x / test specifications (in practice) A runnable test suite definition Suites in <code>tests/suite_*.yaml</code> (e.g., <code>SUITE-TOY-CLASS</code>) that define how to run + what artifacts should be produced"},{"location":"design/ai4h_alignment/#itu-fg-ai4h-background","title":"ITU FG-AI4H Background","text":"<p>The ITU/WHO Focus Group on AI for Health was established to develop international standards for AI in healthcare, focusing on:</p> <ul> <li>Safety: Ensuring AI systems are safe for clinical use</li> <li>Effectiveness: Establishing evidence-based validation methods</li> <li>Transparency: Promoting explainability and interpretability</li> <li>Ethics: Addressing fairness, bias, and patient rights</li> <li>Interoperability: Enabling cross-system compatibility</li> </ul>"},{"location":"design/ai4h_alignment/#key-deliverables-used","title":"Key Deliverables Used","text":""},{"location":"design/ai4h_alignment/#del01-common-unified-terms","title":"DEL0.1: Common Unified Terms","text":"<p>Reference: ITU-T FG-AI4H-DEL0.1 (2022)</p> <p>Purpose: Establish standardized terminology for AI4H systems</p> <p>Our Implementation:</p> AI4H Term Our Usage AI Solution Foundation models (e.g., BrainLM, Geneformer) Benchmarking Run Evaluation instance (<code>eval_id</code> in results) Reference Implementation Baseline models (logistic regression, random forest) Health Topic Domain area (e.g., \"Functional Brain Imaging\", \"Genomics\") AI Task ML task type (classification, reconstruction, regression) Test Dataset Standardized evaluation data (e.g., PBMC 3k, HCP fMRI) <p>Example from our schema:</p> <pre><code># From benchmarks/bm_fmri_granular.yaml\nbenchmark_id: BM-FMRI-GRANULAR\nhealth_topic: Functional Brain Imaging Analysis\nai_task: Classification/Reconstruction\n</code></pre>"},{"location":"design/ai4h_alignment/#del3-ai4h-requirement-specifications","title":"DEL3: AI4H Requirement Specifications","text":"<p>Reference: ITU-T FG-AI4H-DEL3 (2023)</p> <p>Purpose: Define System Requirements Specification (SyRS) framework for AI4H systems</p> <p>Our Implementation:</p>"},{"location":"design/ai4h_alignment/#1-functional-requirements-del3-section-4","title":"1. Functional Requirements (DEL3 Section 4)","text":"<p>We define functional requirements for each benchmark:</p> <pre><code># Example: Cell type annotation benchmark\nbenchmark_id: CELL-TYPE-ANNOTATION\nfunctional_requirements:\n  - input: Single-cell RNA-seq count matrix\n  - output: Cell type labels from standardized ontology\n  - performance_threshold: F1 &gt; 0.80 (vs random baseline)\n</code></pre>"},{"location":"design/ai4h_alignment/#2-performance-requirements-del3-section-6","title":"2. Performance Requirements (DEL3 Section 6)","text":"<p>Our leaderboards track multiple performance dimensions:</p> <ul> <li>Accuracy metrics: AUROC, F1-score, balanced accuracy</li> <li>Robustness: rAUC scores under perturbations</li> <li>Resource usage: Inference time, memory footprint</li> <li>Fairness: Stratified performance by demographic groups</li> </ul> <p>Example:</p> <pre><code># From fmbench/runners.py\nmetrics = {\n    'auroc': roc_auc_score(y_true, y_prob),\n    'accuracy': accuracy_score(y_true, y_pred),\n    'f1_score': f1_score(y_true, y_pred, average='weighted'),\n    'stratified': {\n        'by_age': compute_stratified_metrics(y_true, y_pred, age_groups),\n        'by_sex': compute_stratified_metrics(y_true, y_pred, sex_groups),\n    }\n}\n</code></pre>"},{"location":"design/ai4h_alignment/#3-data-requirements-del3-section-42","title":"3. Data Requirements (DEL3 Section 4.2)","text":"<p>We enforce standardized data formats:</p> <ul> <li>Neuroimaging: NIfTI, preprocessed per fMRI specs</li> <li>Genomics: AnnData (scRNA-seq), FASTA (DNA), VCF (variants)</li> <li>Metadata: YAML schema with required fields</li> </ul> <pre><code># From datasets/*.yaml\ndataset_id: pbmc_68k\nname: PBMC 68k\nmodality: scRNA-seq\nn_samples: 68579\npreprocessing: scanpy_1.9.1\nquality_control:\n  min_genes_per_cell: 200\n  max_pct_mito: 5\n</code></pre>"},{"location":"design/ai4h_alignment/#4-validation-verification-del3-section-5","title":"4. Validation &amp; Verification (DEL3 Section 5)","text":"<p>Our framework includes:</p> <ul> <li>\u2705 Cross-validation: Stratified k-fold for robust estimates</li> <li>\u2705 Baseline comparison: Always compare to random, majority, linear baselines</li> <li>\u2705 Statistical significance: Permutation testing, confidence intervals</li> <li>\u2705 Confound control: Partial correlations, matched controls</li> </ul> <p>See our analysis recipes.</p>"},{"location":"design/ai4h_alignment/#del108-topic-description-document-for-neurology","title":"DEL10.8: Topic Description Document for Neurology","text":"<p>Reference: ITU-T FG-AI4H-DEL10.8 (2023)</p> <p>Purpose: Define neurology-specific evaluation standards for the TG-Neuro topic group</p> <p>Our Implementation:</p>"},{"location":"design/ai4h_alignment/#benchmark-structure-following-tdd-template","title":"Benchmark Structure (Following TDD Template)","text":"<p>Each neurology benchmark follows the TDD structure:</p> <pre><code># Example: bm_fmri_granular.yaml\nbenchmark_id: BM-FMRI-GRANULAR\nname: fMRI Foundation Model Benchmark (Granular)\n\n# 1. Health Topic (TDD Section 2)\nhealth_topic: Functional Brain Imaging Analysis\nhealth_domain: Neurology\n\n# 2. Scope (TDD Section 3)\nscope:\n  clinical_context: \"Evaluating FM robustness and representation quality\"\n  population: General population\n\n# 3. Input Data (TDD Section 4)\ninputs:\n  dataset:\n    modality: fMRI\n    sequence: BOLD\n    preprocessing: fMRIPrep or HCP Pipelines\n\n# 4. Evaluation Metrics (TDD Section 5)\nmetrics:\n  primary: AUROC\n  secondary:\n    - Accuracy\n    - F1-Score\n    - Robustness rAUC\n  stratification:\n    - scanner\n    - preprocessing_pipeline\n    - acquisition_type\n\n# 5. Clinical Relevance (TDD Section 6)\nclinical_relevance:\n  use_case: FM robustness and generalization testing\n  impact: Reliable brain imaging AI systems\n</code></pre>"},{"location":"design/ai4h_alignment/#stratified-evaluation","title":"Stratified Evaluation","text":"<p>DEL10.8 emphasizes evaluation across patient subgroups. Our framework automatically computes stratified metrics:</p> <pre><code># From fmbench/runners.py\ndef compute_stratified_metrics(y_true, y_pred, groups):\n    \"\"\"\n    Compute metrics for each subgroup.\n\n    Aligns with DEL10.8 Section 5.3: Subgroup analysis.\n    \"\"\"\n    stratified = {}\n\n    for group_name in np.unique(groups):\n        mask = groups == group_name\n\n        if mask.sum() &gt; 10:  # Minimum sample size\n            stratified[group_name] = {\n                'accuracy': accuracy_score(y_true[mask], y_pred[mask]),\n                'n_samples': mask.sum(),\n            }\n\n    return stratified\n</code></pre> <p>Example output:</p> <pre><code>metrics:\n  accuracy: 0.85\n  auroc: 0.91\n  stratified:\n    by_age_group:\n      '55-65': {accuracy: 0.88, n_samples: 120}\n      '65-75': {accuracy: 0.85, n_samples: 200}\n      '75+': {accuracy: 0.80, n_samples: 80}\n    by_sex:\n      M: {accuracy: 0.84, n_samples: 180}\n      F: {accuracy: 0.86, n_samples: 220}\n</code></pre>"},{"location":"design/ai4h_alignment/#extensions-beyond-ai4h-standards","title":"Extensions Beyond AI4H Standards","text":"<p>While we align with AI4H deliverables, we extend the framework to address foundation model-specific challenges:</p>"},{"location":"design/ai4h_alignment/#1-robustness-testing","title":"1. Robustness Testing","text":"<p>Motivation: Foundation models must handle real-world data variability (noise, artifacts, missing data).</p> <p>Our Framework:  - Inspired by brainaug-lab methodology - Tests model resilience to controlled perturbations - Produces rAUC (Reverse Area Under Curve) scores</p> <p>Probes: - Channel dropout (missing sensors) - Gaussian noise (SNR variation) - Line noise (50/60 Hz artifacts) - Channel permutation (equivariance test) - Temporal shift (timing jitter)</p> <pre><code># Run robustness evaluation\npython -m fmbench run-robustness \\\n    --model configs/model_brainlm.yaml \\\n    --data toy_data/neuro/robustness \\\n    --out results/robustness_eval\n</code></pre> <p>See: Robustness documentation</p>"},{"location":"design/ai4h_alignment/#2-multi-modal-evaluation","title":"2. Multi-Modal Evaluation","text":"<p>Challenge: Many foundation models integrate multiple data types (e.g., imaging + genomics).</p> <p>Our Approach: - CCA-based cross-modal alignment testing - Multi-modal fusion benchmarks - Modality-specific and joint performance metrics</p> <p>See: CCA &amp; Permutation Testing</p>"},{"location":"design/ai4h_alignment/#3-interpretability-explainability","title":"3. Interpretability &amp; Explainability","text":"<p>Planned Features (aligned with DEL3 Section 8): - Attention map visualization - Feature attribution (SHAP, Integrated Gradients) - Embedding space interpretability</p>"},{"location":"design/ai4h_alignment/#compliance-checklist","title":"Compliance Checklist","text":"<p>Use this checklist to verify AI4H alignment for new benchmarks:</p> <ul> <li>[ ] Terminology: Uses standardized AI4H terms (DEL0.1)</li> <li>[ ] Health Topic: Clearly defined clinical context (DEL10.8 Section 2)</li> <li>[ ] Input Specs: Documented data format and preprocessing (DEL3 Section 4.2)</li> <li>[ ] Metrics: Primary and secondary metrics defined (DEL3 Section 6)</li> <li>[ ] Baselines: Comparison to reference implementations (DEL3 Section 7)</li> <li>[ ] Stratification: Performance across relevant subgroups (DEL10.8 Section 5.3)</li> <li>[ ] Clinical Relevance: Justification for clinical use case (DEL10.8 Section 6)</li> <li>[ ] Reproducibility: Code, data, and results publicly available</li> </ul>"},{"location":"design/ai4h_alignment/#governance-contribution","title":"Governance &amp; Contribution","text":""},{"location":"design/ai4h_alignment/#adding-new-benchmarks","title":"Adding New Benchmarks","text":"<p>To propose a new benchmark aligned with AI4H standards:</p> <ol> <li>Define the Health Topic (following DEL10.8 template)</li> <li>Specify Input/Output (following DEL3 Section 4)</li> <li>Choose Metrics (primary + secondary, with clinical justification)</li> <li>Implement Reference Baselines (see Prediction Baselines)</li> <li>Document Clinical Relevance</li> <li>Submit PR with benchmark YAML + documentation</li> </ol>"},{"location":"design/ai4h_alignment/#citing-ai4h-deliverables","title":"Citing AI4H Deliverables","text":"<p>When publishing results from this benchmark hub, please cite:</p> <pre><code>@techreport{itu_ai4h_del3_2023,\n  title = {AI4H requirement specifications},\n  author = {{ITU-T Focus Group on AI for Health}},\n  year = {2023},\n  institution = {International Telecommunication Union},\n  number = {FG-AI4H-DEL3},\n  url = {https://www.itu.int/dms_pub/itu-t/opb/fg/T-FG-AI4H-2023-11-PDF-E.pdf}\n}\n\n@techreport{itu_ai4h_del10_8_2023,\n  title = {Topic Description Document for the Topic Group on AI for neurological disorders (TG-Neuro)},\n  author = {{ITU-T Focus Group on AI for Health}},\n  year = {2023},\n  institution = {International Telecommunication Union},\n  number = {FG-AI4H-DEL10.8},\n  url = {https://www.itu.int/dms_pub/itu-t/opb/fg/T-FG-AI4H-2023-20-PDF-E.pdf}\n}\n</code></pre>"},{"location":"design/ai4h_alignment/#references","title":"References","text":"<ol> <li> <p>ITU-T Focus Group on AI for Health. (2022). Common unified terms in artificial intelligence for health (DEL0.1). PDF</p> </li> <li> <p>ITU-T Focus Group on AI for Health. (2023). AI4H requirement specifications (DEL3). PDF</p> </li> <li> <p>ITU-T Focus Group on AI for Health. (2023). Topic Description Document for the Topic Group on AI for neurological disorders (TG-Neuro) (DEL10.8). PDF</p> </li> <li> <p>Wiegand, T., et al. (2019). WHO and ITU establish benchmarking process for artificial intelligence in health. The Lancet, 394(10192), 9-11.</p> </li> </ol>"},{"location":"design/ai4h_alignment/#contact-feedback","title":"Contact &amp; Feedback","text":"<p>For questions about AI4H alignment or to suggest improvements:</p> <ul> <li>GitHub Issues: Report an issue</li> <li>ITU FG-AI4H: Official website</li> </ul> <p>\u00a9 ITU 2025. AI4H deliverables are available under the Creative Commons Attribution-Non Commercial-Share Alike 3.0 IGO licence (CC BY-NC-SA 3.0 IGO).</p>"},{"location":"integration/analysis_recipes/cca_permutation/","title":"CCA &amp; Permutation Testing","text":""},{"location":"integration/analysis_recipes/cca_permutation/#overview","title":"Overview","text":"<p>Canonical Correlation Analysis (CCA) with permutation testing is a standard protocol for evaluating the relationship between multimodal brain data representations. This recipe provides a reproducible approach for testing whether learned representations capture meaningful biological signal beyond chance.</p>"},{"location":"integration/analysis_recipes/cca_permutation/#background","title":"Background","text":"<p>Canonical Correlation Analysis (CCA) identifies linear combinations of two sets of variables that maximize their correlation. In the context of foundation models:</p> <ul> <li>Use Case 1: Compare model embeddings from different modalities (e.g., fMRI vs genetics)</li> <li>Use Case 2: Test if model representations align with behavioral or clinical outcomes</li> <li>Use Case 3: Validate cross-modal alignment in multi-modal foundation models</li> </ul> <p>Permutation Testing provides statistical significance by: 1. Computing CCA on real data 2. Shuffling one modality randomly N times 3. Computing CCA on each permutation 4. Comparing real correlation against null distribution</p>"},{"location":"integration/analysis_recipes/cca_permutation/#protocol","title":"Protocol","text":""},{"location":"integration/analysis_recipes/cca_permutation/#1-data-preparation","title":"1. Data Preparation","text":"<pre><code>import numpy as np\nfrom sklearn.cross_decomposition import CCA\n\n# Load your model embeddings\n# X: (n_samples, n_features_1) - e.g., fMRI embeddings\n# Y: (n_samples, n_features_2) - e.g., genomic embeddings\n\nX = model_fmri.encode(fmri_data)  # Shape: (N, D1)\nY = model_genomics.encode(genomic_data)  # Shape: (N, D2)\n\n# Standardize\nX = (X - X.mean(axis=0)) / X.std(axis=0)\nY = (Y - Y.mean(axis=0)) / Y.std(axis=0)\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#2-run-cca","title":"2. Run CCA","text":"<pre><code># Number of canonical components to compute\nn_components = min(X.shape[1], Y.shape[1], 20)\n\ncca = CCA(n_components=n_components)\nX_c, Y_c = cca.fit_transform(X, Y)\n\n# Compute canonical correlations\ncanonical_corrs = [np.corrcoef(X_c[:, i], Y_c[:, i])[0, 1] \n                   for i in range(n_components)]\n\nprint(f\"Top 5 canonical correlations: {canonical_corrs[:5]}\")\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#3-permutation-test","title":"3. Permutation Test","text":"<pre><code>from tqdm import tqdm\n\nn_permutations = 1000\nperm_corrs = []\n\nfor _ in tqdm(range(n_permutations)):\n    # Shuffle one modality\n    idx = np.random.permutation(len(Y))\n    Y_perm = Y[idx]\n\n    # Run CCA on permuted data\n    cca_perm = CCA(n_components=n_components)\n    X_c_perm, Y_c_perm = cca_perm.fit_transform(X, Y_perm)\n\n    # Store top canonical correlation\n    perm_corr = np.corrcoef(X_c_perm[:, 0], Y_c_perm[:, 0])[0, 1]\n    perm_corrs.append(perm_corr)\n\nperm_corrs = np.array(perm_corrs)\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#4-compute-p-value","title":"4. Compute P-Value","text":"<pre><code># P-value: fraction of permutations with correlation &gt;= observed\nreal_corr = canonical_corrs[0]\np_value = (perm_corrs &gt;= real_corr).mean()\n\nprint(f\"Real CCA correlation: {real_corr:.4f}\")\nprint(f\"Mean permuted correlation: {perm_corrs.mean():.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\n# Effect size\neffect_size = (real_corr - perm_corrs.mean()) / perm_corrs.std()\nprint(f\"Effect size (Z-score): {effect_size:.2f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#5-visualization","title":"5. Visualization","text":"<pre><code>import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Histogram of permutation null distribution\naxes[0].hist(perm_corrs, bins=50, alpha=0.7, label='Permuted')\naxes[0].axvline(real_corr, color='red', linestyle='--', \n                label=f'Observed (p={p_value:.3f})')\naxes[0].set_xlabel('Canonical Correlation')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('CCA Permutation Test')\naxes[0].legend()\n\n# Scree plot of canonical correlations\naxes[1].plot(range(1, len(canonical_corrs)+1), canonical_corrs, \n             marker='o', label='Real Data')\naxes[1].axhline(perm_corrs.mean(), color='gray', linestyle='--', \n                label='Permuted Mean')\naxes[1].set_xlabel('Canonical Component')\naxes[1].set_ylabel('Correlation')\naxes[1].set_title('Canonical Correlations (Scree Plot)')\naxes[1].legend()\n\nplt.tight_layout()\nplt.savefig('cca_permutation_results.png', dpi=150)\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#interpretation-guidelines","title":"Interpretation Guidelines","text":""},{"location":"integration/analysis_recipes/cca_permutation/#statistical-significance","title":"Statistical Significance","text":"<ul> <li>p &lt; 0.05: Significant alignment between modalities</li> <li>p &lt; 0.01: Strong evidence of cross-modal relationship</li> <li>p &lt; 0.001: Very strong evidence</li> </ul>"},{"location":"integration/analysis_recipes/cca_permutation/#effect-size","title":"Effect Size","text":"<ul> <li>Z &gt; 2: Small to moderate effect</li> <li>Z &gt; 3: Moderate to large effect  </li> <li>Z &gt; 5: Large effect</li> </ul>"},{"location":"integration/analysis_recipes/cca_permutation/#multiple-comparisons","title":"Multiple Comparisons","text":"<p>When testing multiple canonical components, apply correction:</p> <pre><code>from statsmodels.stats.multitest import multipletests\n\n# Compute p-value for each component\np_values = [(perm_corrs &gt;= corr).mean() for corr in canonical_corrs]\n\n# Bonferroni correction\n_, p_corrected, _, _ = multipletests(p_values, method='bonferroni')\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#example-use-cases","title":"Example Use Cases","text":""},{"location":"integration/analysis_recipes/cca_permutation/#use-case-1-multi-modal-foundation-model-validation","title":"Use Case 1: Multi-Modal Foundation Model Validation","text":"<p>Test if a multi-modal brain model's fMRI and genetics branches learn aligned representations:</p> <pre><code># Extract embeddings from each branch\nfmri_emb = model.encode_fmri(fmri_data)\ngene_emb = model.encode_genetics(genetics_data)\n\n# Run CCA + permutation test\n# \u2192 If p &lt; 0.05, model successfully learns cross-modal alignment\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#use-case-2-behavioral-prediction-alignment","title":"Use Case 2: Behavioral Prediction Alignment","text":"<p>Test if brain embeddings correlate with behavioral scores:</p> <pre><code># Brain embeddings (N x D)\nbrain_emb = model.encode(fmri_data)\n\n# Behavioral scores (N x K)\nbehavior_scores = load_behavioral_data()\n\n# Run CCA\n# \u2192 High correlation suggests embeddings capture behaviorally relevant features\n</code></pre>"},{"location":"integration/analysis_recipes/cca_permutation/#itu-ai4h-alignment","title":"ITU AI4H Alignment","text":"<p>This protocol aligns with:</p> <ul> <li>DEL3 Section 5.2: Validation and verification requirements</li> <li>DEL10.8: Neurology-specific evaluation protocols</li> <li>DEL0.1: Statistical significance standards for AI4H benchmarks</li> </ul>"},{"location":"integration/analysis_recipes/cca_permutation/#references","title":"References","text":"<ol> <li>Hotelling, H. (1936). Relations between two sets of variates. Biometrika, 28(3/4), 321-377.</li> <li>Witten, D. M., Tibshirani, R., &amp; Hastie, T. (2009). A penalized matrix decomposition. Biostatistics, 10(3), 515-534.</li> <li>Wang, W., Yan, X., Lee, H., &amp; Livescu, K. (2016). Deep Variational Canonical Correlation Analysis. arXiv preprint arXiv:1610.03454.</li> </ol>"},{"location":"integration/analysis_recipes/cca_permutation/#related-protocols","title":"Related Protocols","text":"<ul> <li>Prediction Baselines</li> <li>Partial Correlations</li> </ul>"},{"location":"integration/analysis_recipes/partial_correlations/","title":"Partial Correlations","text":""},{"location":"integration/analysis_recipes/partial_correlations/#overview","title":"Overview","text":"<p>Partial correlation analysis controls for confounding variables when evaluating relationships between model representations and outcomes. This is essential for neurogenomic studies where age, sex, site, and other covariates can introduce spurious correlations.</p>"},{"location":"integration/analysis_recipes/partial_correlations/#background","title":"Background","text":"<p>Partial Correlation measures the relationship between two variables while controlling for one or more confounding variables.</p> <p>Given: - X: Feature of interest (e.g., brain embedding dimension) - Y: Outcome (e.g., cognitive score) - Z: Confounders (e.g., age, sex, scanner site)</p> <p>The partial correlation between X and Y controlling for Z is the correlation between the residuals of: 1. X regressed on Z 2. Y regressed on Z</p>"},{"location":"integration/analysis_recipes/partial_correlations/#why-partial-correlations-matter","title":"Why Partial Correlations Matter","text":""},{"location":"integration/analysis_recipes/partial_correlations/#common-confounders-in-neurogenomics","title":"Common Confounders in Neurogenomics","text":""},{"location":"integration/analysis_recipes/partial_correlations/#neuroimaging","title":"Neuroimaging","text":"<ul> <li>Age: Affects brain structure and function</li> <li>Sex: Systematic differences in brain anatomy</li> <li>Scanner site: Multi-site studies have acquisition differences</li> <li>Head motion: Correlates with many clinical variables</li> <li>Total intracranial volume (TIV): Affects structural measurements</li> </ul>"},{"location":"integration/analysis_recipes/partial_correlations/#genomics","title":"Genomics","text":"<ul> <li>Batch effects: Technical variation between sequencing runs</li> <li>Cell composition: Proportion of cell types in bulk data</li> <li>Sequencing depth: Coverage differences across samples</li> <li>Population stratification: Ancestry-related genetic variation</li> </ul>"},{"location":"integration/analysis_recipes/partial_correlations/#protocol","title":"Protocol","text":""},{"location":"integration/analysis_recipes/partial_correlations/#1-basic-partial-correlation","title":"1. Basic Partial Correlation","text":"<pre><code>import numpy as np\nfrom scipy.stats import pearsonr\nfrom sklearn.linear_model import LinearRegression\n\ndef partial_correlation(X, Y, Z):\n    \"\"\"\n    Compute partial correlation between X and Y controlling for Z.\n\n    Args:\n        X: (n_samples,) or (n_samples, 1)\n        Y: (n_samples,) or (n_samples, 1)  \n        Z: (n_samples, n_confounds)\n\n    Returns:\n        r: Partial correlation coefficient\n        p: P-value\n    \"\"\"\n    # Reshape if needed\n    X = np.asarray(X).reshape(-1, 1) if X.ndim == 1 else X\n    Y = np.asarray(Y).reshape(-1, 1) if Y.ndim == 1 else Y\n    Z = np.asarray(Z).reshape(-1, 1) if Z.ndim == 1 else Z\n\n    # Regress out confounds from X\n    lr_x = LinearRegression()\n    lr_x.fit(Z, X)\n    X_resid = X - lr_x.predict(Z)\n\n    # Regress out confounds from Y\n    lr_y = LinearRegression()\n    lr_y.fit(Z, Y)\n    Y_resid = Y - lr_y.predict(Z)\n\n    # Correlation of residuals\n    r, p = pearsonr(X_resid.ravel(), Y_resid.ravel())\n\n    return r, p\n\n# Example usage\nfrom sklearn.datasets import make_regression\n\n# Simulated data\nn_samples = 200\nX, Y = make_regression(n_samples=n_samples, n_features=1, noise=10, random_state=42)\nX = X.ravel()\n\n# Confounders (age, sex)\nage = np.random.randn(n_samples)\nsex = np.random.randint(0, 2, n_samples)\nZ = np.column_stack([age, sex])\n\n# Compute partial correlation\nr_partial, p_partial = partial_correlation(X, Y, Z)\nprint(f\"Partial correlation: r={r_partial:.3f}, p={p_partial:.4f}\")\n\n# Compare to raw correlation (without controlling)\nr_raw, p_raw = pearsonr(X, Y)\nprint(f\"Raw correlation: r={r_raw:.3f}, p={p_raw:.4f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#2-multiple-partial-correlations","title":"2. Multiple Partial Correlations","text":"<p>When testing many features (e.g., all embedding dimensions):</p> <pre><code>from statsmodels.stats.multitest import multipletests\n\ndef partial_correlation_matrix(X, Y, Z):\n    \"\"\"\n    Compute partial correlations for multiple features.\n\n    Args:\n        X: (n_samples, n_features) - features to test\n        Y: (n_samples,) - outcome\n        Z: (n_samples, n_confounds) - confounders\n\n    Returns:\n        correlations: (n_features,) - partial correlation coefficients\n        p_values: (n_features,) - uncorrected p-values\n        p_corrected: (n_features,) - FDR-corrected p-values\n    \"\"\"\n    n_features = X.shape[1]\n    correlations = np.zeros(n_features)\n    p_values = np.zeros(n_features)\n\n    for i in range(n_features):\n        r, p = partial_correlation(X[:, i], Y, Z)\n        correlations[i] = r\n        p_values[i] = p\n\n    # FDR correction\n    _, p_corrected, _, _ = multipletests(p_values, method='fdr_bh')\n\n    return correlations, p_values, p_corrected\n\n# Example: Test all embedding dimensions\nembeddings = model.encode(brain_data)  # Shape: (n_samples, embedding_dim)\ncognitive_score = load_cognitive_scores()  # Shape: (n_samples,)\n\nconfounds = np.column_stack([age, sex, site_indicator])\n\ncorrs, p_raw, p_fdr = partial_correlation_matrix(\n    embeddings, \n    cognitive_score, \n    confounds\n)\n\n# Find significant dimensions (FDR &lt; 0.05)\nsig_dims = np.where(p_fdr &lt; 0.05)[0]\nprint(f\"Significant dimensions: {sig_dims}\")\nprint(f\"Correlations: {corrs[sig_dims]}\")\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#3-partial-correlation-with-standardization","title":"3. Partial Correlation with Standardization","text":"<p>For better interpretability, standardize all variables:</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\ndef partial_correlation_standardized(X, Y, Z):\n    \"\"\"Partial correlation with standardized variables.\"\"\"\n    scaler = StandardScaler()\n\n    X_std = scaler.fit_transform(X.reshape(-1, 1)).ravel()\n    Y_std = scaler.fit_transform(Y.reshape(-1, 1)).ravel()\n    Z_std = scaler.fit_transform(Z)\n\n    return partial_correlation(X_std, Y_std, Z_std)\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#4-using-pingouin-library","title":"4. Using pingouin Library","text":"<p>For more advanced partial correlation analysis:</p> <pre><code>import pingouin as pg\n\n# Create DataFrame\nimport pandas as pd\ndf = pd.DataFrame({\n    'brain_feature': X,\n    'cognitive_score': Y,\n    'age': age,\n    'sex': sex\n})\n\n# Partial correlation\nresult = pg.partial_corr(\n    data=df,\n    x='brain_feature',\n    y='cognitive_score',\n    covar=['age', 'sex'],\n    method='pearson'\n)\n\nprint(result)\n# Output: n, r, CI95%, p-val\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#domain-specific-applications","title":"Domain-Specific Applications","text":""},{"location":"integration/analysis_recipes/partial_correlations/#neuroimaging-example-fmri-connectivity-and-behavior","title":"Neuroimaging Example: fMRI Connectivity and Behavior","text":"<pre><code># Load data\nconnectivity = load_fmri_connectivity()  # (n_subjects, n_connections)\nbehavior = load_behavior_score()  # (n_subjects,)\nage = load_age()  # (n_subjects,)\nsex = load_sex()  # (n_subjects,)\nsite = load_site()  # (n_subjects,)\nmotion = load_head_motion()  # (n_subjects,)\n\n# Confound matrix\nZ = np.column_stack([age, sex, site, motion])\n\n# Test each connection\nn_connections = connectivity.shape[1]\nresults = []\n\nfor i in range(n_connections):\n    r, p = partial_correlation(connectivity[:, i], behavior, Z)\n    results.append({'connection_id': i, 'r': r, 'p': p})\n\nresults_df = pd.DataFrame(results)\n\n# FDR correction\n_, results_df['p_fdr'], _, _ = multipletests(\n    results_df['p'], \n    method='fdr_bh'\n)\n\n# Significant connections\nsig_connections = results_df[results_df['p_fdr'] &lt; 0.05]\nprint(f\"Found {len(sig_connections)} significant connections\")\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#genomics-example-gene-expression-and-disease","title":"Genomics Example: Gene Expression and Disease","text":"<pre><code># Load single-cell data\ngene_expression = load_gene_expression()  # (n_cells, n_genes)\ndisease_score = load_disease_phenotype()  # (n_cells,)\n\n# Confounders\nbatch = load_batch_id()  # (n_cells,)\nsequencing_depth = load_total_counts()  # (n_cells,)\ncell_cycle = load_cell_cycle_score()  # (n_cells,)\n\nZ_genomics = np.column_stack([batch, sequencing_depth, cell_cycle])\n\n# Find disease-associated genes (controlling for technical factors)\ncorrs, p_raw, p_fdr = partial_correlation_matrix(\n    gene_expression,\n    disease_score,\n    Z_genomics\n)\n\n# Top disease-associated genes\ntop_genes_idx = np.argsort(np.abs(corrs))[-20:]\nprint(f\"Top disease-associated genes: {gene_names[top_genes_idx]}\")\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#visualization","title":"Visualization","text":""},{"location":"integration/analysis_recipes/partial_correlations/#1-comparison-plot-raw-vs-partial-correlations","title":"1. Comparison Plot: Raw vs Partial Correlations","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Compute both raw and partial correlations\nraw_corrs = []\npartial_corrs = []\n\nfor i in range(X.shape[1]):\n    r_raw, _ = pearsonr(X[:, i], Y)\n    r_partial, _ = partial_correlation(X[:, i], Y, Z)\n    raw_corrs.append(r_raw)\n    partial_corrs.append(r_partial)\n\nraw_corrs = np.array(raw_corrs)\npartial_corrs = np.array(partial_corrs)\n\n# Scatter plot\nfig, ax = plt.subplots(figsize=(6, 6))\nax.scatter(raw_corrs, partial_corrs, alpha=0.6)\nax.plot([-1, 1], [-1, 1], 'k--', label='Identity')\nax.set_xlabel('Raw Correlation')\nax.set_ylabel('Partial Correlation\\n(controlling for confounds)')\nax.set_title('Effect of Confound Correction')\nax.legend()\nax.grid(alpha=0.3)\nplt.tight_layout()\nplt.savefig('raw_vs_partial_correlation.png', dpi=150)\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#2-manhattan-plot-for-genome-wide-analysis","title":"2. Manhattan Plot for Genome-Wide Analysis","text":"<pre><code>def manhattan_plot(p_values, chromosome_positions=None):\n    \"\"\"Create Manhattan plot for partial correlation p-values.\"\"\"\n    fig, ax = plt.subplots(figsize=(14, 4))\n\n    # -log10(p-value) for y-axis\n    neg_log_p = -np.log10(p_values)\n\n    # Color by chromosome (if provided)\n    if chromosome_positions is not None:\n        colors = ['blue', 'orange']\n        for chrom in np.unique(chromosome_positions):\n            mask = chromosome_positions == chrom\n            color = colors[chrom % 2]\n            ax.scatter(np.where(mask)[0], neg_log_p[mask], \n                      c=color, s=5, alpha=0.7)\n    else:\n        ax.scatter(range(len(p_values)), neg_log_p, s=5, alpha=0.7)\n\n    # Significance threshold line\n    ax.axhline(-np.log10(0.05), color='red', linestyle='--', \n               label='p=0.05')\n    ax.axhline(-np.log10(0.05 / len(p_values)), color='green', \n               linestyle='--', label='Bonferroni')\n\n    ax.set_xlabel('Feature Index')\n    ax.set_ylabel('-log\u2081\u2080(p-value)')\n    ax.set_title('Partial Correlation Significance')\n    ax.legend()\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"integration/analysis_recipes/partial_correlations/#interpretation-guidelines","title":"Interpretation Guidelines","text":""},{"location":"integration/analysis_recipes/partial_correlations/#when-to-use-partial-correlations","title":"When to Use Partial Correlations","text":"<p>\u2705 Use when: - Known confounders exist (age, sex, site) - Multi-site studies - Evaluating specific hypotheses while controlling for nuisance variables</p> <p>\u274c Don't use when: - No clear confounders - Confounders are part of the scientific question - Confounders are colliders (can introduce bias)</p>"},{"location":"integration/analysis_recipes/partial_correlations/#effect-size-interpretation","title":"Effect Size Interpretation","text":"<p>| |r| | Interpretation | |------|----------------| | &lt; 0.1 | Negligible | | 0.1 - 0.3 | Small | | 0.3 - 0.5 | Moderate | | &gt; 0.5 | Large |</p>"},{"location":"integration/analysis_recipes/partial_correlations/#statistical-power","title":"Statistical Power","text":"<p>Required sample size for 80% power:</p> Effect Size (r) n (\u03b1=0.05) 0.1 ~780 0.2 ~195 0.3 ~85 0.5 ~30"},{"location":"integration/analysis_recipes/partial_correlations/#itu-ai4h-alignment","title":"ITU AI4H Alignment","text":"<p>This protocol aligns with:</p> <ul> <li>DEL3 Section 5.3: Confound control in validation</li> <li>DEL10.8 Section 4.2: Covariate adjustment in neurology benchmarks</li> <li>DEL0.1: Statistical terminology standards</li> </ul>"},{"location":"integration/analysis_recipes/partial_correlations/#best-practices","title":"Best Practices","text":"<ol> <li>Pre-register confounders: Decide which confounders to control before analysis</li> <li>Report both raw and partial: Show effect of confound correction</li> <li>Visualize confound effects: Plot raw vs. partial correlations</li> <li>Use appropriate corrections: FDR or Bonferroni for multiple comparisons</li> <li>Check assumptions: Linearity, homoscedasticity, normality</li> </ol>"},{"location":"integration/analysis_recipes/partial_correlations/#references","title":"References","text":"<ol> <li>Fisher, R. A. (1924). The distribution of the partial correlation coefficient. Metron, 3, 329-332.</li> <li>Baba, K., et al. (2004). Partial correlation and conditional correlation as measures of conditional independence. Australian &amp; New Zealand Journal of Statistics, 46(4), 657-664.</li> <li>Vallat, R. (2018). Pingouin: statistics in Python. JOSS, 3(31), 1026.</li> </ol>"},{"location":"integration/analysis_recipes/partial_correlations/#related-protocols","title":"Related Protocols","text":"<ul> <li>CCA &amp; Permutation Testing</li> <li>Prediction Baselines</li> </ul>"},{"location":"integration/analysis_recipes/prediction_baselines/","title":"Prediction Baselines","text":""},{"location":"integration/analysis_recipes/prediction_baselines/#overview","title":"Overview","text":"<p>Establishing strong baselines is critical for fair evaluation of foundation models. This protocol defines standardized baseline approaches for common prediction tasks in neurogenomics, aligned with ITU AI4H standards.</p>"},{"location":"integration/analysis_recipes/prediction_baselines/#baseline-types","title":"Baseline Types","text":""},{"location":"integration/analysis_recipes/prediction_baselines/#1-random-baseline","title":"1. Random Baseline","text":"<p>When to use: Always. Provides lower bound for model performance.</p> <pre><code>import numpy as np\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\n# For binary classification\ny_true = np.array([0, 1, 0, 1, ...])  # True labels\ny_pred_random = np.random.randint(0, 2, size=len(y_true))\n\nacc_random = accuracy_score(y_true, y_pred_random)\nprint(f\"Random baseline accuracy: {acc_random:.3f}\")  # ~0.500\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#2-majority-class-baseline","title":"2. Majority Class Baseline","text":"<p>When to use: Classification tasks with class imbalance.</p> <pre><code>from sklearn.dummy import DummyClassifier\n\n# Majority class classifier\nmajority_clf = DummyClassifier(strategy='most_frequent')\nmajority_clf.fit(X_train, y_train)\ny_pred = majority_clf.predict(X_test)\n\nacc_majority = accuracy_score(y_test, y_pred)\nprint(f\"Majority class baseline: {acc_majority:.3f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#3-logistic-regression-linear","title":"3. Logistic Regression (Linear)","text":"<p>When to use: Standard baseline for classification tasks.</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train logistic regression\nlr = LogisticRegression(max_iter=1000, random_state=42)\nlr.fit(X_train_scaled, y_train)\n\n# Evaluate\ny_pred = lr.predict(X_test_scaled)\ny_prob = lr.predict_proba(X_test_scaled)[:, 1]\n\nacc_lr = accuracy_score(y_test, y_pred)\nauc_lr = roc_auc_score(y_test, y_prob)\n\nprint(f\"Logistic Regression - Accuracy: {acc_lr:.3f}, AUC: {auc_lr:.3f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#4-ridge-regression-linear-regularized","title":"4. Ridge Regression (Linear, Regularized)","text":"<p>When to use: High-dimensional data with multicollinearity (e.g., fMRI, genomics).</p> <pre><code>from sklearn.linear_model import Ridge, RidgeCV\n\n# Cross-validated ridge\nalphas = np.logspace(-3, 3, 50)\nridge_cv = RidgeCV(alphas=alphas, cv=5)\nridge_cv.fit(X_train_scaled, y_train)\n\ny_pred = ridge_cv.predict(X_test_scaled)\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Ridge Regression - MSE: {mse:.3f}, R\u00b2: {r2:.3f}\")\nprint(f\"Optimal alpha: {ridge_cv.alpha_:.3f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#5-random-forest-non-linear","title":"5. Random Forest (Non-linear)","text":"<p>When to use: Benchmark for capturing non-linear relationships.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,\n    min_samples_split=10,\n    random_state=42,\n    n_jobs=-1\n)\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\ny_prob = rf.predict_proba(X_test)[:, 1]\n\nacc_rf = accuracy_score(y_test, y_pred)\nauc_rf = roc_auc_score(y_test, y_prob)\n\nprint(f\"Random Forest - Accuracy: {acc_rf:.3f}, AUC: {auc_rf:.3f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#6-k-nearest-neighbors-instance-based","title":"6. k-Nearest Neighbors (Instance-based)","text":"<p>When to use: Baseline for representation quality (in embedding space).</p> <pre><code>from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_scaled, y_train)\n\ny_pred = knn.predict(X_test_scaled)\nacc_knn = accuracy_score(y_test, y_pred)\n\nprint(f\"k-NN baseline: {acc_knn:.3f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#domain-specific-baselines","title":"Domain-Specific Baselines","text":""},{"location":"integration/analysis_recipes/prediction_baselines/#neuroimaging-fmrismri","title":"Neuroimaging (fMRI/sMRI)","text":""},{"location":"integration/analysis_recipes/prediction_baselines/#regional-mean-baseline","title":"Regional Mean Baseline","text":"<p>Use mean signal per brain region as features:</p> <pre><code># Assuming parcellated brain data\n# X_raw: (n_samples, n_voxels)\n# parcellation: (n_voxels,) - region labels\n\nn_regions = len(np.unique(parcellation))\nX_regional = np.zeros((len(X_raw), n_regions))\n\nfor i, region_id in enumerate(np.unique(parcellation)):\n    mask = parcellation == region_id\n    X_regional[:, i] = X_raw[:, mask].mean(axis=1)\n\n# Use X_regional for prediction\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#functional-connectivity-baseline","title":"Functional Connectivity Baseline","text":"<p>Use pairwise correlations as features:</p> <pre><code>from nilearn.connectome import ConnectivityMeasure\n\nconn_measure = ConnectivityMeasure(kind='correlation')\nconnectivity_matrices = conn_measure.fit_transform(timeseries_data)\n\n# Flatten upper triangle as features\nn_samples = len(connectivity_matrices)\nn_regions = connectivity_matrices.shape[1]\nn_features = n_regions * (n_regions - 1) // 2\n\nX_conn = np.zeros((n_samples, n_features))\nfor i, conn_mat in enumerate(connectivity_matrices):\n    X_conn[i] = conn_mat[np.triu_indices(n_regions, k=1)]\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#genomics-scrna-seq","title":"Genomics (scRNA-seq)","text":""},{"location":"integration/analysis_recipes/prediction_baselines/#highly-variable-genes-hvg-baseline","title":"Highly Variable Genes (HVG) Baseline","text":"<p>Use only the most variable genes:</p> <pre><code>from sklearn.feature_selection import VarianceThreshold\n\n# Select top 2000 most variable genes\ngene_vars = X_train.var(axis=0)\ntop_genes_idx = np.argsort(gene_vars)[-2000:]\n\nX_train_hvg = X_train[:, top_genes_idx]\nX_test_hvg = X_test[:, top_genes_idx]\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#cell-type-marker-genes","title":"Cell-Type Marker Genes","text":"<p>Use known marker genes for classification:</p> <pre><code># Pre-defined marker gene list\nmarker_genes = ['CD3D', 'CD79A', 'CST3', 'NKG7', ...]  # Example\nmarker_idx = [gene_names.index(g) for g in marker_genes if g in gene_names]\n\nX_train_markers = X_train[:, marker_idx]\nX_test_markers = X_test[:, marker_idx]\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#baseline-evaluation-protocol","title":"Baseline Evaluation Protocol","text":""},{"location":"integration/analysis_recipes/prediction_baselines/#1-define-metrics","title":"1. Define Metrics","text":"<pre><code>from sklearn.metrics import (\n    accuracy_score, \n    balanced_accuracy_score,\n    f1_score, \n    roc_auc_score,\n    precision_recall_fscore_support\n)\n\ndef evaluate_classifier(y_true, y_pred, y_prob=None):\n    \"\"\"Comprehensive classification metrics.\"\"\"\n    metrics = {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n        'f1_score': f1_score(y_true, y_pred, average='weighted'),\n    }\n\n    if y_prob is not None:\n        metrics['auroc'] = roc_auc_score(y_true, y_prob)\n\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        y_true, y_pred, average='weighted'\n    )\n    metrics.update({\n        'precision': precision,\n        'recall': recall,\n    })\n\n    return metrics\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#2-cross-validation","title":"2. Cross-Validation","text":"<pre><code>from sklearn.model_selection import cross_validate\n\n# 5-fold cross-validation\ncv_results = cross_validate(\n    estimator=lr,\n    X=X_train_scaled,\n    y=y_train,\n    cv=5,\n    scoring=['accuracy', 'roc_auc', 'f1'],\n    return_train_score=True,\n    n_jobs=-1\n)\n\nprint(f\"CV Accuracy: {cv_results['test_accuracy'].mean():.3f} \"\n      f\"\u00b1 {cv_results['test_accuracy'].std():.3f}\")\nprint(f\"CV AUC: {cv_results['test_roc_auc'].mean():.3f} \"\n      f\"\u00b1 {cv_results['test_roc_auc'].std():.3f}\")\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#3-baseline-comparison-table","title":"3. Baseline Comparison Table","text":"<pre><code>import pandas as pd\n\nbaselines = {\n    'Random': (acc_random, None),\n    'Majority Class': (acc_majority, None),\n    'Logistic Regression': (acc_lr, auc_lr),\n    'Ridge': (acc_ridge, auc_ridge),\n    'Random Forest': (acc_rf, auc_rf),\n    'k-NN': (acc_knn, auc_knn),\n}\n\nresults_df = pd.DataFrame([\n    {'Method': name, 'Accuracy': acc, 'AUC': auc}\n    for name, (acc, auc) in baselines.items()\n])\n\nprint(results_df.to_markdown(index=False, floatfmt='.3f'))\n</code></pre>"},{"location":"integration/analysis_recipes/prediction_baselines/#expected-baseline-performance","title":"Expected Baseline Performance","text":""},{"location":"integration/analysis_recipes/prediction_baselines/#neuroimaging-classification-eg-fmri-task-classification","title":"Neuroimaging Classification (e.g., fMRI Task Classification)","text":"Baseline Typical Accuracy Typical AUC Random 0.50 0.50 Majority 0.50-0.60 - Logistic Regression 0.65-0.75 0.70-0.80 Ridge 0.65-0.75 0.70-0.80 Random Forest 0.70-0.80 0.75-0.85"},{"location":"integration/analysis_recipes/prediction_baselines/#single-cell-classification-cell-type","title":"Single-Cell Classification (Cell Type)","text":"Baseline Typical Accuracy Typical F1 Random 0.10-0.20 0.10-0.20 Marker Genes + Logistic 0.75-0.85 0.70-0.80 HVG + Random Forest 0.80-0.90 0.75-0.85"},{"location":"integration/analysis_recipes/prediction_baselines/#itu-ai4h-alignment","title":"ITU AI4H Alignment","text":"<p>This protocol aligns with:</p> <ul> <li>DEL3 Section 6.3: Performance benchmarking requirements</li> <li>DEL3 Section 7.1: Baseline comparison standards</li> <li>DEL0.1: Terminology for baseline models (\"reference implementation\")</li> </ul>"},{"location":"integration/analysis_recipes/prediction_baselines/#best-practices","title":"Best Practices","text":"<ol> <li>Always report random baseline to establish floor performance</li> <li>Use cross-validation for robust baseline estimates</li> <li>Match preprocessing between baselines and foundation models</li> <li>Report confidence intervals (e.g., via bootstrapping)</li> <li>Document hyperparameters for reproducibility</li> </ol>"},{"location":"integration/analysis_recipes/prediction_baselines/#related-protocols","title":"Related Protocols","text":"<ul> <li>CCA &amp; Permutation Testing</li> <li>Partial Correlations</li> </ul>"},{"location":"integration/modality_features/fmri/","title":"fMRI Data Specifications","text":""},{"location":"integration/modality_features/fmri/#overview","title":"Overview","text":"<p>Functional MRI (fMRI) measures brain activity by detecting changes in blood oxygenation (BOLD signal). This page defines the standardized data formats and preprocessing requirements for fMRI data in this benchmark hub.</p>"},{"location":"integration/modality_features/fmri/#data-format-requirements","title":"Data Format Requirements","text":""},{"location":"integration/modality_features/fmri/#1-raw-fmri-time-series","title":"1. Raw fMRI Time Series","text":"<p>Format: NIfTI (<code>.nii</code> or <code>.nii.gz</code>) or NumPy array Shape: <code>(n_samples, n_timepoints, n_voxels)</code> or <code>(n_samples, n_voxels, n_timepoints)</code> Data Type: <code>float32</code> or <code>float64</code></p> <pre><code>import numpy as np\nimport nibabel as nib\n\n# Load NIfTI file\nimg = nib.load('subject_001_bold.nii.gz')\ndata = img.get_fdata()  # Shape: (x, y, z, time)\n\n# Reshape to 2D: (time, voxels)\nn_timepoints = data.shape[-1]\ntimeseries = data.reshape(-1, n_timepoints).T  # (time, voxels)\n</code></pre>"},{"location":"integration/modality_features/fmri/#2-preprocessed-time-series-recommended","title":"2. Preprocessed Time Series (Recommended)","text":"<p>Minimum preprocessing steps: 1. \u2705 Motion correction 2. \u2705 Slice timing correction 3. \u2705 Spatial normalization to standard space (MNI152) 4. \u2705 Nuisance regression (motion parameters, CSF, white matter) 5. \u2705 Bandpass filtering (0.01 - 0.1 Hz typical for resting-state)</p> <p>Optional: - Spatial smoothing (6-8mm FWHM) - Global signal regression (controversial) - Denoising (ICA-AROMA, CompCor)</p>"},{"location":"integration/modality_features/fmri/#3-parcellatedroi-time-series","title":"3. Parcellated/ROI Time Series","text":"<p>Format: NumPy array or CSV Shape: <code>(n_samples, n_timepoints, n_regions)</code></p> <pre><code># Example: 400 parcels (Schaefer atlas), 200 timepoints\nroi_timeseries = np.load('subject_timeseries.npy')  \n# Shape: (1, 200, 400)\n</code></pre> <p>Recommended atlases: - Schaefer 2018 (100-1000 parcels, 7 or 17 networks) - AAL3 (170 regions) - Gordon (333 parcels) - Harvard-Oxford (cortical + subcortical) - Glasser MMP (360 parcels)</p>"},{"location":"integration/modality_features/fmri/#4-connectivity-matrices","title":"4. Connectivity Matrices","text":"<p>Format: NumPy array Shape: <code>(n_samples, n_regions, n_regions)</code> or <code>(n_samples, n_features)</code> (vectorized)</p> <pre><code>from nilearn.connectome import ConnectivityMeasure\n\n# Compute functional connectivity\nconn_measure = ConnectivityMeasure(kind='correlation')\nconnectivity = conn_measure.fit_transform([roi_timeseries])\n# Shape: (1, n_regions, n_regions)\n\n# Vectorize upper triangle (for ML)\nfrom sklearn.utils import check_array\nimport numpy as np\n\ndef vectorize_connectivity(conn_mat):\n    \"\"\"Extract upper triangle as feature vector.\"\"\"\n    n_regions = conn_mat.shape[0]\n    triu_idx = np.triu_indices(n_regions, k=1)\n    return conn_mat[triu_idx]\n\nfeatures = vectorize_connectivity(connectivity[0])\n# Shape: (n_regions * (n_regions-1) / 2,)\n</code></pre>"},{"location":"integration/modality_features/fmri/#metadata-requirements","title":"Metadata Requirements","text":"<p>Each fMRI dataset should include metadata:</p> <pre><code>dataset_id: ukb_fmri_tensor\nname: UK Biobank fMRI Tensors\nmodality: fmri\ntask: resting_state  # or 'task_based'\nn_subjects: 40000\nn_timepoints: 490\nn_voxels: 91282  # or n_regions if parcellated\ntr: 0.735  # Repetition time in seconds\npreprocessing: fmriprep_20.2.0\natlas: Schaefer2018_400  # if parcellated\nbandpass: [0.01, 0.1]  # Hz\nsmoothing: 6mm  # FWHM, or null\nstandard_space: MNI152NLin6Asym\n</code></pre>"},{"location":"integration/modality_features/fmri/#quality-control-metrics","title":"Quality Control Metrics","text":""},{"location":"integration/modality_features/fmri/#1-motion-parameters","title":"1. Motion Parameters","text":"<p>Framewise Displacement (FD): Measure of head motion</p> <pre><code>def framewise_displacement(motion_params):\n    \"\"\"\n    Calculate framewise displacement (Power et al. 2012).\n\n    Args:\n        motion_params: (n_timepoints, 6) - 3 translations + 3 rotations\n\n    Returns:\n        fd: (n_timepoints-1,) - framewise displacement\n    \"\"\"\n    # Translations in mm\n    trans = motion_params[:, :3]\n\n    # Rotations converted to mm (50mm sphere radius)\n    rot = motion_params[:, 3:] * 50  # radians to mm\n\n    # Absolute derivatives\n    dtrans = np.abs(np.diff(trans, axis=0))\n    drot = np.abs(np.diff(rot, axis=0))\n\n    # Sum\n    fd = np.sum(dtrans, axis=1) + np.sum(drot, axis=1)\n\n    return fd\n\n# Recommended threshold: FD &lt; 0.5mm for resting-state\nmean_fd = fd.mean()\nprint(f\"Mean FD: {mean_fd:.3f} mm\")\n\n# Exclude high-motion timepoints (scrubbing)\nlow_motion_frames = fd &lt; 0.5\nclean_timeseries = timeseries[low_motion_frames]\n</code></pre>"},{"location":"integration/modality_features/fmri/#2-temporal-snr-tsnr","title":"2. Temporal SNR (tSNR)","text":"<pre><code>def temporal_snr(timeseries):\n    \"\"\"\n    Temporal signal-to-noise ratio.\n\n    Args:\n        timeseries: (n_timepoints, n_voxels)\n\n    Returns:\n        tsnr: (n_voxels,) - temporal SNR per voxel\n    \"\"\"\n    mean_signal = timeseries.mean(axis=0)\n    std_signal = timeseries.std(axis=0)\n\n    tsnr = mean_signal / (std_signal + 1e-8)\n\n    return tsnr\n\ntsnr = temporal_snr(timeseries)\nprint(f\"Mean tSNR: {tsnr.mean():.2f}\")\n\n# Typical values: 50-100 for 3T, higher for 7T\n</code></pre>"},{"location":"integration/modality_features/fmri/#3-data-completeness","title":"3. Data Completeness","text":"<pre><code># Check for missing data\nn_nan = np.isnan(timeseries).sum()\ncompleteness = 1 - (n_nan / timeseries.size)\nprint(f\"Data completeness: {completeness*100:.1f}%\")\n\n# Minimum recommended: 95% completeness\nassert completeness &gt; 0.95, \"Too much missing data\"\n</code></pre>"},{"location":"integration/modality_features/fmri/#data-augmentation-for-robustness-testing","title":"Data Augmentation for Robustness Testing","text":"<p>See our robustness testing documentation for details on perturbations:</p> <pre><code>from fmbench.robustness import (\n    ChannelDropout,\n    GaussianNoise,\n    LineNoise,\n    TemporalShift\n)\n\n# Example: Add Gaussian noise\nnoise_probe = GaussianNoise(snr_db=10)\nnoisy_data = noise_probe.apply(timeseries)\n</code></pre>"},{"location":"integration/modality_features/fmri/#example-data-loading","title":"Example Data Loading","text":""},{"location":"integration/modality_features/fmri/#from-nifti","title":"From NIfTI","text":"<pre><code>import nibabel as nib\nfrom nilearn.maskers import NiftiMasker\n\n# Load functional image\nfunc_img = nib.load('subject_001_bold.nii.gz')\n\n# Apply brain mask and extract timeseries\nmasker = NiftiMasker(\n    standardize=True,\n    detrend=True,\n    low_pass=0.1,\n    high_pass=0.01,\n    t_r=2.0\n)\ntimeseries = masker.fit_transform(func_img)\n# Shape: (n_timepoints, n_voxels)\n</code></pre>"},{"location":"integration/modality_features/fmri/#from-parcellated-csv","title":"From Parcellated CSV","text":"<pre><code>import pandas as pd\n\n# Load ROI timeseries\ndf = pd.read_csv('subject_001_schaefer400.csv')\n# Columns: timepoint, region_1, region_2, ..., region_400\n\ntimeseries = df.iloc[:, 1:].values  # Exclude timepoint column\n# Shape: (n_timepoints, 400)\n</code></pre>"},{"location":"integration/modality_features/fmri/#from-hcp-style-data","title":"From HCP-style Data","text":"<pre><code># HCP stores timeseries as CIFTI (cortical surface + subcortical)\nfrom nibabel import cifti2\n\ncifti_img = cifti2.load('subject_REST_LR.dtseries.nii')\ntimeseries = cifti_img.get_fdata()\n# Shape: (n_timepoints, 91282) - 91k grayordinates\n</code></pre>"},{"location":"integration/modality_features/fmri/#benchmark-tasks","title":"Benchmark Tasks","text":""},{"location":"integration/modality_features/fmri/#1-classification","title":"1. Classification","text":"<p>Typical tasks: - Disease vs. Control (AD, ADHD, ASD, schizophrenia) - Cognitive state classification - Task decoding</p> <p>Input: <code>(n_samples, n_timepoints, n_regions)</code> or connectivity matrices Output: Class labels <code>(n_samples,)</code></p>"},{"location":"integration/modality_features/fmri/#2-reconstruction","title":"2. Reconstruction","text":"<p>Typical tasks: - Masked autoencoding (predict masked timepoints/regions) - Denoising - Super-resolution (spatial or temporal)</p> <p>Input: Masked/noisy timeseries Output: Clean timeseries</p>"},{"location":"integration/modality_features/fmri/#3-regression","title":"3. Regression","text":"<p>Typical tasks: - Cognitive score prediction - Age prediction - Symptom severity prediction</p> <p>Input: Timeseries Output: Continuous values <code>(n_samples,)</code></p>"},{"location":"integration/modality_features/fmri/#itu-ai4h-alignment","title":"ITU AI4H Alignment","text":"<p>This specification aligns with:</p> <ul> <li>DEL10.8 Section 3.1: Input data specifications for neurology</li> <li>DEL3 Section 4.2: Data format requirements</li> <li>DEL0.1: Standardized terminology (BOLD, fMRI, parcellation)</li> </ul>"},{"location":"integration/modality_features/fmri/#tools-libraries","title":"Tools &amp; Libraries","text":""},{"location":"integration/modality_features/fmri/#preprocessing","title":"Preprocessing","text":"<ul> <li>fMRIPrep: Robust preprocessing pipeline</li> <li>CONN Toolbox: Connectivity preprocessing</li> <li>DPARSF: Data Processing Assistant for Resting-State fMRI</li> </ul>"},{"location":"integration/modality_features/fmri/#analysis","title":"Analysis","text":"<ul> <li>Nilearn: Machine learning for neuroimaging</li> <li>NiBabel: Read/write neuroimaging formats</li> <li>BrainIAK: Brain Imaging Analysis Kit</li> </ul>"},{"location":"integration/modality_features/fmri/#parcellation","title":"Parcellation","text":"<ul> <li>Schaefer2018: <code>nilearn.datasets.fetch_atlas_schaefer_2018()</code></li> <li>AAL3: <code>nilearn.datasets.fetch_atlas_aal()</code></li> </ul>"},{"location":"integration/modality_features/fmri/#references","title":"References","text":"<ol> <li>Esteban, O., et al. (2019). fMRIPrep: a robust preprocessing pipeline for fMRI data. Nature Methods, 16(1), 111-116.</li> <li>Power, J. D., et al. (2012). Spurious but systematic correlations in functional connectivity MRI. NeuroImage, 59(3), 2142-2154.</li> <li>Schaefer, A., et al. (2018). Local-Global Parcellation of the Human Cerebral Cortex. Cerebral Cortex, 28(9), 3095-3114.</li> </ol>"},{"location":"integration/modality_features/fmri/#related-documentation","title":"Related Documentation","text":"<ul> <li>sMRI Specifications</li> <li>Genomics Specifications</li> <li>Robustness Testing</li> </ul>"},{"location":"integration/modality_features/genomics/","title":"Genomics Data Specifications","text":""},{"location":"integration/modality_features/genomics/#overview","title":"Overview","text":"<p>This page defines standardized data formats and preprocessing requirements for genomics data in the benchmark hub, covering single-cell RNA-seq (scRNA-seq), bulk RNA-seq, DNA sequences, and variant data.</p>"},{"location":"integration/modality_features/genomics/#data-modalities","title":"Data Modalities","text":""},{"location":"integration/modality_features/genomics/#1-single-cell-rna-seq-scrna-seq","title":"1. Single-Cell RNA-seq (scRNA-seq)","text":"<p>Format: AnnData (<code>.h5ad</code>), Loom (<code>.loom</code>), or CSV/TSV matrices Shape: <code>(n_cells, n_genes)</code> Data Type: Raw counts (integer) or normalized (float)</p> <pre><code>import scanpy as sc\nimport numpy as np\n\n# Load single-cell data\nadata = sc.read_h5ad('pbmc_68k.h5ad')\n\n# Inspect\nprint(f\"Shape: {adata.shape}\")  # (n_cells, n_genes)\nprint(f\"Genes: {adata.var_names[:5]}\")\nprint(f\"Cells: {adata.obs_names[:5]}\")\n\n# Access count matrix\nX = adata.X  # Sparse or dense matrix (n_cells, n_genes)\n</code></pre>"},{"location":"integration/modality_features/genomics/#required-metadata-adataobs","title":"Required Metadata (adata.obs)","text":"<pre><code># Cell-level metadata\nadata.obs['cell_type']  # Cell type annotations\nadata.obs['donor_id']  # Subject/donor identifier\nadata.obs['batch']  # Batch/experiment ID\nadata.obs['n_genes']  # Number of detected genes\nadata.obs['n_counts']  # Total UMI counts\nadata.obs['percent_mito']  # % mitochondrial gene expression\n</code></pre>"},{"location":"integration/modality_features/genomics/#gene-metadata-adatavar","title":"Gene Metadata (adata.var)","text":"<pre><code># Gene-level metadata\nadata.var['gene_ids']  # Ensembl IDs\nadata.var['gene_symbols']  # HGNC symbols\nadata.var['highly_variable']  # Boolean for HVG selection\nadata.var['mean_counts']  # Mean expression\nadata.var['dispersions']  # Dispersion (variability)\n</code></pre>"},{"location":"integration/modality_features/genomics/#2-bulk-rna-seq","title":"2. Bulk RNA-seq","text":"<p>Format: NumPy array, CSV, or TSV Shape: <code>(n_samples, n_genes)</code> Data Type: Raw counts, TPM, or FPKM</p> <pre><code>import pandas as pd\n\n# Load bulk RNA-seq data\ngene_expression = pd.read_csv('bulk_rnaseq_tpm.csv', index_col=0)\n# Rows: genes, Columns: samples\n\n# Transpose to (samples, genes) for ML\nX = gene_expression.T.values  # (n_samples, n_genes)\n</code></pre>"},{"location":"integration/modality_features/genomics/#3-dna-sequences","title":"3. DNA Sequences","text":"<p>Format: FASTA (<code>.fasta</code>, <code>.fa</code>), plain text, or tokenized arrays Alphabet: <code>{A, C, G, T, N}</code> (N = ambiguous)</p> <pre><code>from Bio import SeqIO\n\n# Load FASTA file\nsequences = []\nfor record in SeqIO.parse(\"sequences.fasta\", \"fasta\"):\n    sequences.append(str(record.seq))\n\n# Tokenize sequences (for transformer models)\nvocab = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 4, '&lt;PAD&gt;': 5}\n\ndef tokenize_sequence(seq, vocab, max_len=512):\n    \"\"\"Convert DNA sequence to token IDs.\"\"\"\n    tokens = [vocab.get(base, vocab['N']) for base in seq.upper()]\n\n    # Pad or truncate\n    if len(tokens) &lt; max_len:\n        tokens += [vocab['&lt;PAD&gt;']] * (max_len - len(tokens))\n    else:\n        tokens = tokens[:max_len]\n\n    return np.array(tokens)\n\ntokenized = [tokenize_sequence(seq, vocab) for seq in sequences]\nX_dna = np.array(tokenized)  # (n_sequences, max_len)\n</code></pre>"},{"location":"integration/modality_features/genomics/#4-variant-data-vcf","title":"4. Variant Data (VCF)","text":"<p>Format: VCF (Variant Call Format) Use: SNP arrays, whole-genome sequencing (WGS), whole-exome sequencing (WES)</p> <pre><code>import pandas as pd\nimport allel\n\n# Load VCF file\nvcf_path = 'cohort_variants.vcf.gz'\ncallset = allel.read_vcf(vcf_path)\n\n# Extract genotypes\ngenotypes = callset['calldata/GT']  # (n_variants, n_samples, ploidy=2)\n\n# Convert to dosage (0, 1, 2 for diploid)\ndosage = genotypes.sum(axis=-1)  # (n_variants, n_samples)\n\n# Transpose for ML: (samples, variants)\nX_geno = dosage.T\n</code></pre>"},{"location":"integration/modality_features/genomics/#preprocessing-workflows","title":"Preprocessing Workflows","text":""},{"location":"integration/modality_features/genomics/#scrna-seq-standard-preprocessing","title":"scRNA-seq Standard Preprocessing","text":"<pre><code>import scanpy as sc\n\n# Load raw counts\nadata = sc.read_h5ad('raw_counts.h5ad')\n\n# 1. Quality control\nsc.pp.filter_cells(adata, min_genes=200)  # Remove low-quality cells\nsc.pp.filter_genes(adata, min_cells=3)  # Remove rare genes\n\n# Calculate QC metrics\nadata.var['mt'] = adata.var_names.str.startswith('MT-')\nsc.pp.calculate_qc_metrics(\n    adata, \n    qc_vars=['mt'], \n    percent_top=None, \n    log1p=False, \n    inplace=True\n)\n\n# Filter cells by QC\nadata = adata[adata.obs.n_genes_by_counts &lt; 2500, :]\nadata = adata[adata.obs.pct_counts_mt &lt; 5, :]\n\n# 2. Normalization\nsc.pp.normalize_total(adata, target_sum=1e4)  # CPM-like\nsc.pp.log1p(adata)  # Log-transform\n\n# 3. Feature selection\nsc.pp.highly_variable_genes(adata, n_top_genes=2000)\nadata = adata[:, adata.var.highly_variable]  # Keep only HVGs\n\n# 4. Scaling (for PCA/visualization, not always for ML)\nsc.pp.scale(adata, max_value=10)\n\n# Extract preprocessed data\nX_processed = adata.X  # (n_cells, 2000)\n</code></pre>"},{"location":"integration/modality_features/genomics/#bulk-rna-seq-preprocessing","title":"Bulk RNA-seq Preprocessing","text":"<pre><code>from sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Load TPM or FPKM data\nX = gene_expression.values  # (n_samples, n_genes)\n\n# 1. Log-transform (if not already)\nX_log = np.log1p(X)\n\n# 2. Filter low-expression genes\ngene_means = X_log.mean(axis=0)\nhigh_expr_genes = gene_means &gt; 1.0  # Threshold\nX_filtered = X_log[:, high_expr_genes]\n\n# 3. Z-score normalization (optional)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_filtered)\n</code></pre>"},{"location":"integration/modality_features/genomics/#dna-sequence-preprocessing","title":"DNA Sequence Preprocessing","text":"<pre><code># K-mer encoding (alternative to tokenization)\ndef generate_kmers(sequence, k=3):\n    \"\"\"Generate k-mer features from DNA sequence.\"\"\"\n    kmers = []\n    for i in range(len(sequence) - k + 1):\n        kmers.append(sequence[i:i+k])\n    return kmers\n\n# Build k-mer vocabulary\nfrom collections import Counter\n\nall_kmers = []\nfor seq in sequences:\n    all_kmers.extend(generate_kmers(seq, k=3))\n\nkmer_vocab = {kmer: i for i, kmer in enumerate(set(all_kmers))}\n\n# Encode sequence as k-mer counts\ndef encode_kmers(sequence, kmer_vocab, k=3):\n    \"\"\"Encode sequence as k-mer count vector.\"\"\"\n    kmers = generate_kmers(sequence, k)\n    counts = Counter(kmers)\n\n    vec = np.zeros(len(kmer_vocab))\n    for kmer, count in counts.items():\n        if kmer in kmer_vocab:\n            vec[kmer_vocab[kmer]] = count\n\n    return vec\n</code></pre>"},{"location":"integration/modality_features/genomics/#data-augmentation-for-scrna-seq","title":"Data Augmentation for scRNA-seq","text":""},{"location":"integration/modality_features/genomics/#1-poisson-sampling-simulates-sequencing-depth-variation","title":"1. Poisson Sampling (Simulates sequencing depth variation)","text":"<pre><code>def poisson_augment(counts, factor=0.8):\n    \"\"\"\n    Simulate different sequencing depths via Poisson sampling.\n\n    Args:\n        counts: (n_cells, n_genes) - raw count matrix\n        factor: Downsampling factor (0 &lt; factor &lt; 1)\n\n    Returns:\n        augmented: Downsampled count matrix\n    \"\"\"\n    total_counts = counts.sum(axis=1, keepdims=True)\n    target_counts = total_counts * factor\n\n    # Multinomial sampling\n    probs = counts / (total_counts + 1e-8)\n    augmented = np.array([\n        np.random.multinomial(int(tc), p)\n        for tc, p in zip(target_counts, probs)\n    ])\n\n    return augmented\n</code></pre>"},{"location":"integration/modality_features/genomics/#2-cell-mixtures-simulates-doublets","title":"2. Cell Mixtures (Simulates doublets)","text":"<pre><code>def create_doublet(cell1, cell2):\n    \"\"\"Simulate doublet by averaging two cells.\"\"\"\n    return (cell1 + cell2) / 2\n</code></pre>"},{"location":"integration/modality_features/genomics/#benchmark-tasks","title":"Benchmark Tasks","text":""},{"location":"integration/modality_features/genomics/#1-cell-type-annotation-scrna-seq","title":"1. Cell Type Annotation (scRNA-seq)","text":"<p>Input: Gene expression matrix <code>(n_cells, n_genes)</code> Output: Cell type labels <code>(n_cells,)</code></p> <pre><code># Example: PBMC cell type classification\nX_train = adata_train.X  # (n_train, n_genes)\ny_train = adata_train.obs['cell_type'].values  # (n_train,)\n\n# Labels: B cells, T cells, Monocytes, NK cells, etc.\n</code></pre>"},{"location":"integration/modality_features/genomics/#2-gene-expression-prediction","title":"2. Gene Expression Prediction","text":"<p>Input: Partial gene expression or other modalities Output: Full gene expression profiles</p>"},{"location":"integration/modality_features/genomics/#3-variant-effect-prediction","title":"3. Variant Effect Prediction","text":"<p>Input: DNA sequences with variants Output: Pathogenicity scores or functional effects</p>"},{"location":"integration/modality_features/genomics/#4-sequence-classification","title":"4. Sequence Classification","text":"<p>Input: DNA sequences Output: Labels (e.g., promoter vs non-promoter, coding vs non-coding)</p>"},{"location":"integration/modality_features/genomics/#quality-control-metrics","title":"Quality Control Metrics","text":""},{"location":"integration/modality_features/genomics/#scrna-seq-qc","title":"scRNA-seq QC","text":"<pre><code># Check key QC metrics\nprint(f\"Mean genes/cell: {adata.obs['n_genes'].mean():.0f}\")\nprint(f\"Mean UMIs/cell: {adata.obs['n_counts'].mean():.0f}\")\nprint(f\"Mean % mito: {adata.obs['percent_mito'].mean():.2f}%\")\n\n# Recommended thresholds:\n# - n_genes: 200 - 5000\n# - percent_mito: &lt; 5-10%\n# - n_counts: &gt; 500\n</code></pre>"},{"location":"integration/modality_features/genomics/#variant-data-qc","title":"Variant Data QC","text":"<pre><code># Check missing rate\nmissing_rate = (genotypes == -1).mean()\nprint(f\"Missing genotype rate: {missing_rate*100:.2f}%\")\n\n# Filter variants by minor allele frequency (MAF)\nfrom allel import GenotypeArray\n\ngt = GenotypeArray(genotypes)\nallele_counts = gt.count_alleles()\nmaf = allele_counts[:, 1] / allele_counts.sum(axis=1)\n\n# Keep MAF &gt; 0.01\nkeep_variants = maf &gt; 0.01\n</code></pre>"},{"location":"integration/modality_features/genomics/#data-formats","title":"Data Formats","text":""},{"location":"integration/modality_features/genomics/#anndata-structure","title":"AnnData Structure","text":"<pre><code># Recommended AnnData structure for benchmarking\nadata.X  # Main data matrix (can be sparse)\nadata.obs  # Cell/sample metadata (pandas DataFrame)\nadata.var  # Gene/feature metadata (pandas DataFrame)\nadata.layers  # Alternative representations (raw, scaled, etc.)\nadata.obsm  # Multi-dimensional annotations (PCA, UMAP)\nadata.varm  # Multi-dimensional gene annotations\nadata.uns  # Unstructured metadata (dataset info)\n</code></pre>"},{"location":"integration/modality_features/genomics/#example-dataset-metadata","title":"Example Dataset Metadata","text":"<pre><code>dataset_id: pbmc_68k\nname: PBMC 68k (10x Genomics)\nmodality: scRNA-seq\nn_cells: 68579\nn_genes: 20387\ntechnology: 10x Chromium v2\norganism: Homo sapiens\ntissue: Peripheral blood mononuclear cells (PBMC)\npreprocessing: Scanpy 1.9.1\nnormalization: log1p(CPM)\nfeature_selection: top_2000_HVG\nreference: Zheng et al. (2017)\n</code></pre>"},{"location":"integration/modality_features/genomics/#foundation-model-input-formats","title":"Foundation Model Input Formats","text":""},{"location":"integration/modality_features/genomics/#for-geneformer-and-similar-models","title":"For Geneformer and Similar Models","text":"<pre><code># Geneformer expects rank-ordered gene tokens\ndef prepare_geneformer_input(adata, n_genes=2048):\n    \"\"\"\n    Convert AnnData to Geneformer input format.\n\n    Returns ranked gene indices per cell.\n    \"\"\"\n    # Rank genes by expression within each cell\n    cell_inputs = []\n\n    for i in range(adata.n_obs):\n        cell_expr = adata.X[i].toarray().ravel()\n\n        # Rank genes (descending)\n        ranked_genes = np.argsort(cell_expr)[::-1][:n_genes]\n\n        cell_inputs.append(ranked_genes)\n\n    return np.array(cell_inputs)  # (n_cells, n_genes)\n</code></pre>"},{"location":"integration/modality_features/genomics/#for-dna-sequence-models-dnabert-hyenadna-etc","title":"For DNA Sequence Models (DNABERT, HyenaDNA, etc.)","text":"<pre><code># K-mer tokenization for DNABERT\ndef kmer_tokenize(sequence, k=6, stride=1):\n    \"\"\"\n    Tokenize DNA sequence into k-mers.\n\n    Args:\n        sequence: DNA string\n        k: K-mer size\n        stride: Step size\n\n    Returns:\n        tokens: List of k-mer strings\n    \"\"\"\n    tokens = []\n    for i in range(0, len(sequence) - k + 1, stride):\n        tokens.append(sequence[i:i+k])\n\n    return tokens\n\n# Example\nseq = \"ATCGATCGATCG\"\ntokens = kmer_tokenize(seq, k=6, stride=3)\nprint(tokens)  # ['ATCGAT', 'GATCGA', 'TCGATC']\n</code></pre>"},{"location":"integration/modality_features/genomics/#itu-ai4h-alignment","title":"ITU AI4H Alignment","text":"<p>This specification aligns with:</p> <ul> <li>DEL3 Section 4.2: Data format requirements for AI4H benchmarks</li> <li>DEL0.1: Genomics terminology standardization</li> <li>FAIR principles: Findable, Accessible, Interoperable, Reusable data</li> </ul>"},{"location":"integration/modality_features/genomics/#tools-libraries","title":"Tools &amp; Libraries","text":""},{"location":"integration/modality_features/genomics/#scrna-seq","title":"scRNA-seq","text":"<ul> <li>Scanpy: Single-cell analysis in Python</li> <li>Seurat: R toolkit for scRNA-seq</li> <li>scVI-tools: Probabilistic models for single-cell omics</li> </ul>"},{"location":"integration/modality_features/genomics/#genomics","title":"Genomics","text":"<ul> <li>BioPython: Sequence analysis</li> <li>PyVCF / scikit-allel: Variant data handling</li> <li>pybedtools: Genomic interval operations</li> </ul>"},{"location":"integration/modality_features/genomics/#foundation-model-libraries","title":"Foundation Model Libraries","text":"<ul> <li>Geneformer: Transformer for scRNA-seq</li> <li>DNABERT: BERT for DNA sequences</li> <li>Nucleotide Transformer: Multi-species genomic foundation model</li> </ul>"},{"location":"integration/modality_features/genomics/#references","title":"References","text":"<ol> <li>Wolf, F. A., et al. (2018). SCANPY: large-scale single-cell gene expression data analysis. Genome Biology, 19(1), 15.</li> <li>Theodoris, C. V., et al. (2023). Transfer learning enables predictions in network biology. Nature, 618, 616-624. (Geneformer)</li> <li>Ji, Y., et al. (2021). DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome. Bioinformatics, 37(15), 2112-2120.</li> </ol>"},{"location":"integration/modality_features/genomics/#related-documentation","title":"Related Documentation","text":"<ul> <li>fMRI Specifications</li> <li>sMRI Specifications</li> <li>Prediction Baselines</li> </ul>"},{"location":"integration/modality_features/smri/","title":"sMRI Data Specifications","text":""},{"location":"integration/modality_features/smri/#overview","title":"Overview","text":"<p>Structural MRI (sMRI) captures anatomical brain structure, typically using T1-weighted or T2-weighted sequences. This page defines standardized data formats and preprocessing requirements for sMRI data in this benchmark hub.</p>"},{"location":"integration/modality_features/smri/#data-format-requirements","title":"Data Format Requirements","text":""},{"location":"integration/modality_features/smri/#1-raw-structural-mri","title":"1. Raw Structural MRI","text":"<p>Format: NIfTI (<code>.nii</code> or <code>.nii.gz</code>) Typical Shape: <code>(256, 256, 256)</code> at 1mm\u00b3 isotropic resolution Data Type: <code>float32</code> or <code>int16</code> Contrast: T1-weighted (most common), T2-weighted, FLAIR, or multi-contrast</p> <pre><code>import nibabel as nib\nimport numpy as np\n\n# Load T1-weighted image\nimg = nib.load('subject_001_T1w.nii.gz')\ndata = img.get_fdata()  # Shape: (x, y, z)\n\nprint(f\"Image shape: {data.shape}\")\nprint(f\"Voxel size: {img.header.get_zooms()[:3]} mm\")\n</code></pre>"},{"location":"integration/modality_features/smri/#2-preprocessed-structural-images","title":"2. Preprocessed Structural Images","text":"<p>Minimum preprocessing: 1. \u2705 Skull stripping / brain extraction 2. \u2705 Bias field correction (inhomogeneity correction) 3. \u2705 Spatial normalization to standard space (MNI152) 4. \u2705 Segmentation (GM, WM, CSF)</p> <p>Optional: - Denoising - Resolution standardization - Intensity normalization</p> <pre><code># Example: Load preprocessed brain-extracted image\nbrain_img = nib.load('subject_001_T1w_brain.nii.gz')\nbrain_data = brain_img.get_fdata()\n</code></pre>"},{"location":"integration/modality_features/smri/#3-derived-features","title":"3. Derived Features","text":""},{"location":"integration/modality_features/smri/#a-tissue-segmentation-maps","title":"A. Tissue Segmentation Maps","text":"<p>Format: NIfTI (probability maps or binary masks) Channels: Gray matter (GM), White matter (WM), CSF</p> <pre><code># Load tissue probability maps from FreeSurfer/FSL/SPM\ngm_prob = nib.load('subject_001_GM_prob.nii.gz').get_fdata()\nwm_prob = nib.load('subject_001_WM_prob.nii.gz').get_fdata()\ncsf_prob = nib.load('subject_001_CSF_prob.nii.gz').get_fdata()\n\n# Stack as multi-channel input (for CNNs)\ntissue_stack = np.stack([gm_prob, wm_prob, csf_prob], axis=-1)\n# Shape: (x, y, z, 3)\n</code></pre>"},{"location":"integration/modality_features/smri/#b-freesurfer-derived-metrics","title":"B. FreeSurfer-Derived Metrics","text":"<p>Format: CSV or NumPy array Features: Regional volumes, cortical thickness, surface area</p> <pre><code>import pandas as pd\n\n# Load FreeSurfer stats\nfs_stats = pd.read_csv('subject_001_aparc_stats.csv')\n# Columns: region, volume_mm3, thickness_mm, surface_area_mm2\n\n# Extract feature vector\nfeatures = fs_stats[['volume_mm3', 'thickness_mm']].values.flatten()\n# Shape: (n_regions * 2,)\n</code></pre> <p>Common FreeSurfer outputs: - <code>aseg.stats</code>: Subcortical volumes (hippocampus, amygdala, etc.) - <code>aparc.stats</code>: Cortical parcellation (Desikan-Killiany atlas, 68 regions) - <code>aparc.a2009s.stats</code>: Destrieux atlas (148 regions)</p>"},{"location":"integration/modality_features/smri/#c-voxel-based-morphometry-vbm","title":"C. Voxel-Based Morphometry (VBM)","text":"<p>Format: NIfTI (modulated GM/WM maps) Use case: Voxel-wise comparison of tissue density</p> <pre><code># VBM: Modulated, smoothed gray matter\nvbm_gm = nib.load('subject_001_VBM_GM.nii.gz').get_fdata()\n# Typically smoothed with 8mm FWHM Gaussian kernel\n</code></pre>"},{"location":"integration/modality_features/smri/#metadata-requirements","title":"Metadata Requirements","text":"<pre><code>dataset_id: ukb_smri\nname: UK Biobank Structural MRI\nmodality: sMRI\nsequence: T1-weighted\nn_subjects: 40000\nresolution: [1.0, 1.0, 1.0]  # mm (x, y, z)\nfield_strength: 3T\nmanufacturer: Siemens\npreprocessing: UK Biobank Pipeline\nstandard_space: MNI152NLin2009cAsym\natlas: Desikan-Killiany  # or Destrieux, AAL, etc.\n</code></pre>"},{"location":"integration/modality_features/smri/#quality-control-metrics","title":"Quality Control Metrics","text":""},{"location":"integration/modality_features/smri/#1-image-quality-assessment","title":"1. Image Quality Assessment","text":"<pre><code>def compute_snr(img_data, brain_mask):\n    \"\"\"\n    Signal-to-noise ratio for structural MRI.\n\n    Args:\n        img_data: (x, y, z) - T1w image\n        brain_mask: (x, y, z) - binary brain mask\n\n    Returns:\n        snr: Signal-to-noise ratio\n    \"\"\"\n    brain_signal = img_data[brain_mask &gt; 0]\n\n    # Estimate noise from background (air)\n    background = img_data[brain_mask == 0]\n    noise_std = background.std()\n\n    snr = brain_signal.mean() / (noise_std + 1e-8)\n\n    return snr\n\n# Typical SNR for 3T: 20-40\nsnr = compute_snr(data, brain_mask)\nprint(f\"SNR: {snr:.2f}\")\n</code></pre>"},{"location":"integration/modality_features/smri/#2-contrast-to-noise-ratio-cnr","title":"2. Contrast-to-Noise Ratio (CNR)","text":"<pre><code>def compute_cnr(img_data, gm_mask, wm_mask):\n    \"\"\"\n    Contrast-to-noise ratio between gray and white matter.\n    \"\"\"\n    gm_signal = img_data[gm_mask &gt; 0].mean()\n    wm_signal = img_data[wm_mask &gt; 0].mean()\n\n    noise_std = img_data[gm_mask &gt; 0].std()\n\n    cnr = abs(gm_signal - wm_signal) / (noise_std + 1e-8)\n\n    return cnr\n\ncnr = compute_cnr(data, gm_mask, wm_mask)\nprint(f\"CNR (GM-WM): {cnr:.2f}\")\n</code></pre>"},{"location":"integration/modality_features/smri/#3-freesurfer-qc-metrics","title":"3. FreeSurfer QC Metrics","text":"<pre><code># Check FreeSurfer reconstruction quality\ndef check_freesurfer_qc(subjects_dir, subject_id):\n    \"\"\"\n    Check FreeSurfer quality control metrics.\n    \"\"\"\n    # Euler number (lower is better, typically &lt; -50)\n    euler_path = f\"{subjects_dir}/{subject_id}/stats/lh.aparc.stats\"\n\n    # Mean cortical thickness (typical: 2.0-3.0 mm)\n    # Total brain volume (typical: 1000-1500 cm\u00b3)\n\n    # Flag for manual inspection if outliers\n    pass\n</code></pre>"},{"location":"integration/modality_features/smri/#data-normalization","title":"Data Normalization","text":""},{"location":"integration/modality_features/smri/#1-intensity-normalization","title":"1. Intensity Normalization","text":"<pre><code>from sklearn.preprocessing import StandardScaler\n\ndef normalize_intensity(img_data, brain_mask):\n    \"\"\"\n    Z-score normalization within brain mask.\n    \"\"\"\n    brain_voxels = img_data[brain_mask &gt; 0]\n\n    mean = brain_voxels.mean()\n    std = brain_voxels.std()\n\n    img_normalized = (img_data - mean) / (std + 1e-8)\n    img_normalized[brain_mask == 0] = 0  # Keep background at 0\n\n    return img_normalized\n\nnormalized = normalize_intensity(data, brain_mask)\n</code></pre>"},{"location":"integration/modality_features/smri/#2-total-intracranial-volume-tiv-correction","title":"2. Total Intracranial Volume (TIV) Correction","text":"<pre><code>def tiv_correction(regional_volumes, tiv):\n    \"\"\"\n    Correct regional volumes for head size.\n\n    Args:\n        regional_volumes: (n_regions,) - volumes in mm\u00b3\n        tiv: Total intracranial volume in mm\u00b3\n\n    Returns:\n        corrected_volumes: TIV-corrected volumes\n    \"\"\"\n    # Method 1: Proportional scaling\n    corrected = (regional_volumes / tiv) * 1500000  # Scale to 1.5L\n\n    # Method 2: Residuals after regression (preferred)\n    from sklearn.linear_model import LinearRegression\n    lr = LinearRegression()\n    lr.fit(tiv.reshape(-1, 1), regional_volumes.reshape(-1, 1))\n    residuals = regional_volumes - lr.predict(tiv.reshape(-1, 1)).ravel()\n\n    return residuals\n\n# Load TIV from FreeSurfer\n# Typically in aseg.stats: \"Estimated Total Intracranial Volume\"\ntiv = 1450000  # mm\u00b3\n\nvolumes_corrected = tiv_correction(regional_volumes, tiv)\n</code></pre>"},{"location":"integration/modality_features/smri/#data-augmentation","title":"Data Augmentation","text":""},{"location":"integration/modality_features/smri/#for-deep-learning-models","title":"For Deep Learning Models","text":"<pre><code>import torchio as tio\n\n# Define augmentation pipeline\ntransforms = tio.Compose([\n    tio.RandomAffine(\n        scales=(0.9, 1.1),\n        degrees=10,\n        translation=5,\n        p=0.75\n    ),\n    tio.RandomFlip(axes=('LR',), p=0.5),  # Left-Right flip\n    tio.RandomNoise(std=(0, 0.1), p=0.25),\n    tio.RandomBiasField(coefficients=0.5, p=0.25),\n])\n\n# Apply to image\nsubject = tio.Subject(\n    t1=tio.ScalarImage('subject_001_T1w.nii.gz'),\n)\naugmented = transforms(subject)\naugmented_data = augmented.t1.data  # Shape: (1, x, y, z)\n</code></pre>"},{"location":"integration/modality_features/smri/#benchmark-tasks","title":"Benchmark Tasks","text":""},{"location":"integration/modality_features/smri/#1-classification","title":"1. Classification","text":"<p>Typical tasks: - Sex classification - Age group classification - Site/scanner classification (for robustness testing)</p> <p>Input: T1w images <code>(x, y, z)</code> or derived features Output: Class labels</p> <pre><code># Example: Binary classification\nX_train = load_t1w_images(train_ids)  # (n_train, x, y, z)\ny_train = load_labels(train_ids)  # (n_train,) - {0, 1}\n</code></pre>"},{"location":"integration/modality_features/smri/#2-regression","title":"2. Regression","text":"<p>Typical tasks: - Age prediction (brain age) - Phenotype prediction - Continuous score prediction</p> <p>Input: T1w images Output: Continuous values</p> <pre><code># Example: Brain age prediction\nX = load_t1w_images(subject_ids)  # (n, x, y, z)\ny_age = load_ages(subject_ids)  # (n,) - chronological age in years\n</code></pre>"},{"location":"integration/modality_features/smri/#3-segmentation","title":"3. Segmentation","text":"<p>Typical tasks: - Tissue segmentation (GM, WM, CSF) - Lesion segmentation (MS, stroke, tumor) - Hippocampal subfield segmentation</p> <p>Input: T1w images Output: Segmentation masks <code>(x, y, z)</code> or <code>(x, y, z, n_classes)</code></p>"},{"location":"integration/modality_features/smri/#4-reconstructiondenoising","title":"4. Reconstruction/Denoising","text":"<p>Input: Low-quality or partial images Output: High-quality reconstructed images</p>"},{"location":"integration/modality_features/smri/#example-data-loading","title":"Example Data Loading","text":""},{"location":"integration/modality_features/smri/#using-nilearn","title":"Using Nilearn","text":"<pre><code>from nilearn import datasets, plotting\nfrom nilearn.image import resample_to_img\n\n# Load example T1w image\nt1_img = nib.load('subject_001_T1w.nii.gz')\n\n# Resample to standard resolution (e.g., 2mm isotropic)\nfrom nilearn.image import resample_img\nt1_resampled = resample_img(\n    t1_img,\n    target_affine=np.diag([2, 2, 2]),\n    interpolation='continuous'\n)\n\n# Visualize\nplotting.plot_anat(t1_resampled)\n</code></pre>"},{"location":"integration/modality_features/smri/#loading-freesurfer-derived-features","title":"Loading FreeSurfer-Derived Features","text":"<pre><code>def load_freesurfer_features(subjects_dir, subject_id):\n    \"\"\"\n    Load FreeSurfer morphometric features.\n\n    Returns:\n        features: Dictionary with volumes, thickness, etc.\n    \"\"\"\n    import re\n\n    # Parse aparc.stats\n    stats_file = f\"{subjects_dir}/{subject_id}/stats/lh.aparc.stats\"\n\n    features = {}\n    with open(stats_file, 'r') as f:\n        for line in f:\n            if line.startswith('#'):\n                continue\n            parts = line.split()\n            if len(parts) &gt;= 5:\n                region = parts[0]\n                features[f\"lh_{region}_volume\"] = float(parts[3])\n                features[f\"lh_{region}_thickness\"] = float(parts[4])\n\n    # Repeat for right hemisphere\n    # ... (similar code for rh.aparc.stats)\n\n    return features\n</code></pre>"},{"location":"integration/modality_features/smri/#itu-ai4h-alignment","title":"ITU AI4H Alignment","text":"<p>This specification aligns with:</p> <ul> <li>DEL10.8 Section 3.1: Input data specifications for neurology</li> <li>DEL3 Section 4.2: Data format and quality requirements</li> <li>DEL0.1: Standardized neuroimaging terminology</li> </ul>"},{"location":"integration/modality_features/smri/#tools-libraries","title":"Tools &amp; Libraries","text":""},{"location":"integration/modality_features/smri/#preprocessing","title":"Preprocessing","text":"<ul> <li>FreeSurfer: Comprehensive cortical reconstruction</li> <li>FSL: FMRIB Software Library (BET, FAST, FLIRT/FNIRT)</li> <li>SPM: Statistical Parametric Mapping</li> <li>ANTs: Advanced Normalization Tools</li> <li>CAT12: Computational Anatomy Toolbox</li> </ul>"},{"location":"integration/modality_features/smri/#python-libraries","title":"Python Libraries","text":"<ul> <li>NiBabel: Read/write neuroimaging formats</li> <li>Nilearn: Machine learning for neuroimaging</li> <li>TorchIO: Medical image augmentation</li> <li>MONAI: Medical imaging deep learning</li> </ul>"},{"location":"integration/modality_features/smri/#references","title":"References","text":"<ol> <li>Fischl, B. (2012). FreeSurfer. NeuroImage, 62(2), 774-781.</li> <li>Ashburner, J., &amp; Friston, K. J. (2000). Voxel-based morphometry. NeuroImage, 11(6), 805-821.</li> <li>Klein, A., et al. (2009). Evaluation of volume-based and surface-based brain image registration methods. NeuroImage, 51(1), 214-220.</li> </ol>"},{"location":"integration/modality_features/smri/#related-documentation","title":"Related Documentation","text":"<ul> <li>fMRI Specifications</li> <li>Genomics Specifications</li> <li>Prediction Baselines</li> </ul>"},{"location":"leaderboards/","title":"\ud83c\udfc6 Foundation Model Leaderboards","text":"<p>Benchmark Hub Overview</p> <p>\ud83d\udcca 8 Benchmarks | \ud83e\udd16 24 Models | \ud83d\udcc8 59 Evaluations</p> <p>What is this? This page ranks AI models for healthcare applications.  Higher-ranked models perform better on standardized tests.</p> <p>How to read it: Each table shows models from best (\ud83e\udd47) to developing (\ud83d\udcc8). Click \"How are scores calculated?\" for details on what the numbers mean.</p>"},{"location":"leaderboards/#example-what-a-real-submission-looks-like","title":"Example: what a real submission looks like","text":"<p>This is a real, end-to-end run using the built-in baseline model. Your submission should look like this: a local run that produces <code>report.md</code> + <code>eval.yaml</code>.</p> Model ID Suite / Benchmark Task AUROC dropout rAUC noise rAUC <code>dummy_classifier</code> <code>SUITE-TOY-CLASS</code> / <code>BM-TOY-CLASS</code> Toy fMRI-like classification 0.5597 0.7760 0.7867 <p>Artifacts: Example classification eval.yaml \u00b7 Example classification report.md \u00b7 Example robustness eval.yaml \u00b7 Example robustness report.md</p>"},{"location":"leaderboards/#jump-to","title":"\ud83e\udded Jump To","text":"<ul> <li>\ud83c\udf10 Overall Rankings \u2014 Best across all categories</li> <li>\ud83e\uddec Genomics</li> <li>\ud83e\udde0 Brain Imaging (MRI/fMRI)</li> </ul>"},{"location":"leaderboards/#overall-rankings-all-modalities","title":"\ud83c\udf10 Overall Rankings (All Modalities)","text":"<p>Best score per model across all benchmarks</p> Rank Model Best Score Metric Benchmark Modality \ud83e\udd47 SWIFT \ud83d\udc51 0.9999 <code>dropout_rAUC</code> - \ud83d\udcca Other \ud83e\udd48 BrainMT 0.9999 <code>dropout_rAUC</code> - \ud83d\udcca Other \ud83e\udd49 neuroclips 0.9999 <code>dropout_rAUC</code> - \ud83d\udcca Other \ud83c\udfc5 geneformer 0.9995 <code>robustness_score</code> Foundation Model Robustness Evaluation \ud83d\udcca Other \ud83c\udfc5 Evo 2 0.9841 <code>dropout_rAUC</code> - \ud83d\udcca Other \ud83c\udf96\ufe0f Caduceus 0.9841 <code>dropout_rAUC</code> - \ud83d\udcca Other \ud83c\udf96\ufe0f Brain-JEPA 0.9250 <code>AUROC</code> fMRI Foundation Model Benchmark (Granular) \ud83e\udde0 Brain Imaging (MRI/fMRI) \ud83c\udf96\ufe0f BrainLM 0.9100 <code>AUROC</code> fMRI Foundation Model Benchmark (Granular) \ud83e\udde0 Brain Imaging (MRI/fMRI) \ud83c\udf96\ufe0f Geneformer 0.9100 <code>Accuracy</code> Cell Type Annotation \ud83e\uddec Genomics \ud83c\udf96\ufe0f Me-LLaMA 0.8750 <code>report_quality_score</code> Clinical Report Generation Quality \ud83e\uddec Genomics #11 HyenaDNA 0.8720 <code>AUROC</code> DNA Promoter Classification \ud83e\uddec Genomics #12 HyenaDNA 0.8700 <code>Accuracy</code> Cell Type Annotation \ud83e\uddec Genomics #13 BrainBERT 0.8700 <code>AUROC</code> fMRI Foundation Model Benchmark (Granular) \ud83e\udde0 Brain Imaging (MRI/fMRI) #14 M3FM 0.8600 <code>report_quality_score</code> Clinical Report Generation Quality \ud83e\uddec Genomics #15 DNABERT-2 0.8500 <code>Accuracy</code> Cell Type Annotation \ud83e\uddec Genomics #16 BrainHarmony 0.8450 <code>robustness_score</code> Foundation Model Robustness Evaluation \ud83d\udcca Other #17 kmer_baseline 0.8426 <code>AUROC</code> - \ud83d\udcca Other #18 OpenFlamingo 0.8400 <code>report_quality_score</code> Clinical Report Generation Quality \ud83e\uddec Genomics #19 kmer_k6 0.8357 <code>AUROC</code> DNA Promoter Classification \ud83e\uddec Genomics #20 NeuroClips 0.8300 <code>AUROC</code> fMRI Foundation Model Benchmark (Granular) \ud83e\udde0 Brain Imaging (MRI/fMRI) #21 TITAN 0.8100 <code>report_quality_score</code> Clinical Report Generation Quality \ud83e\uddec Genomics #22 Baseline (Random/Majority) 0.7810 <code>robustness_score</code> Foundation Model Robustness Evaluation \ud83d\udcca Other #23 Med-Flamingo 0.7800 <code>report_quality_score</code> Clinical Report Generation Quality \ud83e\uddec Genomics #24 RadBERT 0.6900 <code>report_quality_score</code> Clinical Report Generation Quality \ud83e\uddec Genomics <p>Performance Distribution</p> <p>\u2b50 9 Excellent | \u2705 12 Good | \ud83d\udd36 2 Fair | \ud83d\udcc8 1 Developing</p>"},{"location":"leaderboards/#genomics","title":"\ud83e\uddec Genomics","text":""},{"location":"leaderboards/#classification","title":"\ud83c\udfaf Classification","text":""},{"location":"leaderboards/#dna-promoter-classification","title":"DNA Promoter Classification","text":"<p>*Benchmark for classifying DNA sequences as promoters or non-promoters. Promoters are regulatory regions at transcription start sites (TSS). This benchmark focuses on non-TATA promoters, which lack the canonical TATA box and represent ~75% of human promoters. *</p> <pre><code>                    \ud83c\udfc6                    \n\n              \ud83e\udd47  HyenaDNA              \n                 (0.872)                 \n             \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557             \n             \u2551               \u2551             \n   \ud83e\udd48  kmer_k6    \u2551               \u2551   \ud83e\udd49 DNABERT-2   \n      (0.836)      \u2551               \u2551      (0.836)      \n  \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d               \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557  \n  \u2551                                       \u2551  \n\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\n</code></pre> <p>4 models ranked by <code>AUROC</code>:</p> Rank Model Score Level Details \ud83e\udd47 HyenaDNA \ud83d\udc51 0.8720 \u2705 Good DS-DNA-PROMOTER, 2025-12-18T21:03:12.030852 \ud83e\udd48 kmer_k6 0.8357 \u2705 Good Human Non-TATA Promo, 2025-12-18T18:44:10.847321 \ud83e\udd49 DNABERT-2 0.8357 \u2705 Good Human Non-TATA Promo, 2025-12-18T18:44:27.391206 \ud83c\udfc5 HyenaDNA 0.8357 \u2705 Good Human Non-TATA Promo, 2025-12-18T18:44:19.651418 <p>Quick Comparison</p> <p>\ud83e\udd47 HyenaDNA leads with AUROC = 0.8720</p> <ul> <li>Gap to \ud83e\udd48 kmer_k6: +0.0363</li> <li>Score spread (best to worst): 0.0363</li> </ul> \ud83d\udcd0 How are scores calculated? (click to expand)  ---  ### \ud83d\udcd6 Understanding This Leaderboard  This section explains how we measure and compare AI models. Don't worry if you're new to AI metrics \u2014 we'll break it down step by step.  ---  ### \ud83c\udfaf The Main Metric: `AUROC`  **Area Under ROC Curve (AUROC)**  **In simple terms:** Measures how well the model can tell apart different categories (e.g., healthy vs. diseased)  **How it works:** Think of it like this: if you randomly pick one positive case and one negative case, AUROC tells you the probability that the model correctly identifies which is which. A score of 0.5 means the model is just guessing randomly (like flipping a coin), while 1.0 means it perfectly separates all cases.  **Score range:** 0.5 (random guessing) \u2192 1.0 (perfect separation)  \ud83d\udca1 **Example:** An AUROC of 0.85 means the model correctly ranks a positive case higher than a negative case 85% of the time.  ---  ### \ud83e\udde0 How This Metric Fits This Task  Different tasks emphasize different aspects of performance. Here's how this metric should be interpreted for this benchmark:  - For **classification** tasks (e.g., disease vs. no disease), this metric helps you understand   how reliably the model separates different outcome groups. In addition to raw accuracy,   we recommend also looking at metrics like AUROC and F1 Score, especially when classes are   imbalanced (for example, when positive cases are rare).  ---  ### \ud83d\udcca Performance Tiers: What Do the Scores Mean?  We group models into performance tiers to help you quickly understand how ready they are for different uses:  | Score Range | Rating | Interpretation | Suitable For | |:---:|:---:|:---|:---| | **\u2265 0.90** | \u2b50 Excellent | Top-tier performance, consistently reliable | Clinical pilots with physician oversight | | **0.80 \u2013 0.89** | \u2705 Good | Strong performance, shows real promise | Validation studies, controlled testing | | **0.70 \u2013 0.79** | \ud83d\udd36 Fair | Moderate performance, has limitations | Research and development only | | **&lt; 0.70** | \ud83d\udcc8 Developing | Below typical benchmarks, needs improvement | Early research, not for clinical use |  !!! tip \"Important Context\"     These thresholds are general guidelines. The acceptable score depends on the specific clinical application, risk level, and whether the AI assists or replaces human judgment. Always consult domain experts when evaluating fitness for a particular use case.  ---  ### \ud83d\udccf How We Determine Rankings  Models are ranked following these principles:  1. **Primary metric determines rank** \u2014 The model with the highest score in the main metric ranks first. For metrics where lower is better (like error rates), the lowest score wins.  2. **Ties are broken by secondary metrics** \u2014 If two models have identical primary scores, we look at other relevant metrics to determine which performs better overall.  3. **Best run per model** \u2014 If a model was evaluated multiple times (e.g., with different settings), only its best result appears on the leaderboard. This ensures fair comparison.  4. **Reproducibility required** \u2014 All results must be reproducible. We record the evaluation date, dataset used, and configuration to ensure transparency.  ---  ### \ud83c\udfe5 Why This Matters for Healthcare AI  Healthcare AI has higher stakes than many other AI applications. A model that works 95% of the time might sound good, but that 5% could mean missed diagnoses or incorrect treatments. That's why we:  - Use **multiple metrics** to capture different aspects of performance - Test **robustness** to real-world data quality issues - Require **transparency** about evaluation conditions - Follow **international standards** for healthcare AI assessment  ---  ### \ud83c\udf0d Standards Alignment  This benchmark follows the [ITU/WHO Focus Group on AI for Health (FG-AI4H)](https://www.itu.int/pub/T-FG-AI4H) framework, which provides internationally recognized guidelines for evaluating healthcare AI systems. This ensures our evaluations are:  - **Rigorous** \u2014 Following established scientific methodology - **Comparable** \u2014 Using standardized metrics across different models - **Trustworthy** \u2014 Aligned with WHO/ITU recommendations for health AI"},{"location":"leaderboards/#cell-type-annotation","title":"Cell Type Annotation","text":"<p>Predicting cell types from single-cell RNA-seq data.</p> <pre><code>                    \ud83c\udfc6                    \n\n              \ud83e\udd47   Evo 2                 \n                 (0.925)                 \n             \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557             \n             \u2551               \u2551             \n   \ud83e\udd48 Geneformer   \u2551               \u2551   \ud83e\udd49   SWIFT      \n      (0.910)      \u2551               \u2551      (0.895)      \n  \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d               \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557  \n  \u2551                                       \u2551  \n\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\n</code></pre> <p>8 models ranked by <code>Accuracy</code>:</p> Rank Model Score Level Details \ud83e\udd47 Evo 2 \ud83d\udc51 0.9250 \u2b50 Excellent PBMC 68k, 2024-02-01 \ud83e\udd48 Geneformer 0.9100 \u2b50 Excellent PBMC 3k (processed, , 2023-11-01 \ud83e\udd49 SWIFT 0.8950 \u2705 Good PBMC 68k, 2024-01-15 \ud83c\udfc5 Caduceus 0.8850 \u2705 Good PBMC 68k, 2024-01-12 \ud83c\udfc5 HyenaDNA 0.8700 \u2705 Good PBMC 68k, 2024-01-08 \ud83c\udf96\ufe0f DNABERT-2 0.8500 \u2705 Good PBMC 68k, 2024-01-05 \ud83c\udf96\ufe0f Baseline (Random/Majority) 0.0000 \ud83d\udcc8 Developing PBMC 3k (processed, , 2025-12-18 \ud83c\udf96\ufe0f geneformer 0.0000 \ud83d\udcc8 Developing PBMC 3k (processed, , 2025-12-18 <p>Quick Comparison</p> <p>\ud83e\udd47 Evo 2 leads with Accuracy = 0.9250</p> <ul> <li>Gap to \ud83e\udd48 Geneformer: +0.0150</li> <li>Score spread (best to worst): 0.9250</li> </ul> \ud83d\udcd0 How are scores calculated? (click to expand)  ---  ### \ud83d\udcd6 Understanding This Leaderboard  This section explains how we measure and compare AI models. Don't worry if you're new to AI metrics \u2014 we'll break it down step by step.  ---  ### \ud83c\udfaf The Main Metric: `Accuracy`  **Accuracy**  **In simple terms:** The percentage of predictions the model got right  **How it works:** This is the most intuitive metric: out of all the predictions the model made, how many were correct? For example, if a model makes 100 predictions and 90 are correct, the accuracy is 90% (or 0.90). While easy to understand, accuracy can be misleading when classes are imbalanced (e.g., if 95% of cases are healthy, a model that always predicts 'healthy' would have 95% accuracy but miss all diseases).  **Score range:** 0.0 (all wrong) \u2192 1.0 (all correct)  \ud83d\udca1 **Example:** An accuracy of 0.92 means the model correctly classified 92 out of every 100 samples.  ---  ### \ud83e\udde0 How This Metric Fits This Task  Different tasks emphasize different aspects of performance. Here's how this metric should be interpreted for this benchmark:  - For **classification** tasks (e.g., disease vs. no disease), this metric helps you understand   how reliably the model separates different outcome groups. In addition to raw accuracy,   we recommend also looking at metrics like AUROC and F1 Score, especially when classes are   imbalanced (for example, when positive cases are rare).  ---  ### \ud83d\udcca Performance Tiers: What Do the Scores Mean?  We group models into performance tiers to help you quickly understand how ready they are for different uses:  | Score Range | Rating | Interpretation | Suitable For | |:---:|:---:|:---|:---| | **\u2265 0.90** | \u2b50 Excellent | Top-tier performance, consistently reliable | Clinical pilots with physician oversight | | **0.80 \u2013 0.89** | \u2705 Good | Strong performance, shows real promise | Validation studies, controlled testing | | **0.70 \u2013 0.79** | \ud83d\udd36 Fair | Moderate performance, has limitations | Research and development only | | **&lt; 0.70** | \ud83d\udcc8 Developing | Below typical benchmarks, needs improvement | Early research, not for clinical use |  !!! tip \"Important Context\"     These thresholds are general guidelines. The acceptable score depends on the specific clinical application, risk level, and whether the AI assists or replaces human judgment. Always consult domain experts when evaluating fitness for a particular use case.  ---  ### \ud83d\udccf How We Determine Rankings  Models are ranked following these principles:  1. **Primary metric determines rank** \u2014 The model with the highest score in the main metric ranks first. For metrics where lower is better (like error rates), the lowest score wins.  2. **Ties are broken by secondary metrics** \u2014 If two models have identical primary scores, we look at other relevant metrics to determine which performs better overall.  3. **Best run per model** \u2014 If a model was evaluated multiple times (e.g., with different settings), only its best result appears on the leaderboard. This ensures fair comparison.  4. **Reproducibility required** \u2014 All results must be reproducible. We record the evaluation date, dataset used, and configuration to ensure transparency.  ---  ### \ud83c\udfe5 Why This Matters for Healthcare AI  Healthcare AI has higher stakes than many other AI applications. A model that works 95% of the time might sound good, but that 5% could mean missed diagnoses or incorrect treatments. That's why we:  - Use **multiple metrics** to capture different aspects of performance - Test **robustness** to real-world data quality issues - Require **transparency** about evaluation conditions - Follow **international standards** for healthcare AI assessment  ---  ### \ud83c\udf0d Standards Alignment  This benchmark follows the [ITU/WHO Focus Group on AI for Health (FG-AI4H)](https://www.itu.int/pub/T-FG-AI4H) framework, which provides internationally recognized guidelines for evaluating healthcare AI systems. This ensures our evaluations are:  - **Rigorous** \u2014 Following established scientific methodology - **Comparable** \u2014 Using standardized metrics across different models - **Trustworthy** \u2014 Aligned with WHO/ITU recommendations for health AI"},{"location":"leaderboards/#dna-enhancer-classification","title":"DNA Enhancer Classification","text":"<p>*Benchmark for classifying DNA sequences as enhancers or non-enhancers. Enhancers are distal regulatory elements that activate gene expression. Accurate enhancer prediction is critical for understanding gene regulation and identifying disease-associated variants. *</p> <pre><code>                    \ud83c\udfc6                    \n\n              \ud83e\udd47  HyenaDNA              \n                 (0.788)                 \n             \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557             \n             \u2551               \u2551             \n   \ud83e\udd48  kmer_k6    \u2551               \u2551   \ud83e\udd49 DNABERT-2   \n      (0.737)      \u2551               \u2551      (0.737)      \n  \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d               \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557  \n  \u2551                                       \u2551  \n\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\n</code></pre> <p>4 models ranked by <code>AUROC</code>:</p> Rank Model Score Level Details \ud83e\udd47 HyenaDNA \ud83d\udc51 0.7883 \ud83d\udd36 Fair DS-DNA-ENHANCER, 2025-12-18T21:03:03.285801 \ud83e\udd48 kmer_k6 0.7365 \ud83d\udd36 Fair Human Enhancers (Coh, 2025-12-18T18:44:08.075706 \ud83e\udd49 DNABERT-2 0.7365 \ud83d\udd36 Fair Human Enhancers (Coh, 2025-12-18T18:44:24.678525 \ud83c\udfc5 HyenaDNA 0.7365 \ud83d\udd36 Fair Human Enhancers (Coh, 2025-12-18T18:44:17.006557 <p>Quick Comparison</p> <p>\ud83e\udd47 HyenaDNA leads with AUROC = 0.7883</p> <ul> <li>Gap to \ud83e\udd48 kmer_k6: +0.0518</li> <li>Score spread (best to worst): 0.0518</li> </ul> \ud83d\udcd0 How are scores calculated? (click to expand)  ---  ### \ud83d\udcd6 Understanding This Leaderboard  This section explains how we measure and compare AI models. Don't worry if you're new to AI metrics \u2014 we'll break it down step by step.  ---  ### \ud83c\udfaf The Main Metric: `AUROC`  **Area Under ROC Curve (AUROC)**  **In simple terms:** Measures how well the model can tell apart different categories (e.g., healthy vs. diseased)  **How it works:** Think of it like this: if you randomly pick one positive case and one negative case, AUROC tells you the probability that the model correctly identifies which is which. A score of 0.5 means the model is just guessing randomly (like flipping a coin), while 1.0 means it perfectly separates all cases.  **Score range:** 0.5 (random guessing) \u2192 1.0 (perfect separation)  \ud83d\udca1 **Example:** An AUROC of 0.85 means the model correctly ranks a positive case higher than a negative case 85% of the time.  ---  ### \ud83e\udde0 How This Metric Fits This Task  Different tasks emphasize different aspects of performance. Here's how this metric should be interpreted for this benchmark:  - For **classification** tasks (e.g., disease vs. no disease), this metric helps you understand   how reliably the model separates different outcome groups. In addition to raw accuracy,   we recommend also looking at metrics like AUROC and F1 Score, especially when classes are   imbalanced (for example, when positive cases are rare).  ---  ### \ud83d\udcca Performance Tiers: What Do the Scores Mean?  We group models into performance tiers to help you quickly understand how ready they are for different uses:  | Score Range | Rating | Interpretation | Suitable For | |:---:|:---:|:---|:---| | **\u2265 0.90** | \u2b50 Excellent | Top-tier performance, consistently reliable | Clinical pilots with physician oversight | | **0.80 \u2013 0.89** | \u2705 Good | Strong performance, shows real promise | Validation studies, controlled testing | | **0.70 \u2013 0.79** | \ud83d\udd36 Fair | Moderate performance, has limitations | Research and development only | | **&lt; 0.70** | \ud83d\udcc8 Developing | Below typical benchmarks, needs improvement | Early research, not for clinical use |  !!! tip \"Important Context\"     These thresholds are general guidelines. The acceptable score depends on the specific clinical application, risk level, and whether the AI assists or replaces human judgment. Always consult domain experts when evaluating fitness for a particular use case.  ---  ### \ud83d\udccf How We Determine Rankings  Models are ranked following these principles:  1. **Primary metric determines rank** \u2014 The model with the highest score in the main metric ranks first. For metrics where lower is better (like error rates), the lowest score wins.  2. **Ties are broken by secondary metrics** \u2014 If two models have identical primary scores, we look at other relevant metrics to determine which performs better overall.  3. **Best run per model** \u2014 If a model was evaluated multiple times (e.g., with different settings), only its best result appears on the leaderboard. This ensures fair comparison.  4. **Reproducibility required** \u2014 All results must be reproducible. We record the evaluation date, dataset used, and configuration to ensure transparency.  ---  ### \ud83c\udfe5 Why This Matters for Healthcare AI  Healthcare AI has higher stakes than many other AI applications. A model that works 95% of the time might sound good, but that 5% could mean missed diagnoses or incorrect treatments. That's why we:  - Use **multiple metrics** to capture different aspects of performance - Test **robustness** to real-world data quality issues - Require **transparency** about evaluation conditions - Follow **international standards** for healthcare AI assessment  ---  ### \ud83c\udf0d Standards Alignment  This benchmark follows the [ITU/WHO Focus Group on AI for Health (FG-AI4H)](https://www.itu.int/pub/T-FG-AI4H) framework, which provides internationally recognized guidelines for evaluating healthcare AI systems. This ensures our evaluations are:  - **Rigorous** \u2014 Following established scientific methodology - **Comparable** \u2014 Using standardized metrics across different models - **Trustworthy** \u2014 Aligned with WHO/ITU recommendations for health AI"},{"location":"leaderboards/#generation","title":"\u270d\ufe0f Generation","text":""},{"location":"leaderboards/#clinical-report-generation-quality","title":"Clinical Report Generation Quality","text":"<pre><code>                    \ud83c\udfc6                    \n\n              \ud83e\udd47   Me-LLaMA                \n                 (0.875)                 \n             \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557             \n             \u2551               \u2551             \n   \ud83e\udd48     M3FM       \u2551               \u2551   \ud83e\udd49 OpenFlamingo   \n      (0.860)      \u2551               \u2551      (0.840)      \n  \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d               \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557  \n  \u2551                                       \u2551  \n\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\n</code></pre> <p>6 models ranked by <code>report_quality_score</code>:</p> Rank Model Score Level Details \ud83e\udd47 Me-LLaMA \ud83d\udc51 0.8750 \u2705 Good mimic_cxr_reports, 2024-02-05 \ud83e\udd48 M3FM 0.8600 \u2705 Good mimic_cxr_reports, 2024-01-28 \ud83e\udd49 OpenFlamingo 0.8400 \u2705 Good mimic_cxr_reports, 2024-01-20 \ud83c\udfc5 TITAN 0.8100 \u2705 Good mimic_cxr_reports, 2024-01-25 \ud83c\udfc5 Med-Flamingo 0.7800 \ud83d\udd36 Fair mimic_cxr_reports, 2024-01-18 \ud83c\udf96\ufe0f RadBERT 0.6900 \ud83d\udcc8 Developing mimic_cxr_reports, 2024-01-12 <p>Quick Comparison</p> <p>\ud83e\udd47 Me-LLaMA leads with report_quality_score = 0.8750</p> <ul> <li>Gap to \ud83e\udd48 M3FM: +0.0150</li> <li>Score spread (best to worst): 0.1850</li> </ul> \ud83d\udcd0 How are scores calculated? (click to expand)  ---  ### \ud83d\udcd6 Understanding This Leaderboard  This section explains how we measure and compare AI models. Don't worry if you're new to AI metrics \u2014 we'll break it down step by step.  ---  ### \ud83c\udfaf The Main Metric: `report_quality_score`  **Report Quality Score**  **In simple terms:** An overall measure of how good the AI-generated medical reports are  **How it works:** This composite score combines multiple aspects of report quality: clinical accuracy (are the findings correct?), completeness (are important findings mentioned?), language quality (is it well-written?), and safety (no harmful content). It provides a single number to compare models, though looking at individual components gives more insight into specific strengths and weaknesses.  **Score range:** 0.0 (poor quality) \u2192 1.0 (excellent quality)  \ud83d\udca1 **Example:** A score of 0.85 indicates the model generates reports that are mostly accurate, complete, and well-structured.  ---  ### \ud83e\udde0 How This Metric Fits This Task  Different tasks emphasize different aspects of performance. Here's how this metric should be interpreted for this benchmark:  - For **report generation**, we care not only about language quality but also clinical safety.   This metric is usually combined with others (e.g., clinical accuracy, hallucination rate,   and completeness of findings) to judge whether the generated report is both readable **and**   medically reliable.  ---  ### \ud83d\udcca Performance Tiers: What Do the Scores Mean?  We group models into performance tiers to help you quickly understand how ready they are for different uses:  | Score Range | Rating | Interpretation | Suitable For | |:---:|:---:|:---|:---| | **\u2265 0.90** | \u2b50 Excellent | Top-tier performance, consistently reliable | Clinical pilots with physician oversight | | **0.80 \u2013 0.89** | \u2705 Good | Strong performance, shows real promise | Validation studies, controlled testing | | **0.70 \u2013 0.79** | \ud83d\udd36 Fair | Moderate performance, has limitations | Research and development only | | **&lt; 0.70** | \ud83d\udcc8 Developing | Below typical benchmarks, needs improvement | Early research, not for clinical use |  !!! tip \"Important Context\"     These thresholds are general guidelines. The acceptable score depends on the specific clinical application, risk level, and whether the AI assists or replaces human judgment. Always consult domain experts when evaluating fitness for a particular use case.  ---  ### \ud83d\udccf How We Determine Rankings  Models are ranked following these principles:  1. **Primary metric determines rank** \u2014 The model with the highest score in the main metric ranks first. For metrics where lower is better (like error rates), the lowest score wins.  2. **Ties are broken by secondary metrics** \u2014 If two models have identical primary scores, we look at other relevant metrics to determine which performs better overall.  3. **Best run per model** \u2014 If a model was evaluated multiple times (e.g., with different settings), only its best result appears on the leaderboard. This ensures fair comparison.  4. **Reproducibility required** \u2014 All results must be reproducible. We record the evaluation date, dataset used, and configuration to ensure transparency.  ---  ### \ud83c\udfe5 Why This Matters for Healthcare AI  Healthcare AI has higher stakes than many other AI applications. A model that works 95% of the time might sound good, but that 5% could mean missed diagnoses or incorrect treatments. That's why we:  - Use **multiple metrics** to capture different aspects of performance - Test **robustness** to real-world data quality issues - Require **transparency** about evaluation conditions - Follow **international standards** for healthcare AI assessment  ---  ### \ud83c\udf0d Standards Alignment  This benchmark follows the [ITU/WHO Focus Group on AI for Health (FG-AI4H)](https://www.itu.int/pub/T-FG-AI4H) framework, which provides internationally recognized guidelines for evaluating healthcare AI systems. This ensures our evaluations are:  - **Rigorous** \u2014 Following established scientific methodology - **Comparable** \u2014 Using standardized metrics across different models - **Trustworthy** \u2014 Aligned with WHO/ITU recommendations for health AI"},{"location":"leaderboards/#brain-imaging-mrifmri","title":"\ud83e\udde0 Brain Imaging (MRI/fMRI)","text":""},{"location":"leaderboards/#classification_1","title":"\ud83c\udfaf Classification","text":""},{"location":"leaderboards/#toy-classification-benchmark","title":"Toy Classification Benchmark","text":"<p>A toy benchmark for testing the pipeline.</p> <p>2 models ranked by <code>AUROC</code>:</p> Rank Model Score Level Details \ud83e\udd47 Baseline (Random/Majority) \ud83d\udc51 0.5597 \ud83d\udcc8 Developing Toy fMRI Classificat, 2025-11-27 \ud83e\udd48 BrainLM 0.5193 \ud83d\udcc8 Developing Toy fMRI Classificat, 2025-11-27 <p>Quick Comparison</p> <p>\ud83e\udd47 Baseline (Random/Majority) leads with AUROC = 0.5597</p> <ul> <li>Gap to \ud83e\udd48 BrainLM: +0.0404</li> </ul> \ud83d\udcd0 How are scores calculated? (click to expand)  ---  ### \ud83d\udcd6 Understanding This Leaderboard  This section explains how we measure and compare AI models. Don't worry if you're new to AI metrics \u2014 we'll break it down step by step.  ---  ### \ud83c\udfaf The Main Metric: `AUROC`  **Area Under ROC Curve (AUROC)**  **In simple terms:** Measures how well the model can tell apart different categories (e.g., healthy vs. diseased)  **How it works:** Think of it like this: if you randomly pick one positive case and one negative case, AUROC tells you the probability that the model correctly identifies which is which. A score of 0.5 means the model is just guessing randomly (like flipping a coin), while 1.0 means it perfectly separates all cases.  **Score range:** 0.5 (random guessing) \u2192 1.0 (perfect separation)  \ud83d\udca1 **Example:** An AUROC of 0.85 means the model correctly ranks a positive case higher than a negative case 85% of the time.  ---  ### \ud83e\udde0 How This Metric Fits This Task  Different tasks emphasize different aspects of performance. Here's how this metric should be interpreted for this benchmark:  - For **classification** tasks (e.g., disease vs. no disease), this metric helps you understand   how reliably the model separates different outcome groups. In addition to raw accuracy,   we recommend also looking at metrics like AUROC and F1 Score, especially when classes are   imbalanced (for example, when positive cases are rare).  ---  ### \ud83d\udcca Performance Tiers: What Do the Scores Mean?  We group models into performance tiers to help you quickly understand how ready they are for different uses:  | Score Range | Rating | Interpretation | Suitable For | |:---:|:---:|:---|:---| | **\u2265 0.90** | \u2b50 Excellent | Top-tier performance, consistently reliable | Clinical pilots with physician oversight | | **0.80 \u2013 0.89** | \u2705 Good | Strong performance, shows real promise | Validation studies, controlled testing | | **0.70 \u2013 0.79** | \ud83d\udd36 Fair | Moderate performance, has limitations | Research and development only | | **&lt; 0.70** | \ud83d\udcc8 Developing | Below typical benchmarks, needs improvement | Early research, not for clinical use |  !!! tip \"Important Context\"     These thresholds are general guidelines. The acceptable score depends on the specific clinical application, risk level, and whether the AI assists or replaces human judgment. Always consult domain experts when evaluating fitness for a particular use case.  ---  ### \ud83d\udccf How We Determine Rankings  Models are ranked following these principles:  1. **Primary metric determines rank** \u2014 The model with the highest score in the main metric ranks first. For metrics where lower is better (like error rates), the lowest score wins.  2. **Ties are broken by secondary metrics** \u2014 If two models have identical primary scores, we look at other relevant metrics to determine which performs better overall.  3. **Best run per model** \u2014 If a model was evaluated multiple times (e.g., with different settings), only its best result appears on the leaderboard. This ensures fair comparison.  4. **Reproducibility required** \u2014 All results must be reproducible. We record the evaluation date, dataset used, and configuration to ensure transparency.  ---  ### \ud83c\udfe5 Why This Matters for Healthcare AI  Healthcare AI has higher stakes than many other AI applications. A model that works 95% of the time might sound good, but that 5% could mean missed diagnoses or incorrect treatments. That's why we:  - Use **multiple metrics** to capture different aspects of performance - Test **robustness** to real-world data quality issues - Require **transparency** about evaluation conditions - Follow **international standards** for healthcare AI assessment  ---  ### \ud83c\udf0d Standards Alignment  This benchmark follows the [ITU/WHO Focus Group on AI for Health (FG-AI4H)](https://www.itu.int/pub/T-FG-AI4H) framework, which provides internationally recognized guidelines for evaluating healthcare AI systems. This ensures our evaluations are:  - **Rigorous** \u2014 Following established scientific methodology - **Comparable** \u2014 Using standardized metrics across different models - **Trustworthy** \u2014 Aligned with WHO/ITU recommendations for health AI"},{"location":"leaderboards/#classificationreconstruction","title":"\ud83d\udccb Classification/Reconstruction","text":""},{"location":"leaderboards/#fmri-foundation-model-benchmark-granular","title":"fMRI Foundation Model Benchmark (Granular)","text":"<pre><code>                    \ud83c\udfc6                    \n\n              \ud83e\udd47 Brain-JEPA              \n                 (0.925)                 \n             \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557             \n             \u2551               \u2551             \n   \ud83e\udd48  BrainLM     \u2551               \u2551   \ud83e\udd49 BrainBERT    \n      (0.910)      \u2551               \u2551      (0.870)      \n  \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d               \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557  \n  \u2551                                       \u2551  \n\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\n</code></pre> <p>5 models ranked by <code>AUROC</code>:</p> Rank Model Score Level Details \ud83e\udd47 Brain-JEPA \ud83d\udc51 0.9250 \u2b50 Excellent hcp_1200, 2024-01-22 \ud83e\udd48 BrainLM 0.9100 \u2b50 Excellent hcp_1200, 2024-01-15 \ud83e\udd49 BrainBERT 0.8700 \u2705 Good hcp_1200, 2024-01-10 \ud83c\udfc5 BrainMT 0.8500 \u2705 Good hcp_1200, 2024-01-18 \ud83c\udfc5 NeuroClips 0.8300 \u2705 Good hcp_1200, 2024-01-05 <p>Quick Comparison</p> <p>\ud83e\udd47 Brain-JEPA leads with AUROC = 0.9250</p> <ul> <li>Gap to \ud83e\udd48 BrainLM: +0.0150</li> <li>Score spread (best to worst): 0.0950</li> </ul> \ud83d\udcd0 How are scores calculated? (click to expand)  ---  ### \ud83d\udcd6 Understanding This Leaderboard  This section explains how we measure and compare AI models. Don't worry if you're new to AI metrics \u2014 we'll break it down step by step.  ---  ### \ud83c\udfaf The Main Metric: `AUROC`  **Area Under ROC Curve (AUROC)**  **In simple terms:** Measures how well the model can tell apart different categories (e.g., healthy vs. diseased)  **How it works:** Think of it like this: if you randomly pick one positive case and one negative case, AUROC tells you the probability that the model correctly identifies which is which. A score of 0.5 means the model is just guessing randomly (like flipping a coin), while 1.0 means it perfectly separates all cases.  **Score range:** 0.5 (random guessing) \u2192 1.0 (perfect separation)  \ud83d\udca1 **Example:** An AUROC of 0.85 means the model correctly ranks a positive case higher than a negative case 85% of the time.  ---  ### \ud83e\udde0 How This Metric Fits This Task  Different tasks emphasize different aspects of performance. Here's how this metric should be interpreted for this benchmark:  - For **classification** tasks (e.g., disease vs. no disease), this metric helps you understand   how reliably the model separates different outcome groups. In addition to raw accuracy,   we recommend also looking at metrics like AUROC and F1 Score, especially when classes are   imbalanced (for example, when positive cases are rare).  ---  ### \ud83d\udcca Performance Tiers: What Do the Scores Mean?  We group models into performance tiers to help you quickly understand how ready they are for different uses:  | Score Range | Rating | Interpretation | Suitable For | |:---:|:---:|:---|:---| | **\u2265 0.90** | \u2b50 Excellent | Top-tier performance, consistently reliable | Clinical pilots with physician oversight | | **0.80 \u2013 0.89** | \u2705 Good | Strong performance, shows real promise | Validation studies, controlled testing | | **0.70 \u2013 0.79** | \ud83d\udd36 Fair | Moderate performance, has limitations | Research and development only | | **&lt; 0.70** | \ud83d\udcc8 Developing | Below typical benchmarks, needs improvement | Early research, not for clinical use |  !!! tip \"Important Context\"     These thresholds are general guidelines. The acceptable score depends on the specific clinical application, risk level, and whether the AI assists or replaces human judgment. Always consult domain experts when evaluating fitness for a particular use case.  ---  ### \ud83d\udccf How We Determine Rankings  Models are ranked following these principles:  1. **Primary metric determines rank** \u2014 The model with the highest score in the main metric ranks first. For metrics where lower is better (like error rates), the lowest score wins.  2. **Ties are broken by secondary metrics** \u2014 If two models have identical primary scores, we look at other relevant metrics to determine which performs better overall.  3. **Best run per model** \u2014 If a model was evaluated multiple times (e.g., with different settings), only its best result appears on the leaderboard. This ensures fair comparison.  4. **Reproducibility required** \u2014 All results must be reproducible. We record the evaluation date, dataset used, and configuration to ensure transparency.  ---  ### \ud83c\udfe5 Why This Matters for Healthcare AI  Healthcare AI has higher stakes than many other AI applications. A model that works 95% of the time might sound good, but that 5% could mean missed diagnoses or incorrect treatments. That's why we:  - Use **multiple metrics** to capture different aspects of performance - Test **robustness** to real-world data quality issues - Require **transparency** about evaluation conditions - Follow **international standards** for healthcare AI assessment  ---  ### \ud83c\udf0d Standards Alignment  This benchmark follows the [ITU/WHO Focus Group on AI for Health (FG-AI4H)](https://www.itu.int/pub/T-FG-AI4H) framework, which provides internationally recognized guidelines for evaluating healthcare AI systems. This ensures our evaluations are:  - **Rigorous** \u2014 Following established scientific methodology - **Comparable** \u2014 Using standardized metrics across different models - **Trustworthy** \u2014 Aligned with WHO/ITU recommendations for health AI"},{"location":"leaderboards/#reconstruction","title":"\ud83d\udd04 Reconstruction","text":""},{"location":"leaderboards/#brain-time-series-modeling","title":"Brain Time-Series Modeling","text":"<p>Evaluating ability to reconstruct masked fMRI voxel time-series.</p> <p>1 models ranked by <code>Correlation</code>:</p> Rank Model Score Level Details \ud83e\udd47 BrainLM \ud83d\udc51 0.7800 \ud83d\udd36 Fair UK Biobank fMRI tens, 2025-11-15 \ud83d\udcd0 How are scores calculated? (click to expand)  ---  ### \ud83d\udcd6 Understanding This Leaderboard  This section explains how we measure and compare AI models. Don't worry if you're new to AI metrics \u2014 we'll break it down step by step.  ---  ### \ud83c\udfaf The Main Metric: `Correlation`  **Correlation**  **In simple terms:** How closely the model's predictions match the actual values  **How it works:** Correlation measures the strength and direction of the relationship between predicted and actual values. A correlation of 1.0 means perfect positive agreement (when actual goes up, prediction goes up proportionally), while 0 means no relationship at all. This is commonly used for reconstruction tasks where we want to see how well the model can recreate the original signal.  **Score range:** -1.0 (perfect inverse) \u2192 0 (no relationship) \u2192 1.0 (perfect match)  \ud83d\udca1 **Example:** A correlation of 0.78 means the model's outputs track reasonably well with the true values.  ---  ### \ud83e\udde0 How This Metric Fits This Task  Different tasks emphasize different aspects of performance. Here's how this metric should be interpreted for this benchmark:  - For **regression / continuous prediction** tasks, this metric captures how closely the model's   predicted values track the true values over a range (e.g., symptom severity, signal amplitude).   We are usually interested in both overall fit (correlation) and error magnitude.  ---  ### \ud83d\udcca Performance Tiers: What Do the Scores Mean?  We group models into performance tiers to help you quickly understand how ready they are for different uses:  | Score Range | Rating | Interpretation | Suitable For | |:---:|:---:|:---|:---| | **\u2265 0.90** | \u2b50 Excellent | Top-tier performance, consistently reliable | Clinical pilots with physician oversight | | **0.80 \u2013 0.89** | \u2705 Good | Strong performance, shows real promise | Validation studies, controlled testing | | **0.70 \u2013 0.79** | \ud83d\udd36 Fair | Moderate performance, has limitations | Research and development only | | **&lt; 0.70** | \ud83d\udcc8 Developing | Below typical benchmarks, needs improvement | Early research, not for clinical use |  !!! tip \"Important Context\"     These thresholds are general guidelines. The acceptable score depends on the specific clinical application, risk level, and whether the AI assists or replaces human judgment. Always consult domain experts when evaluating fitness for a particular use case.  ---  ### \ud83d\udccf How We Determine Rankings  Models are ranked following these principles:  1. **Primary metric determines rank** \u2014 The model with the highest score in the main metric ranks first. For metrics where lower is better (like error rates), the lowest score wins.  2. **Ties are broken by secondary metrics** \u2014 If two models have identical primary scores, we look at other relevant metrics to determine which performs better overall.  3. **Best run per model** \u2014 If a model was evaluated multiple times (e.g., with different settings), only its best result appears on the leaderboard. This ensures fair comparison.  4. **Reproducibility required** \u2014 All results must be reproducible. We record the evaluation date, dataset used, and configuration to ensure transparency.  ---  ### \ud83c\udfe5 Why This Matters for Healthcare AI  Healthcare AI has higher stakes than many other AI applications. A model that works 95% of the time might sound good, but that 5% could mean missed diagnoses or incorrect treatments. That's why we:  - Use **multiple metrics** to capture different aspects of performance - Test **robustness** to real-world data quality issues - Require **transparency** about evaluation conditions - Follow **international standards** for healthcare AI assessment  ---  ### \ud83c\udf0d Standards Alignment  This benchmark follows the [ITU/WHO Focus Group on AI for Health (FG-AI4H)](https://www.itu.int/pub/T-FG-AI4H) framework, which provides internationally recognized guidelines for evaluating healthcare AI systems. This ensures our evaluations are:  - **Rigorous** \u2014 Following established scientific methodology - **Comparable** \u2014 Using standardized metrics across different models - **Trustworthy** \u2014 Aligned with WHO/ITU recommendations for health AI"},{"location":"leaderboards/#other-benchmarks","title":"\ud83d\udccb Other Benchmarks","text":""},{"location":"leaderboards/#foundation-model-robustness-evaluation","title":"Foundation Model Robustness Evaluation","text":"Rank Model Score Level Details \ud83e\udd47 geneformer \ud83d\udc51 0.9995 \u2b50 Excellent neuro/robustness, 2025-11-27 \ud83e\udd48 Brain-JEPA 0.8650 \u2705 Good DS-TOY-NEURO-ROBUSTN, 2024-01-20 \ud83e\udd49 BrainHarmony 0.8450 \u2705 Good DS-TOY-NEURO-ROBUSTN, 2024-01-18 \ud83c\udfc5 Geneformer 0.8350 \u2705 Good DS-TOY-GENOMICS, 2024-01-10 \ud83c\udfc5 BrainLM 0.8250 \u2705 Good DS-TOY-NEURO-ROBUSTN, 2024-01-16 \ud83c\udf96\ufe0f HyenaDNA 0.7950 \ud83d\udd36 Fair DS-TOY-GENOMICS, 2024-01-12 \ud83c\udf96\ufe0f Baseline (Random/Majority) 0.7810 \ud83d\udd36 Fair neuro/robustness, 2025-11-27"},{"location":"leaderboards/#add-your-model","title":"\ud83d\ude80 Add Your Model","text":"<p>Want your model on this leaderboard?</p> <ol> <li>Download the benchmark toolkit</li> <li>Run locally on your model (your code stays private!)</li> <li>Submit results via GitHub Issue</li> </ol> <p>\ud83d\udce5 Get Started \ud83d\udcd6 Submission Guide</p> <p>Aligned with ITU/WHO FG-AI4H standards for healthcare AI evaluation.</p>"},{"location":"models/","title":"Foundation Models Catalog","text":"<p>This page provides an overview of the foundation models evaluated in this benchmark hub. Each model is defined by a YAML configuration file in the <code>models/</code> directory.</p>"},{"location":"models/#neurology-brain-imaging-models","title":"\ud83e\udde0 Neurology / Brain Imaging Models","text":""},{"location":"models/#brainlm","title":"BrainLM","text":"<p>Model ID: <code>brainlm</code> Modality: fMRI (Brain functional imaging) Architecture: ViT-MAE + Nystromformer encoder/decoder Parameters: 111M / 650M Repository: github.com/vandijklab/BrainLM</p> <p>Masked autoencoding language model for fMRI voxel time-series. Uses ViT-MAE scaffolding with custom BrainLM embeddings that mix voxel coordinates and patched time windows to learn denoised cortical dynamics.</p>"},{"location":"models/#brainjepa","title":"BrainJEPA","text":"<p>Model ID: <code>brainjepa</code> Modality: fMRI, EEG Repository: Brain-JEPA Repository</p> <p>Joint-Embedding Predictive Architecture for brain signals. Self-supervised learning approach that learns representations by predicting latent representations rather than raw pixels/signals.</p>"},{"location":"models/#brainmt","title":"BrainMT","text":"<p>Model ID: <code>brainmt</code> Modality: Multi-modal brain imaging Repository: BrainMT Repository</p> <p>Multi-modal brain transformer for integrating structural and functional brain imaging data.</p>"},{"location":"models/#brainharmony","title":"BrainHarmony","text":"<p>Model ID: <code>brainharmony</code> Modality: Multi-site neuroimaging Repository: BrainHarmony Repository</p> <p>Harmonization framework for multi-site neuroimaging studies, addressing scanner and acquisition protocol variability.</p>"},{"location":"models/#uni","title":"UNI","text":"<p>Model ID: <code>MOD-UNI</code> Modality: MRI (Structural brain imaging) Repository: github.com/insitro/uni</p> <p>Vision Transformer trained on massive MRI datasets for general-purpose brain imaging analysis.</p>"},{"location":"models/#genomics-single-cell-models","title":"\ud83e\uddec Genomics / Single-Cell Models","text":""},{"location":"models/#geneformer","title":"Geneformer","text":"<p>Model ID: <code>MOD-GENEFORMER</code> Modality: scRNA-seq (Single-cell transcriptomics) Repository: huggingface.co/ctheodoris/Geneformer</p> <p>Transformer model pretrained on 30 million single cell transcriptomes. Learns context-aware gene embeddings for cell type annotation, gene regulatory network inference, and therapeutic target discovery.</p>"},{"location":"models/#caduceus","title":"Caduceus","text":"<p>Model ID: <code>caduceus</code> Modality: DNA sequences Repository: Caduceus Repository</p> <p>Long-range DNA sequence model using efficient attention mechanisms for genomic variant interpretation.</p>"},{"location":"models/#dnabert-2","title":"DNABERT-2","text":"<p>Model ID: <code>dnabert2</code> Modality: DNA sequences Repository: DNABERT-2 Repository</p> <p>BERT-based model for DNA sequence understanding, supporting tasks like promoter prediction, splice site detection, and variant effect prediction.</p>"},{"location":"models/#evo2","title":"Evo2","text":"<p>Model ID: <code>evo2</code> Modality: DNA/RNA sequences Repository: Evo2 Repository</p> <p>Evolution-inspired foundation model for sequence analysis and generation.</p>"},{"location":"models/#hyenadna","title":"HyenaDNA","text":"<p>Model ID: <code>hyenadna</code> Modality: Long DNA sequences Repository: HyenaDNA Repository</p> <p>Efficient long-range sequence model using Hyena operators for genomic analysis at scale.</p>"},{"location":"models/#nucleotide-transformer-generator","title":"Nucleotide Transformer (Generator)","text":"<p>Model ID: <code>generator</code> Modality: Nucleotide sequences Repository: Generator Repository</p> <p>Transformer-based generative model for nucleotide sequence synthesis and analysis.</p>"},{"location":"models/#multi-modal-models","title":"\ud83d\udd2c Multi-Modal Models","text":""},{"location":"models/#m3fm","title":"M3FM","text":"<p>Model ID: <code>m3fm</code> Modality: Multi-modal (imaging + genomics) Repository: M3FM Repository</p> <p>Multi-modal foundation model integrating imaging, genomics, and clinical data.</p>"},{"location":"models/#flamingo","title":"Flamingo","text":"<p>Model ID: <code>flamingo</code> Modality: Vision-Language (Medical imaging + text) Repository: Flamingo Repository</p> <p>Vision-language model adapted for medical imaging and clinical text understanding.</p>"},{"location":"models/#vlm-dev-clinical","title":"VLM Dev Clinical","text":"<p>Model ID: <code>vlm_dev_clinical</code> Modality: Vision-Language (Clinical) Repository: Clinical VLM Development</p> <p>Vision-language model specifically developed for clinical applications.</p>"},{"location":"models/#specialized-models","title":"\ud83e\uddea Specialized Models","text":""},{"location":"models/#swift","title":"SWIFT","text":"<p>Model ID: <code>swift</code> Modality: Time-series analysis Repository: SWIFT Repository</p> <p>Specialized model for time-series analysis in biological signals.</p>"},{"location":"models/#tabpfn","title":"TabPFN","text":"<p>Model ID: <code>tabpfn</code> Modality: Tabular data Repository: TabPFN Repository</p> <p>Prior-data fitted network for tabular data classification, useful for clinical structured data.</p>"},{"location":"models/#titan","title":"Titan","text":"<p>Model ID: <code>titan</code> Modality: Large-scale biological data Repository: Titan Repository</p> <p>Large-scale foundation model for integrative biological data analysis.</p>"},{"location":"models/#me-llama","title":"ME-LLaMA","text":"<p>Model ID: <code>me_llama</code> Modality: Medical language Repository: ME-LLaMA Repository</p> <p>Medical-specialized language model based on LLaMA architecture.</p>"},{"location":"models/#llm-semantic-bridge","title":"LLM Semantic Bridge","text":"<p>Model ID: <code>llm_semantic_bridge</code> Modality: Multi-modal semantic alignment Repository: Semantic Bridge Development</p> <p>Model for bridging semantic representations across different medical data modalities.</p>"},{"location":"models/#model-configuration-format","title":"\ud83d\udcdd Model Configuration Format","text":"<p>Each model is defined in a YAML file with the following structure:</p> <pre><code>model_id: unique_identifier\nname: Human-Readable Name\nmodality: primary_modality\nupstream_repo: https://github.com/org/repo\nnotes: Description of the model architecture and capabilities\narch: Architecture details (optional)\nparams: Parameter count (optional)\n</code></pre>"},{"location":"models/#adding-your-model","title":"\ud83c\udfaf Adding Your Model","text":"<p>To add your foundation model to the benchmark:</p> <ol> <li>Create a model configuration YAML in <code>models/</code></li> <li>Implement the model interface (see <code>fmbench/models.py</code>)</li> <li>Run the benchmark suite(s) relevant to your model's modality</li> <li>Submit results for leaderboard inclusion</li> </ol> <p>See the contributing guide for more details.</p>"},{"location":"models/#model-performance","title":"\ud83d\udcca Model Performance","text":"<p>For detailed performance metrics and rankings, see the Leaderboards.</p>"}]}