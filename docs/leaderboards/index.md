# ğŸ† Foundation Model Leaderboards

!!! success "Benchmark Hub Stats"
    ğŸ¯ **7** Benchmarks | ğŸ¤– **20** Models Evaluated | ğŸ“Š **36** Total Evaluations

Welcome to the **AI4H-Inspired FM Benchmark Hub**! Rankings below show **all submitted models** from best to developing, helping you find the right model for your use case.

## ğŸ§­ Quick Navigation

- [ğŸŒ Cross-Domain](#cross-domain)
- [ğŸ§¬ Genomics](#genomics)
- [ğŸ§  Neurology](#neurology)

---

## ğŸŒ Cross-Domain

### ğŸŒ Clinical Report Generation Quality

âœï¸ **Task**: Generation | ğŸ¥ **Health Topic**: Automated Clinical Reporting

!!! info "Clinical Relevance"
    Foundation models increasingly generate clinical reports, radiology  interpretations, and patient summaries. Quality metrics must capture both linguistic fluency and clinical accuracy/safety.


#### ğŸ† Leaderboard

```
         ğŸ¥‡          
     [Me-LLaMA]    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
 ğŸ¥ˆ â”‚         â”‚ ğŸ¥‰  
[M3FM]â”‚         â”‚[OpenFlamin]
â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€
```

**All 6 models ranked by report_quality_score:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **Me-LLaMA** ğŸ‘‘ | 0.8750 |  | mimic_cxr_reports | 2024-02-05 |
| ğŸ¥ˆ | **M3FM** ğŸŒŸ | 0.8600 |  | mimic_cxr_reports | 2024-01-28 |
| ğŸ¥‰ | **OpenFlamingo** âœ¨ | 0.8400 |  | mimic_cxr_reports | 2024-01-20 |
| ğŸ… | TITAN | 0.8100 |  | mimic_cxr_reports | 2024-01-25 |
| ğŸ… | Med-Flamingo | 0.7800 |  | mimic_cxr_reports | 2024-01-18 |
| ğŸ–ï¸ | RadBERT | 0.6900 |  | mimic_cxr_reports | 2024-01-12 |


#### ğŸ“– Ranking Explanation

!!! abstract "Why These Rankings?"
    **ğŸ¥‡ Me-LLaMA** leads with report_quality_score=0.8750

    - Gap to ğŸ¥ˆ **M3FM**: +0.0150 (1.7% better)
    - Score range across all models: 0.1850


<details>
<summary>ğŸ“ <strong>How are models scored?</strong></summary>

**Ranking by:** `report_quality_score`

| Score | Rating | Meaning |
|:---:|:---:|:---|
| â‰¥0.90 | â­ Excellent | Clinical-ready |
| 0.80-0.89 | âœ… Good | Needs validation |
| 0.70-0.79 | ğŸ”¶ Fair | Research only |
| <0.70 | ğŸ“ˆ Developing | Not recommended |

*Aligned with [ITU/WHO AI4H](https://www.itu.int/pub/T-FG-AI4H) standards (DEL3).*

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | report_quality_score | clinical_accuracy | bertscore | bleu | finding_recall | hallucination_rate | finding_precision | flesch_kincaid |
|:---:|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| ğŸ¥‡  | Me-LLaMA | **0.8750** | 0.9200 | 0.9000 | 43.5000 | 0.8800 | 0.0400 | 0.9300 | 9.5000 |
| ğŸ¥ˆ  | M3FM | **0.8600** | 0.9100 | 0.8900 | 41.2000 | 0.8700 | 0.0450 | 0.9200 | 9.8000 |
| ğŸ¥‰  | OpenFlamingo | **0.8400** | 0.8900 | 0.8700 | 38.5000 | 0.8500 | 0.0600 | 0.9100 | 10.2000 |
| ğŸ…  | TITAN | **0.8100** | 0.8600 | 0.8500 | 35.2000 | 0.8200 | 0.0700 | 0.8800 | 10.8000 |
| ğŸ…  | Med-Flamingo | **0.7800** | 0.8200 | 0.8200 | 32.5000 | 0.7900 | 0.0900 | 0.8500 | 11.5000 |
| ğŸ–ï¸  | RadBERT | **0.6900** | 0.7200 | 0.7400 | 24.2000 | 0.6800 | 0.1500 | 0.7500 | 13.2000 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: report_quality_score (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

#### ğŸ“Š Granular Performance Breakdown

Expand sections below to see how models perform across different conditions:


<details>
<summary>ğŸ“„ <strong>OpenFlamingo</strong> by Report Type</summary>

| Report Type | clinical_accuracy | finding_recall | bertscore | N |
|---|---|---|---|---|
| ğŸ¥‡ chest_xray | 0.9100 | 0.8700 | 0.8800 | 2000 |
| ğŸ¥ˆ brain_mri | 0.8800 | 0.8400 | 0.8600 | 600 |
| ct_abdomen | 0.8600 | 0.8200 | 0.8500 | 800 |

</details>

<details>
<summary>ğŸ“Š <strong>OpenFlamingo</strong> by Complexity</summary>

| Complexity | clinical_accuracy | hallucination_rate | N |
|---|---|---|---|
| ğŸ¥‡ simple | 0.9400 | 0.0300 | 1500 |
| ğŸ¥ˆ moderate | 0.8800 | 0.0600 | 1200 |
| complex | 0.8200 | 0.1000 | 700 |

</details>

<details>
<summary>ğŸ“„ <strong>Med-Flamingo</strong> by Report Type</summary>

| Report Type | clinical_accuracy | finding_recall | bertscore | N |
|---|---|---|---|---|
| ğŸ¥‡ chest_xray | 0.8500 | 0.8100 | 0.8300 | 2000 |
| ğŸ¥ˆ brain_mri | 0.8000 | 0.7600 | 0.8000 | 600 |
| ct_abdomen | 0.7800 | 0.7400 | 0.7900 | 800 |

</details>

---
*Ranked by **report_quality_score** (higher is better). Last updated from 6 evaluation(s).*

### ğŸŒ Foundation Model Robustness Evaluation

ğŸ›¡ï¸ **Task**: Robustness Assessment | ğŸ¥ **Health Topic**: Model Reliability and Artifact Resilience

!!! info "Clinical Relevance"
    Clinical deployment of AI models requires robustness to real-world data variability including sensor noise, signal artifacts, and acquisition differences. This benchmark evaluates model stability under controlled perturbations that simulate common data quality issues.


#### ğŸ† Leaderboard

```
         ğŸ¥‡          
     [Brain-JEPA]    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
 ğŸ¥ˆ â”‚         â”‚ ğŸ¥‰  
[BrainHarmo]â”‚         â”‚[Geneformer]
â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€
```

**All 6 models ranked by robustness_score:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **Brain-JEPA** ğŸ‘‘ | 0.8650 | âœ… Good | DS-TOY-NEURO-ROBUSTNESS | 2024-01-20 |
| ğŸ¥ˆ | **BrainHarmony** ğŸŒŸ | 0.8450 | âœ… Good | DS-TOY-NEURO-ROBUSTNESS | 2024-01-18 |
| ğŸ¥‰ | **Geneformer** âœ¨ | 0.8350 | âœ… Good | DS-TOY-GENOMICS | 2024-01-10 |
| ğŸ… | BrainLM | 0.8250 | âœ… Good | DS-TOY-NEURO-ROBUSTNESS | 2024-01-16 |
| ğŸ… | HyenaDNA | 0.7950 | ğŸ”¶ Fair | DS-TOY-GENOMICS | 2024-01-12 |
| ğŸ–ï¸ | Baseline (Random/Majority) | 0.7810 | ğŸ”¶ Fair | - | 2025-11-27 |


#### ğŸ“– Ranking Explanation

!!! abstract "Why These Rankings?"
    **ğŸ¥‡ Brain-JEPA** leads with robustness_score=0.8650

    - Gap to ğŸ¥ˆ **BrainHarmony**: +0.0200 (2.4% better)
    - Score range across all models: 0.0840
    - Performance distribution: âœ… 4 good, ğŸ”¶ 2 fair


<details>
<summary>ğŸ“ <strong>How are models scored?</strong></summary>

**Ranking by:** `robustness_score`

| Score | Rating | Meaning |
|:---:|:---:|:---|
| â‰¥0.90 | â­ Excellent | Clinical-ready |
| 0.80-0.89 | âœ… Good | Needs validation |
| 0.70-0.79 | ğŸ”¶ Fair | Research only |
| <0.70 | ğŸ“ˆ Developing | Not recommended |

*Aligned with [ITU/WHO AI4H](https://www.itu.int/pub/T-FG-AI4H) standards (DEL3).*

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | robustness_score | dropout_rAUC | expression_rAUC | line_noise_rAUC | masking_rAUC | noise_rAUC | perm_equivariance | shift_rAUC |
|:---:|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| ğŸ¥‡ âœ… | Brain-JEPA | **0.8650** | 0.8800 | - | 0.8400 | - | 0.8500 | 0.8900 | 0.8650 |
| ğŸ¥ˆ âœ… | BrainHarmony | **0.8450** | 0.8600 | - | 0.8200 | - | 0.8300 | 0.8700 | 0.8450 |
| ğŸ¥‰ âœ… | Geneformer | **0.8350** | 0.8500 | 0.8100 | - | 0.8600 | 0.8200 | 0.8350 | - |
| ğŸ… âœ… | BrainLM | **0.8250** | 0.8400 | - | 0.8000 | - | 0.8100 | 0.8500 | 0.8250 |
| ğŸ… ğŸ”¶ | HyenaDNA | **0.7950** | 0.8100 | 0.7700 | - | 0.8200 | 0.7800 | 0.8000 | - |
| ğŸ–ï¸ ğŸ”¶ | Baseline (Random/Majority) | **0.7810** | 0.7760 | - | 0.7737 | - | 0.7867 | 0.7819 | 0.7874 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: robustness_score (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

---
*Ranked by **robustness_score** (higher is better). Last updated from 10 evaluation(s).*

## ğŸ§¬ Genomics

### ğŸ§¬ Cell Type Annotation

ğŸ¯ **Task**: Classification | ğŸ¥ **Health Topic**: Single-cell Transcriptomics

*Predicting cell types from single-cell RNA-seq data.*

!!! info "Clinical Relevance"
    Automated characterization of immune cell populations.

#### ğŸ† Leaderboard

```
         ğŸ¥‡          
     [Evo 2]    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
 ğŸ¥ˆ â”‚         â”‚ ğŸ¥‰  
[Geneformer]â”‚         â”‚[SWIFT]
â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€
```

**All 6 models ranked by Accuracy:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **Evo 2** ğŸ‘‘ | 0.9250 | â­ Excellent | PBMC 68k | 2024-02-01 |
| ğŸ¥ˆ | **Geneformer** ğŸŒŸ | 0.9100 | â­ Excellent | PBMC 68k | 2023-11-01 |
| ğŸ¥‰ | **SWIFT** âœ¨ | 0.8950 | âœ… Good | PBMC 68k | 2024-01-15 |
| ğŸ… | Caduceus | 0.8850 | âœ… Good | PBMC 68k | 2024-01-12 |
| ğŸ… | HyenaDNA | 0.8700 | âœ… Good | PBMC 68k | 2024-01-08 |
| ğŸ–ï¸ | DNABERT-2 | 0.8500 | âœ… Good | PBMC 68k | 2024-01-05 |


#### ğŸ“– Ranking Explanation

!!! abstract "Why These Rankings?"
    **ğŸ¥‡ Evo 2** leads with Accuracy=0.9250

    - Gap to ğŸ¥ˆ **Geneformer**: +0.0150 (1.6% better)
    - Score range across all models: 0.0750
    - Performance distribution: â­ 2 excellent, âœ… 4 good


<details>
<summary>ğŸ“ <strong>How are models scored?</strong></summary>

**Ranking by:** `Accuracy`

| Score | Rating | Meaning |
|:---:|:---:|:---|
| â‰¥0.90 | â­ Excellent | Clinical-ready |
| 0.80-0.89 | âœ… Good | Needs validation |
| 0.70-0.79 | ğŸ”¶ Fair | Research only |
| <0.70 | ğŸ“ˆ Developing | Not recommended |

*Aligned with [ITU/WHO AI4H](https://www.itu.int/pub/T-FG-AI4H) standards (DEL3).*

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | Accuracy | F1-Score |
|:---:|:---|:---:|:---:|
| ğŸ¥‡ â­ | Evo 2 | **0.9250** | 0.8900 |
| ğŸ¥ˆ â­ | Geneformer | **0.9100** | 0.8500 |
| ğŸ¥‰ âœ… | SWIFT | **0.8950** | 0.8550 |
| ğŸ… âœ… | Caduceus | **0.8850** | 0.8400 |
| ğŸ… âœ… | HyenaDNA | **0.8700** | 0.8200 |
| ğŸ–ï¸ âœ… | DNABERT-2 | **0.8500** | 0.8000 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: Accuracy (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

---
*Ranked by **Accuracy** (higher is better). Last updated from 6 evaluation(s).*

## ğŸ§  Neurology

### ğŸ§  Alzheimer's Disease Classification using Brain MRI

ğŸ¯ **Task**: Classification | ğŸ¥ **Health Topic**: Alzheimer's Disease

*Binary classification of AD vs CN using structural MRI data.*

!!! info "Clinical Relevance"
    Automated screening for AD to assist radiological workflow.

#### ğŸ† Leaderboard

```
         ğŸ¥‡          
     [Brain-JEPA]    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
 ğŸ¥ˆ â”‚         â”‚ ğŸ¥‰  
[UNI]â”‚         â”‚[BrainLM]
â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€
```

**All 3 models ranked by AUROC:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **Brain-JEPA** ğŸ‘‘ | 0.9350 | â­ Excellent | ADNI | 2024-01-20 |
| ğŸ¥ˆ | **UNI** ğŸŒŸ | 0.9200 | â­ Excellent | Alzheimer's Disease Neuroimaging Initiative (ADNI) | 2023-10-27 |
| ğŸ¥‰ | **BrainLM** âœ¨ | 0.9100 | â­ Excellent | ADNI | 2024-01-15 |


#### ğŸ“– Ranking Explanation

!!! abstract "Why These Rankings?"
    **ğŸ¥‡ Brain-JEPA** leads with AUROC=0.9350

    - Gap to ğŸ¥ˆ **UNI**: +0.0150 (1.6% better)
    - Score range across all models: 0.0250
    - Performance distribution: â­ 3 excellent


<details>
<summary>ğŸ“ <strong>How are models scored?</strong></summary>

**Ranking by:** `AUROC`

| Score | Rating | Meaning |
|:---:|:---:|:---|
| â‰¥0.90 | â­ Excellent | Clinical-ready |
| 0.80-0.89 | âœ… Good | Needs validation |
| 0.70-0.79 | ğŸ”¶ Fair | Research only |
| <0.70 | ğŸ“ˆ Developing | Not recommended |

*Aligned with [ITU/WHO AI4H](https://www.itu.int/pub/T-FG-AI4H) standards (DEL3).*

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | AUROC | Accuracy |
|:---:|:---|:---:|:---:|
| ğŸ¥‡ â­ | Brain-JEPA | **0.9350** | 0.8950 |
| ğŸ¥ˆ â­ | UNI | **0.9200** | 0.8800 |
| ğŸ¥‰ â­ | BrainLM | **0.9100** | 0.8700 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: AUROC (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

---
*Ranked by **AUROC** (higher is better). Last updated from 3 evaluation(s).*

### ğŸ§  Brain Time-Series Modeling

ğŸ”„ **Task**: Reconstruction | ğŸ¥ **Health Topic**: Functional Brain Connectivity

*Evaluating ability to reconstruct masked fMRI voxel time-series.*

!!! info "Clinical Relevance"
    Foundation for understanding functional connectivity patterns.

#### ğŸ† Leaderboard

**All 1 models ranked by Correlation:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **BrainLM** ğŸ‘‘ | 0.7800 |  | UK Biobank fMRI tensors | 2025-11-15 |


<details>
<summary>ğŸ“ <strong>How are models scored?</strong></summary>

**Ranking by:** `Correlation`

| Score | Rating | Meaning |
|:---:|:---:|:---|
| â‰¥0.90 | â­ Excellent | Clinical-ready |
| 0.80-0.89 | âœ… Good | Needs validation |
| 0.70-0.79 | ğŸ”¶ Fair | Research only |
| <0.70 | ğŸ“ˆ Developing | Not recommended |

*Aligned with [ITU/WHO AI4H](https://www.itu.int/pub/T-FG-AI4H) standards (DEL3).*

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | Correlation | MSE |
|:---:|:---|:---:|:---:|
| ğŸ¥‡  | BrainLM | **0.7800** | 0.4500 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: Correlation (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

---
*Ranked by **Correlation** (higher is better). Last updated from 1 evaluation(s).*

### ğŸ§  Toy Classification Benchmark

ğŸ¯ **Task**: Classification | ğŸ¥ **Health Topic**: N/A

*A toy benchmark for testing the pipeline.*

#### ğŸ† Leaderboard

**All 1 models ranked by AUROC:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **Baseline (Random/Majority)** ğŸ‘‘ | 0.5597 | ğŸ“ˆ Developing | Toy fMRI Classification | 2025-11-27 |


<details>
<summary>ğŸ“ <strong>How are models scored?</strong></summary>

**Ranking by:** `AUROC`

| Score | Rating | Meaning |
|:---:|:---:|:---|
| â‰¥0.90 | â­ Excellent | Clinical-ready |
| 0.80-0.89 | âœ… Good | Needs validation |
| 0.70-0.79 | ğŸ”¶ Fair | Research only |
| <0.70 | ğŸ“ˆ Developing | Not recommended |

*Aligned with [ITU/WHO AI4H](https://www.itu.int/pub/T-FG-AI4H) standards (DEL3).*

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | AUROC | Accuracy | F1-Score |
|:---:|:---|:---:|:---:|:---:|
| ğŸ¥‡ ğŸ“ˆ | Baseline (Random/Majority) | **0.5597** | 0.5750 | 0.5732 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: AUROC (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

#### ğŸ“Š Granular Performance Breakdown

Expand sections below to see how models perform across different conditions:


<details>
<summary>ğŸ”¬ <strong>Baseline (Random/Majority)</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ GE | 0.6373 | 0.6286 | 0.6274 | 70 |
| ğŸ¥ˆ Siemens | 0.5844 | 0.5789 | 0.5788 | 57 |
| Philips | 0.4662 | 0.5205 | 0.5147 | 73 |

</details>

<details>
<summary>ğŸ¥ <strong>Baseline (Random/Majority)</strong> by Site</summary>

| Site | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ SiteC | 0.6348 | 0.5915 | 0.5912 | 71 |
| ğŸ¥ˆ SiteB | 0.6305 | 0.6316 | 0.6298 | 57 |
| SiteA | 0.4201 | 0.5139 | 0.5093 | 72 |

</details>

<details>
<summary>ğŸ©º <strong>Baseline (Random/Majority)</strong> by Disease Stage</summary>

| Disease Stage | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ MCI | 0.6085 | 0.6000 | 0.5987 | 70 |
| ğŸ¥ˆ CN | 0.5559 | 0.5429 | 0.5414 | 70 |
| AD | 0.4955 | 0.5833 | 0.5804 | 60 |

</details>

<details>
<summary>ğŸ‘¤ <strong>Baseline (Random/Majority)</strong> by Sex</summary>

| Sex | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ M | 0.6061 | 0.6111 | 0.6045 | 108 |
| F | 0.5021 | 0.5326 | 0.5326 | 92 |

</details>

<details>
<summary>ğŸ“… <strong>Baseline (Random/Majority)</strong> by Age Group</summary>

| Age Group | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ age_80-100 | 0.6000 | 0.5455 | 0.5299 | 11 |
| ğŸ¥ˆ age_60-80 | 0.5943 | 0.5857 | 0.5788 | 70 |
| ğŸ¥‰ age_20-40 | 0.5819 | 0.5741 | 0.5668 | 54 |
| age_40-60 | 0.4810 | 0.5692 | 0.5513 | 65 |

</details>

---
*Ranked by **AUROC** (higher is better). Last updated from 5 evaluation(s).*

### ğŸ§  fMRI Foundation Model Benchmark (Granular)

ğŸ“‹ **Task**: Classification/Reconstruction | ğŸ¥ **Health Topic**: Functional Brain Imaging Analysis

!!! info "Clinical Relevance"
    Foundation models for fMRI must generalize across diverse acquisition  parameters, scanner manufacturers, and preprocessing pipelines. This benchmark provides granular rankings to identify optimal model-data matches.


#### ğŸ† Leaderboard

```
         ğŸ¥‡          
     [Brain-JEPA]    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
 ğŸ¥ˆ â”‚         â”‚ ğŸ¥‰  
[BrainLM]â”‚         â”‚[BrainBERT]
â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€
```

**All 5 models ranked by AUROC:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **Brain-JEPA** ğŸ‘‘ | 0.9250 | â­ Excellent | hcp_1200 | 2024-01-22 |
| ğŸ¥ˆ | **BrainLM** ğŸŒŸ | 0.9100 | â­ Excellent | hcp_1200 | 2024-01-15 |
| ğŸ¥‰ | **BrainBERT** âœ¨ | 0.8700 | âœ… Good | hcp_1200 | 2024-01-10 |
| ğŸ… | BrainMT | 0.8500 | âœ… Good | hcp_1200 | 2024-01-18 |
| ğŸ… | NeuroCLIP | 0.8300 | âœ… Good | hcp_1200 | 2024-01-05 |


#### ğŸ“– Ranking Explanation

!!! abstract "Why These Rankings?"
    **ğŸ¥‡ Brain-JEPA** leads with AUROC=0.9250

    - Gap to ğŸ¥ˆ **BrainLM**: +0.0150 (1.6% better)
    - Score range across all models: 0.0950
    - Performance distribution: â­ 2 excellent, âœ… 3 good


<details>
<summary>ğŸ“ <strong>How are models scored?</strong></summary>

**Ranking by:** `AUROC`

| Score | Rating | Meaning |
|:---:|:---:|:---|
| â‰¥0.90 | â­ Excellent | Clinical-ready |
| 0.80-0.89 | âœ… Good | Needs validation |
| 0.70-0.79 | ğŸ”¶ Fair | Research only |
| <0.70 | ğŸ“ˆ Developing | Not recommended |

*Aligned with [ITU/WHO AI4H](https://www.itu.int/pub/T-FG-AI4H) standards (DEL3).*

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | AUROC | Accuracy | F1-Score | Correlation | MSE |
|:---:|:---|:---:|:---:|:---:|:---:|:---:|
| ğŸ¥‡ â­ | Brain-JEPA | **0.9250** | 0.8900 | 0.8800 | 0.8300 | 0.3900 |
| ğŸ¥ˆ â­ | BrainLM | **0.9100** | 0.8700 | 0.8600 | 0.8100 | 0.4200 |
| ğŸ¥‰ âœ… | BrainBERT | **0.8700** | 0.8200 | 0.8100 | 0.7600 | 0.5100 |
| ğŸ… âœ… | BrainMT | **0.8500** | 0.8100 | 0.8000 | 0.7400 | 0.5500 |
| ğŸ… âœ… | NeuroCLIP | **0.8300** | 0.7900 | 0.7800 | 0.7200 | 0.5800 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: AUROC (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

#### ğŸ“Š Granular Performance Breakdown

Expand sections below to see how models perform across different conditions:


<details>
<summary>ğŸ”¬ <strong>Brain-JEPA</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ Siemens | 0.9400 | 0.9100 | 0.9000 | 450 |
| ğŸ¥ˆ Philips | 0.9200 | 0.8800 | 0.8700 | 370 |
| GE | 0.9100 | 0.8700 | 0.8600 | 380 |

</details>

<details>
<summary>ğŸ“¡ <strong>Brain-JEPA</strong> by Acquisition Type</summary>

| Acquisition Type | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ resting_state | 0.9350 | 0.9000 | 0.8900 | 600 |
| task_based | 0.9100 | 0.8700 | 0.8600 | 400 |

</details>

<details>
<summary>ğŸ”¬ <strong>BrainLM</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ Siemens | 0.9300 | 0.8900 | 0.8800 | 450 |
| ğŸ¥ˆ Philips | 0.9000 | 0.8600 | 0.8500 | 370 |
| GE | 0.8800 | 0.8400 | 0.8300 | 380 |

</details>

<details>
<summary>ğŸ¥ <strong>BrainLM</strong> by Site</summary>

| Site | AUROC | Accuracy | N |
|---|---|---|---|
| ğŸ¥‡ WashU | 0.9300 | 0.8900 | 220 |
| ğŸ¥ˆ MGH | 0.9200 | 0.8800 | 200 |
| ğŸ¥‰ Oxford | 0.9100 | 0.8700 | 200 |
| UCLA | 0.9000 | 0.8600 | 180 |
| UMinn | 0.8900 | 0.8500 | 200 |

</details>

<details>
<summary>ğŸ“¡ <strong>BrainLM</strong> by Acquisition Type</summary>

| Acquisition Type | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ resting_state | 0.9200 | 0.8800 | 0.8700 | 600 |
| ğŸ¥ˆ language | 0.9100 | 0.8700 | - | 100 |
| ğŸ¥‰ working_memory | 0.9000 | 0.8600 | - | 150 |
| task_based | 0.8900 | 0.8500 | 0.8400 | 400 |
| motor | 0.8800 | 0.8400 | - | 150 |

</details>

<details>
<summary>âš™ï¸ <strong>BrainLM</strong> by Preprocessing</summary>

| Preprocessing | AUROC | Accuracy | N |
|---|---|---|---|
| ğŸ¥‡ fmriprep | 0.9200 | 0.8800 | 500 |
| ğŸ¥ˆ hcp | 0.9100 | 0.8700 | 400 |
| minimal | 0.8500 | 0.8100 | 300 |

</details>

<details>
<summary>ğŸ§² <strong>BrainLM</strong> by Field Strength</summary>

| Field Strength | AUROC | Accuracy | N |
|---|---|---|---|
| ğŸ¥‡ 7T | 0.9400 | 0.9100 | 100 |
| 3T | 0.9100 | 0.8700 | 900 |

</details>

<details>
<summary>ğŸ”¬ <strong>BrainBERT</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ Siemens | 0.8900 | 0.8400 | 0.8300 | 450 |
| ğŸ¥ˆ GE | 0.8600 | 0.8100 | 0.8000 | 380 |
| Philips | 0.8500 | 0.8000 | 0.7900 | 370 |

</details>

<details>
<summary>ğŸ“¡ <strong>BrainBERT</strong> by Acquisition Type</summary>

| Acquisition Type | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ resting_state | 0.8800 | 0.8300 | 0.8200 | 600 |
| task_based | 0.8500 | 0.8000 | 0.7900 | 400 |

</details>

<details>
<summary>ğŸ”¬ <strong>BrainMT</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ Siemens | 0.8700 | 0.8300 | 0.8200 | 450 |
| ğŸ¥ˆ GE | 0.8400 | 0.8000 | 0.7900 | 380 |
| Philips | 0.8300 | 0.7900 | 0.7800 | 370 |

</details>

<details>
<summary>ğŸ”¬ <strong>NeuroCLIP</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ Siemens | 0.8500 | 0.8100 | 0.8000 | 450 |
| ğŸ¥ˆ GE | 0.8200 | 0.7800 | 0.7700 | 380 |
| Philips | 0.8100 | 0.7700 | 0.7600 | 370 |

</details>

---
*Ranked by **AUROC** (higher is better). Last updated from 5 evaluation(s).*

---

## ğŸš€ Get Your Model on the Leaderboard

Want to see your Foundation Model ranked here?

1. ğŸ“¥ **Download** the benchmark suite and run locally
2. ğŸ§ª **Evaluate** your model: `python -m fmbench run --help`
3. ğŸ“¤ **Submit** your results via [GitHub Issue](https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub/issues/new?template=benchmark_submission.md)

ğŸ’¡ **Propose new evaluation protocols** via [Issue](https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub/issues/new?template=protocol_proposal.md)

!!! note "Curated Benchmark Hub"
    All submissions are reviewed before being added. See [Submission Guide](../contributing/submission_guide.md) for details.

*Aligned with [ITU/WHO FG-AI4H](https://www.itu.int/pub/T-FG-AI4H) standards for healthcare AI evaluation.*
