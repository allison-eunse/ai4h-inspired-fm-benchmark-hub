# ğŸ† Foundation Model Leaderboards

!!! success "Benchmark Hub Stats"
    ğŸ¯ **7** Benchmarks | ğŸ¤– **9** Models Evaluated | ğŸ“Š **19** Total Evaluations

Welcome to the **AI4H-Inspired FM Benchmark Hub**! Rankings below show **all submitted models** from best to developing, helping you find the right model for your use case.

## ğŸ§­ Quick Navigation

- [ğŸŒ Cross-Domain](#cross-domain)
- [ğŸ§¬ Genomics](#genomics)
- [ğŸ§  Neurology](#neurology)

---

## ğŸŒ Cross-Domain

### ğŸŒ Clinical Report Generation Quality

âœï¸ **Task**: Generation | ğŸ¥ **Health Topic**: Automated Clinical Reporting

!!! info "Clinical Relevance"
    Foundation models increasingly generate clinical reports, radiology  interpretations, and patient summaries. Quality metrics must capture both linguistic fluency and clinical accuracy/safety.


#### ğŸ† Leaderboard

```
         ğŸ¥‡          
     [Flamingo]    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
 ğŸ¥ˆ â”‚         â”‚ ğŸ¥‰  
[Med-Flamin]â”‚         â”‚[RadBERT]
â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€
```

**All 3 models ranked by report_quality_score:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **Flamingo** ğŸ‘‘ | 0.8400 |  | mimic_cxr_reports | 2024-01-20 |
| ğŸ¥ˆ | **Med-Flamingo** ğŸŒŸ | 0.7800 |  | mimic_cxr_reports | 2024-01-18 |
| ğŸ¥‰ | **RadBERT** âœ¨ | 0.6900 |  | mimic_cxr_reports | 2024-01-12 |


#### ğŸ“– Ranking Explanation

!!! abstract "Why These Rankings?"
    **ğŸ¥‡ Flamingo** leads with report_quality_score=0.8400

    - Gap to ğŸ¥ˆ **Med-Flamingo**: +0.0600 (7.7% better)
    - Score range across all models: 0.1500


#### ğŸ“ Scoring Methodology

<details>
<summary>ğŸ” <strong>How are models scored? (ITU/WHO AI4H Aligned)</strong></summary>

!!! note "ITU/WHO FG-AI4H Alignment"
    This evaluation framework follows [ITU-T FG-AI4H](https://www.itu.int/pub/T-FG-AI4H) standards:

    - **DEL3**: Performance metrics per System Requirement Specifications (SyRS)
    - **DEL0.1**: Standardized terminology (AI Solution, Benchmarking Run)
    - **DEL10.x**: Topic Description Documents for health domains

**Primary Ranking Metric: `report_quality_score`**

> Composite score of linguistic fluency + clinical accuracy (0.0-1.0)

**How is the primary metric chosen?** *(per DEL3 Section 6)*

For **generation tasks**, we prioritize:
1. `report_quality_score` â€“ composite clinical + linguistic quality
2. `clinical_accuracy` â€“ correctness of medical content
3. `bertscore` â€“ semantic similarity
4. `hallucination_rate` â€“ safety-critical (lower is better)

**Score Interpretation** *(Clinical Deployment Readiness)*

| Range | Tier | DEL3 Deployment Level | Clinical Guidance |
|:---:|:---:|:---:|:---|
| â‰¥ 0.90 | â­ Excellent | **Production Ready** | Suitable for clinical decision support with monitoring |
| 0.80-0.89 | âœ… Good | **Pilot/Validation** | Promising; requires prospective validation study |
| 0.70-0.79 | ğŸ”¶ Fair | **Research Only** | Research use; not for patient-facing applications |
| < 0.70 | ğŸ“ˆ Developing | **Development** | Requires significant improvement before deployment |

**Generalizability Analysis** *(DEL3 Section 4.3)*

Models are evaluated across demographic and technical strata:

- ğŸ‘¤ **Demographics**: Age groups, sex, ethnicity
- ğŸ”¬ **Technical**: Scanner manufacturer, acquisition parameters
- ğŸ¥ **Clinical**: Disease stage, comorbidities, site

Sub-group performance gaps > 10% are flagged for fairness review.

**Ranking Rules**

1. Models ranked by **primary metric** (descending, higher = better)
2. Ties broken by secondary metrics in priority order
3. Each model's **best evaluation run** is used
4. Scores reported to 4 decimal places for precision
5. Statistical significance assessed via bootstrap CI (when available)

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | report_quality_score | clinical_accuracy | bertscore | bleu | finding_recall | hallucination_rate | finding_precision | flesch_kincaid |
|:---:|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| ğŸ¥‡  | Flamingo | **0.8400** | 0.8900 | 0.8700 | 38.5000 | 0.8500 | 0.0600 | 0.9100 | 10.2000 |
| ğŸ¥ˆ  | Med-Flamingo | **0.7800** | 0.8200 | 0.8200 | 32.5000 | 0.7900 | 0.0900 | 0.8500 | 11.5000 |
| ğŸ¥‰  | RadBERT | **0.6900** | 0.7200 | 0.7400 | 24.2000 | 0.6800 | 0.1500 | 0.7500 | 13.2000 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: report_quality_score (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

#### ğŸ“Š Granular Performance Breakdown

Expand sections below to see how models perform across different conditions:


<details>
<summary>ğŸ“„ <strong>Flamingo</strong> by Report Type</summary>

| Report Type | clinical_accuracy | finding_recall | bertscore | N |
|---|---|---|---|---|
| ğŸ¥‡ chest_xray | 0.9100 | 0.8700 | 0.8800 | 2000 |
| ğŸ¥ˆ brain_mri | 0.8800 | 0.8400 | 0.8600 | 600 |
| ct_abdomen | 0.8600 | 0.8200 | 0.8500 | 800 |

</details>

<details>
<summary>ğŸ“Š <strong>Flamingo</strong> by Complexity</summary>

| Complexity | clinical_accuracy | hallucination_rate | N |
|---|---|---|---|
| ğŸ¥‡ simple | 0.9400 | 0.0300 | 1500 |
| ğŸ¥ˆ moderate | 0.8800 | 0.0600 | 1200 |
| complex | 0.8200 | 0.1000 | 700 |

</details>

<details>
<summary>ğŸ“„ <strong>Med-Flamingo</strong> by Report Type</summary>

| Report Type | clinical_accuracy | finding_recall | bertscore | N |
|---|---|---|---|---|
| ğŸ¥‡ chest_xray | 0.8500 | 0.8100 | 0.8300 | 2000 |
| ğŸ¥ˆ brain_mri | 0.8000 | 0.7600 | 0.8000 | 600 |
| ct_abdomen | 0.7800 | 0.7400 | 0.7900 | 800 |

</details>

---
*Ranked by **report_quality_score** (higher is better). Last updated from 3 evaluation(s).*

### ğŸŒ Foundation Model Robustness Evaluation

ğŸ›¡ï¸ **Task**: Robustness Assessment | ğŸ¥ **Health Topic**: Model Reliability and Artifact Resilience

!!! info "Clinical Relevance"
    Clinical deployment of AI models requires robustness to real-world data variability including sensor noise, signal artifacts, and acquisition differences. This benchmark evaluates model stability under controlled perturbations that simulate common data quality issues.


#### ğŸ† Leaderboard

**All 1 models ranked by robustness_score:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **dummy_classifier** ğŸ‘‘ | 0.7810 | ğŸ”¶ Fair | - | 2025-11-27 |


#### ğŸ“ Scoring Methodology

<details>
<summary>ğŸ” <strong>How are models scored? (ITU/WHO AI4H Aligned)</strong></summary>

!!! note "ITU/WHO FG-AI4H Alignment"
    This evaluation framework follows [ITU-T FG-AI4H](https://www.itu.int/pub/T-FG-AI4H) standards:

    - **DEL3**: Performance metrics per System Requirement Specifications (SyRS)
    - **DEL0.1**: Standardized terminology (AI Solution, Benchmarking Run)
    - **DEL10.x**: Topic Description Documents for health domains

**Primary Ranking Metric: `robustness_score`**

> Average performance retention under data perturbations (0.0-1.0)

**How is the primary metric chosen?** *(per DEL3 Section 6)*

For **robustness testing**, we prioritize:
1. `robustness_score` â€“ overall perturbation resilience
2. Individual probe scores (dropout, noise, shift, etc.)
3. `perm_equivariance` â€“ consistency under input reordering

**Score Interpretation** *(Clinical Deployment Readiness)*

| Range | Tier | DEL3 Deployment Level | Clinical Guidance |
|:---:|:---:|:---:|:---|
| â‰¥ 0.90 | â­ Excellent | **Production Ready** | Suitable for clinical decision support with monitoring |
| 0.80-0.89 | âœ… Good | **Pilot/Validation** | Promising; requires prospective validation study |
| 0.70-0.79 | ğŸ”¶ Fair | **Research Only** | Research use; not for patient-facing applications |
| < 0.70 | ğŸ“ˆ Developing | **Development** | Requires significant improvement before deployment |

**Generalizability Analysis** *(DEL3 Section 4.3)*

Models are evaluated across demographic and technical strata:

- ğŸ‘¤ **Demographics**: Age groups, sex, ethnicity
- ğŸ”¬ **Technical**: Scanner manufacturer, acquisition parameters
- ğŸ¥ **Clinical**: Disease stage, comorbidities, site

Sub-group performance gaps > 10% are flagged for fairness review.

**Ranking Rules**

1. Models ranked by **primary metric** (descending, higher = better)
2. Ties broken by secondary metrics in priority order
3. Each model's **best evaluation run** is used
4. Scores reported to 4 decimal places for precision
5. Statistical significance assessed via bootstrap CI (when available)

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | robustness_score | dropout_rAUC | line_noise_rAUC | noise_rAUC | perm_equivariance | shift_rAUC | shift_sensitivity |
|:---:|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| ğŸ¥‡ ğŸ”¶ | dummy_classifier | **0.7810** | 0.7760 | 0.7737 | 0.7867 | 0.7819 | 0.7874 | 0.7897 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: robustness_score (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

---
*Ranked by **robustness_score** (higher is better). Last updated from 5 evaluation(s).*

## ğŸ§¬ Genomics

### ğŸ§¬ Cell Type Annotation

ğŸ¯ **Task**: Classification | ğŸ¥ **Health Topic**: Single-cell Transcriptomics

*Predicting cell types from single-cell RNA-seq data.*

!!! info "Clinical Relevance"
    Automated characterization of immune cell populations.

#### ğŸ† Leaderboard

**All 1 models ranked by Accuracy:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **Geneformer** ğŸ‘‘ | 0.9100 | â­ Excellent | PBMC 68k | 2023-11-01 |


#### ğŸ“ Scoring Methodology

<details>
<summary>ğŸ” <strong>How are models scored? (ITU/WHO AI4H Aligned)</strong></summary>

!!! note "ITU/WHO FG-AI4H Alignment"
    This evaluation framework follows [ITU-T FG-AI4H](https://www.itu.int/pub/T-FG-AI4H) standards:

    - **DEL3**: Performance metrics per System Requirement Specifications (SyRS)
    - **DEL0.1**: Standardized terminology (AI Solution, Benchmarking Run)
    - **DEL10.x**: Topic Description Documents for health domains

**Primary Ranking Metric: `Accuracy`**

> Proportion of correct predictions (0.0-1.0)

**How is the primary metric chosen?** *(per DEL3 Section 6)*

For **classification/regression tasks**, we prioritize:
1. `AUROC` â€“ best for imbalanced medical data (DEL3 recommended)
2. `Accuracy` â€“ overall correctness rate
3. `F1-Score` â€“ precision-recall balance
4. `Sensitivity/Specificity` â€“ for diagnostic screening

**Score Interpretation** *(Clinical Deployment Readiness)*

| Range | Tier | DEL3 Deployment Level | Clinical Guidance |
|:---:|:---:|:---:|:---|
| â‰¥ 0.90 | â­ Excellent | **Production Ready** | Suitable for clinical decision support with monitoring |
| 0.80-0.89 | âœ… Good | **Pilot/Validation** | Promising; requires prospective validation study |
| 0.70-0.79 | ğŸ”¶ Fair | **Research Only** | Research use; not for patient-facing applications |
| < 0.70 | ğŸ“ˆ Developing | **Development** | Requires significant improvement before deployment |

**Generalizability Analysis** *(DEL3 Section 4.3)*

Models are evaluated across demographic and technical strata:

- ğŸ‘¤ **Demographics**: Age groups, sex, ethnicity
- ğŸ”¬ **Technical**: Scanner manufacturer, acquisition parameters
- ğŸ¥ **Clinical**: Disease stage, comorbidities, site

Sub-group performance gaps > 10% are flagged for fairness review.

**Ranking Rules**

1. Models ranked by **primary metric** (descending, higher = better)
2. Ties broken by secondary metrics in priority order
3. Each model's **best evaluation run** is used
4. Scores reported to 4 decimal places for precision
5. Statistical significance assessed via bootstrap CI (when available)

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | Accuracy | F1-Score |
|:---:|:---|:---:|:---:|
| ğŸ¥‡ â­ | Geneformer | **0.9100** | 0.8500 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: Accuracy (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

---
*Ranked by **Accuracy** (higher is better). Last updated from 1 evaluation(s).*

## ğŸ§  Neurology

### ğŸ§  Alzheimer's Disease Classification using Brain MRI

ğŸ¯ **Task**: Classification | ğŸ¥ **Health Topic**: Alzheimer's Disease

*Binary classification of AD vs CN using structural MRI data.*

!!! info "Clinical Relevance"
    Automated screening for AD to assist radiological workflow.

#### ğŸ† Leaderboard

**All 1 models ranked by AUROC:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **UNI** ğŸ‘‘ | 0.9200 | â­ Excellent | Alzheimer's Disease Neuroimaging Initiative (ADNI) | 2023-10-27 |


#### ğŸ“ Scoring Methodology

<details>
<summary>ğŸ” <strong>How are models scored? (ITU/WHO AI4H Aligned)</strong></summary>

!!! note "ITU/WHO FG-AI4H Alignment"
    This evaluation framework follows [ITU-T FG-AI4H](https://www.itu.int/pub/T-FG-AI4H) standards:

    - **DEL3**: Performance metrics per System Requirement Specifications (SyRS)
    - **DEL0.1**: Standardized terminology (AI Solution, Benchmarking Run)
    - **DEL10.x**: Topic Description Documents for health domains

**Primary Ranking Metric: `AUROC`**

> Area Under ROC Curve - measures discrimination ability (0.5 = random, 1.0 = perfect)

**How is the primary metric chosen?** *(per DEL3 Section 6)*

For **classification/regression tasks**, we prioritize:
1. `AUROC` â€“ best for imbalanced medical data (DEL3 recommended)
2. `Accuracy` â€“ overall correctness rate
3. `F1-Score` â€“ precision-recall balance
4. `Sensitivity/Specificity` â€“ for diagnostic screening

**Score Interpretation** *(Clinical Deployment Readiness)*

| Range | Tier | DEL3 Deployment Level | Clinical Guidance |
|:---:|:---:|:---:|:---|
| â‰¥ 0.90 | â­ Excellent | **Production Ready** | Suitable for clinical decision support with monitoring |
| 0.80-0.89 | âœ… Good | **Pilot/Validation** | Promising; requires prospective validation study |
| 0.70-0.79 | ğŸ”¶ Fair | **Research Only** | Research use; not for patient-facing applications |
| < 0.70 | ğŸ“ˆ Developing | **Development** | Requires significant improvement before deployment |

**Generalizability Analysis** *(DEL3 Section 4.3)*

Models are evaluated across demographic and technical strata:

- ğŸ‘¤ **Demographics**: Age groups, sex, ethnicity
- ğŸ”¬ **Technical**: Scanner manufacturer, acquisition parameters
- ğŸ¥ **Clinical**: Disease stage, comorbidities, site

Sub-group performance gaps > 10% are flagged for fairness review.

**Ranking Rules**

1. Models ranked by **primary metric** (descending, higher = better)
2. Ties broken by secondary metrics in priority order
3. Each model's **best evaluation run** is used
4. Scores reported to 4 decimal places for precision
5. Statistical significance assessed via bootstrap CI (when available)

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | AUROC | Accuracy |
|:---:|:---|:---:|:---:|
| ğŸ¥‡ â­ | UNI | **0.9200** | 0.8800 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: AUROC (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

---
*Ranked by **AUROC** (higher is better). Last updated from 1 evaluation(s).*

### ğŸ§  Brain Time-Series Modeling

ğŸ”„ **Task**: Reconstruction | ğŸ¥ **Health Topic**: Functional Brain Connectivity

*Evaluating ability to reconstruct masked fMRI voxel time-series.*

!!! info "Clinical Relevance"
    Foundation for understanding functional connectivity patterns.

#### ğŸ† Leaderboard

**All 1 models ranked by Correlation:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **BrainLM** ğŸ‘‘ | 0.7800 |  | UK Biobank fMRI tensors | 2025-11-15 |


#### ğŸ“ Scoring Methodology

<details>
<summary>ğŸ” <strong>How are models scored? (ITU/WHO AI4H Aligned)</strong></summary>

!!! note "ITU/WHO FG-AI4H Alignment"
    This evaluation framework follows [ITU-T FG-AI4H](https://www.itu.int/pub/T-FG-AI4H) standards:

    - **DEL3**: Performance metrics per System Requirement Specifications (SyRS)
    - **DEL0.1**: Standardized terminology (AI Solution, Benchmarking Run)
    - **DEL10.x**: Topic Description Documents for health domains

**Primary Ranking Metric: `Correlation`**

> Pearson correlation between predicted and actual values (-1 to 1)

**How is the primary metric chosen?** *(per DEL3 Section 6)*

For **classification/regression tasks**, we prioritize:
1. `AUROC` â€“ best for imbalanced medical data (DEL3 recommended)
2. `Accuracy` â€“ overall correctness rate
3. `F1-Score` â€“ precision-recall balance
4. `Sensitivity/Specificity` â€“ for diagnostic screening

**Score Interpretation** *(Clinical Deployment Readiness)*

| Range | Tier | DEL3 Deployment Level | Clinical Guidance |
|:---:|:---:|:---:|:---|
| â‰¥ 0.90 | â­ Excellent | **Production Ready** | Suitable for clinical decision support with monitoring |
| 0.80-0.89 | âœ… Good | **Pilot/Validation** | Promising; requires prospective validation study |
| 0.70-0.79 | ğŸ”¶ Fair | **Research Only** | Research use; not for patient-facing applications |
| < 0.70 | ğŸ“ˆ Developing | **Development** | Requires significant improvement before deployment |

**Generalizability Analysis** *(DEL3 Section 4.3)*

Models are evaluated across demographic and technical strata:

- ğŸ‘¤ **Demographics**: Age groups, sex, ethnicity
- ğŸ”¬ **Technical**: Scanner manufacturer, acquisition parameters
- ğŸ¥ **Clinical**: Disease stage, comorbidities, site

Sub-group performance gaps > 10% are flagged for fairness review.

**Ranking Rules**

1. Models ranked by **primary metric** (descending, higher = better)
2. Ties broken by secondary metrics in priority order
3. Each model's **best evaluation run** is used
4. Scores reported to 4 decimal places for precision
5. Statistical significance assessed via bootstrap CI (when available)

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | Correlation | MSE |
|:---:|:---|:---:|:---:|
| ğŸ¥‡  | BrainLM | **0.7800** | 0.4500 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: Correlation (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

---
*Ranked by **Correlation** (higher is better). Last updated from 1 evaluation(s).*

### ğŸ§  Toy Classification Benchmark

ğŸ¯ **Task**: Classification | ğŸ¥ **Health Topic**: N/A

*A toy benchmark for testing the pipeline.*

#### ğŸ† Leaderboard

**All 1 models ranked by AUROC:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **dummy_classifier** ğŸ‘‘ | 0.5597 | ğŸ“ˆ Developing | Toy fMRI Classification | 2025-11-27 |


#### ğŸ“ Scoring Methodology

<details>
<summary>ğŸ” <strong>How are models scored? (ITU/WHO AI4H Aligned)</strong></summary>

!!! note "ITU/WHO FG-AI4H Alignment"
    This evaluation framework follows [ITU-T FG-AI4H](https://www.itu.int/pub/T-FG-AI4H) standards:

    - **DEL3**: Performance metrics per System Requirement Specifications (SyRS)
    - **DEL0.1**: Standardized terminology (AI Solution, Benchmarking Run)
    - **DEL10.x**: Topic Description Documents for health domains

**Primary Ranking Metric: `AUROC`**

> Area Under ROC Curve - measures discrimination ability (0.5 = random, 1.0 = perfect)

**How is the primary metric chosen?** *(per DEL3 Section 6)*

For **classification/regression tasks**, we prioritize:
1. `AUROC` â€“ best for imbalanced medical data (DEL3 recommended)
2. `Accuracy` â€“ overall correctness rate
3. `F1-Score` â€“ precision-recall balance
4. `Sensitivity/Specificity` â€“ for diagnostic screening

**Score Interpretation** *(Clinical Deployment Readiness)*

| Range | Tier | DEL3 Deployment Level | Clinical Guidance |
|:---:|:---:|:---:|:---|
| â‰¥ 0.90 | â­ Excellent | **Production Ready** | Suitable for clinical decision support with monitoring |
| 0.80-0.89 | âœ… Good | **Pilot/Validation** | Promising; requires prospective validation study |
| 0.70-0.79 | ğŸ”¶ Fair | **Research Only** | Research use; not for patient-facing applications |
| < 0.70 | ğŸ“ˆ Developing | **Development** | Requires significant improvement before deployment |

**Generalizability Analysis** *(DEL3 Section 4.3)*

Models are evaluated across demographic and technical strata:

- ğŸ‘¤ **Demographics**: Age groups, sex, ethnicity
- ğŸ”¬ **Technical**: Scanner manufacturer, acquisition parameters
- ğŸ¥ **Clinical**: Disease stage, comorbidities, site

Sub-group performance gaps > 10% are flagged for fairness review.

**Ranking Rules**

1. Models ranked by **primary metric** (descending, higher = better)
2. Ties broken by secondary metrics in priority order
3. Each model's **best evaluation run** is used
4. Scores reported to 4 decimal places for precision
5. Statistical significance assessed via bootstrap CI (when available)

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | AUROC | Accuracy | F1-Score |
|:---:|:---|:---:|:---:|:---:|
| ğŸ¥‡ ğŸ“ˆ | dummy_classifier | **0.5597** | 0.5750 | 0.5732 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: AUROC (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

#### ğŸ“Š Granular Performance Breakdown

Expand sections below to see how models perform across different conditions:


<details>
<summary>ğŸ”¬ <strong>dummy_classifier</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ GE | 0.6373 | 0.6286 | 0.6274 | 70 |
| ğŸ¥ˆ Siemens | 0.5844 | 0.5789 | 0.5788 | 57 |
| Philips | 0.4662 | 0.5205 | 0.5147 | 73 |

</details>

<details>
<summary>ğŸ¥ <strong>dummy_classifier</strong> by Site</summary>

| Site | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ SiteC | 0.6348 | 0.5915 | 0.5912 | 71 |
| ğŸ¥ˆ SiteB | 0.6305 | 0.6316 | 0.6298 | 57 |
| SiteA | 0.4201 | 0.5139 | 0.5093 | 72 |

</details>

<details>
<summary>ğŸ©º <strong>dummy_classifier</strong> by Disease Stage</summary>

| Disease Stage | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ MCI | 0.6085 | 0.6000 | 0.5987 | 70 |
| ğŸ¥ˆ CN | 0.5559 | 0.5429 | 0.5414 | 70 |
| AD | 0.4955 | 0.5833 | 0.5804 | 60 |

</details>

<details>
<summary>ğŸ‘¤ <strong>dummy_classifier</strong> by Sex</summary>

| Sex | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ M | 0.6061 | 0.6111 | 0.6045 | 108 |
| F | 0.5021 | 0.5326 | 0.5326 | 92 |

</details>

<details>
<summary>ğŸ“… <strong>dummy_classifier</strong> by Age Group</summary>

| Age Group | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ age_80-100 | 0.6000 | 0.5455 | 0.5299 | 11 |
| ğŸ¥ˆ age_60-80 | 0.5943 | 0.5857 | 0.5788 | 70 |
| ğŸ¥‰ age_20-40 | 0.5819 | 0.5741 | 0.5668 | 54 |
| age_40-60 | 0.4810 | 0.5692 | 0.5513 | 65 |

</details>

---
*Ranked by **AUROC** (higher is better). Last updated from 5 evaluation(s).*

### ğŸ§  fMRI Foundation Model Benchmark (Granular)

ğŸ“‹ **Task**: Classification/Reconstruction | ğŸ¥ **Health Topic**: Functional Brain Imaging Analysis

!!! info "Clinical Relevance"
    Foundation models for fMRI must generalize across diverse acquisition  parameters, scanner manufacturers, and preprocessing pipelines. This benchmark provides granular rankings to identify optimal model-data matches.


#### ğŸ† Leaderboard

```
         ğŸ¥‡          
     [BrainLM]    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
 ğŸ¥ˆ â”‚         â”‚ ğŸ¥‰  
[BrainBERT]â”‚         â”‚[NeuroCLIP]
â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€
```

**All 3 models ranked by AUROC:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **BrainLM** ğŸ‘‘ | 0.9100 | â­ Excellent | hcp_1200 | 2024-01-15 |
| ğŸ¥ˆ | **BrainBERT** ğŸŒŸ | 0.8700 | âœ… Good | hcp_1200 | 2024-01-10 |
| ğŸ¥‰ | **NeuroCLIP** âœ¨ | 0.8300 | âœ… Good | hcp_1200 | 2024-01-05 |


#### ğŸ“– Ranking Explanation

!!! abstract "Why These Rankings?"
    **ğŸ¥‡ BrainLM** leads with AUROC=0.9100

    - Gap to ğŸ¥ˆ **BrainBERT**: +0.0400 (4.6% better)
    - Score range across all models: 0.0800
    - Performance distribution: â­ 1 excellent, âœ… 2 good


#### ğŸ“ Scoring Methodology

<details>
<summary>ğŸ” <strong>How are models scored? (ITU/WHO AI4H Aligned)</strong></summary>

!!! note "ITU/WHO FG-AI4H Alignment"
    This evaluation framework follows [ITU-T FG-AI4H](https://www.itu.int/pub/T-FG-AI4H) standards:

    - **DEL3**: Performance metrics per System Requirement Specifications (SyRS)
    - **DEL0.1**: Standardized terminology (AI Solution, Benchmarking Run)
    - **DEL10.x**: Topic Description Documents for health domains

**Primary Ranking Metric: `AUROC`**

> Area Under ROC Curve - measures discrimination ability (0.5 = random, 1.0 = perfect)

**How is the primary metric chosen?** *(per DEL3 Section 6)*

For **classification/regression tasks**, we prioritize:
1. `AUROC` â€“ best for imbalanced medical data (DEL3 recommended)
2. `Accuracy` â€“ overall correctness rate
3. `F1-Score` â€“ precision-recall balance
4. `Sensitivity/Specificity` â€“ for diagnostic screening

**Score Interpretation** *(Clinical Deployment Readiness)*

| Range | Tier | DEL3 Deployment Level | Clinical Guidance |
|:---:|:---:|:---:|:---|
| â‰¥ 0.90 | â­ Excellent | **Production Ready** | Suitable for clinical decision support with monitoring |
| 0.80-0.89 | âœ… Good | **Pilot/Validation** | Promising; requires prospective validation study |
| 0.70-0.79 | ğŸ”¶ Fair | **Research Only** | Research use; not for patient-facing applications |
| < 0.70 | ğŸ“ˆ Developing | **Development** | Requires significant improvement before deployment |

**Generalizability Analysis** *(DEL3 Section 4.3)*

Models are evaluated across demographic and technical strata:

- ğŸ‘¤ **Demographics**: Age groups, sex, ethnicity
- ğŸ”¬ **Technical**: Scanner manufacturer, acquisition parameters
- ğŸ¥ **Clinical**: Disease stage, comorbidities, site

Sub-group performance gaps > 10% are flagged for fairness review.

**Ranking Rules**

1. Models ranked by **primary metric** (descending, higher = better)
2. Ties broken by secondary metrics in priority order
3. Each model's **best evaluation run** is used
4. Scores reported to 4 decimal places for precision
5. Statistical significance assessed via bootstrap CI (when available)

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | AUROC | Accuracy | F1-Score | Correlation | MSE |
|:---:|:---|:---:|:---:|:---:|:---:|:---:|
| ğŸ¥‡ â­ | BrainLM | **0.9100** | 0.8700 | 0.8600 | 0.8100 | 0.4200 |
| ğŸ¥ˆ âœ… | BrainBERT | **0.8700** | 0.8200 | 0.8100 | 0.7600 | 0.5100 |
| ğŸ¥‰ âœ… | NeuroCLIP | **0.8300** | 0.7900 | 0.7800 | 0.7200 | 0.5800 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: AUROC (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

#### ğŸ“Š Granular Performance Breakdown

Expand sections below to see how models perform across different conditions:


<details>
<summary>ğŸ”¬ <strong>BrainLM</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ Siemens | 0.9300 | 0.8900 | 0.8800 | 450 |
| ğŸ¥ˆ Philips | 0.9000 | 0.8600 | 0.8500 | 370 |
| GE | 0.8800 | 0.8400 | 0.8300 | 380 |

</details>

<details>
<summary>ğŸ¥ <strong>BrainLM</strong> by Site</summary>

| Site | AUROC | Accuracy | N |
|---|---|---|---|
| ğŸ¥‡ WashU | 0.9300 | 0.8900 | 220 |
| ğŸ¥ˆ MGH | 0.9200 | 0.8800 | 200 |
| ğŸ¥‰ Oxford | 0.9100 | 0.8700 | 200 |
| UCLA | 0.9000 | 0.8600 | 180 |
| UMinn | 0.8900 | 0.8500 | 200 |

</details>

<details>
<summary>ğŸ“¡ <strong>BrainLM</strong> by Acquisition Type</summary>

| Acquisition Type | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ resting_state | 0.9200 | 0.8800 | 0.8700 | 600 |
| ğŸ¥ˆ language | 0.9100 | 0.8700 | - | 100 |
| ğŸ¥‰ working_memory | 0.9000 | 0.8600 | - | 150 |
| task_based | 0.8900 | 0.8500 | 0.8400 | 400 |
| motor | 0.8800 | 0.8400 | - | 150 |

</details>

<details>
<summary>âš™ï¸ <strong>BrainLM</strong> by Preprocessing</summary>

| Preprocessing | AUROC | Accuracy | N |
|---|---|---|---|
| ğŸ¥‡ fmriprep | 0.9200 | 0.8800 | 500 |
| ğŸ¥ˆ hcp | 0.9100 | 0.8700 | 400 |
| minimal | 0.8500 | 0.8100 | 300 |

</details>

<details>
<summary>ğŸ§² <strong>BrainLM</strong> by Field Strength</summary>

| Field Strength | AUROC | Accuracy | N |
|---|---|---|---|
| ğŸ¥‡ 7T | 0.9400 | 0.9100 | 100 |
| 3T | 0.9100 | 0.8700 | 900 |

</details>

<details>
<summary>ğŸ”¬ <strong>BrainBERT</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ Siemens | 0.8900 | 0.8400 | 0.8300 | 450 |
| ğŸ¥ˆ GE | 0.8600 | 0.8100 | 0.8000 | 380 |
| Philips | 0.8500 | 0.8000 | 0.7900 | 370 |

</details>

<details>
<summary>ğŸ“¡ <strong>BrainBERT</strong> by Acquisition Type</summary>

| Acquisition Type | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ resting_state | 0.8800 | 0.8300 | 0.8200 | 600 |
| task_based | 0.8500 | 0.8000 | 0.7900 | 400 |

</details>

<details>
<summary>ğŸ”¬ <strong>NeuroCLIP</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ Siemens | 0.8500 | 0.8100 | 0.8000 | 450 |
| ğŸ¥ˆ GE | 0.8200 | 0.7800 | 0.7700 | 380 |
| Philips | 0.8100 | 0.7700 | 0.7600 | 370 |

</details>

---
*Ranked by **AUROC** (higher is better). Last updated from 3 evaluation(s).*

---

## ğŸš€ Get Your Model on the Leaderboard

Want to see your Foundation Model ranked here?

1. ğŸ“¥ **Download** the benchmark suite and run locally
2. ğŸ§ª **Evaluate** your model: `python -m fmbench run --help`
3. ğŸ“¤ **Submit** your results via [GitHub Issue](https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub/issues/new?template=benchmark_submission.md)

ğŸ’¡ **Propose new evaluation protocols** via [Issue](https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub/issues/new?template=protocol_proposal.md)

!!! note "Curated Benchmark Hub"
    All submissions are reviewed before being added. See [Submission Guide](../contributing/submission_guide.md) for details.

*Aligned with [ITU/WHO FG-AI4H](https://www.itu.int/pub/T-FG-AI4H) standards for healthcare AI evaluation.*
