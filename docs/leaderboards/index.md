# ğŸ† Foundation Model Leaderboards

!!! success "Benchmark Hub Stats"
    ğŸ¯ **7** Benchmarks | ğŸ¤– **21** Models Evaluated | ğŸ“Š **38** Total Evaluations

Welcome to the **AI4H-Inspired FM Benchmark Hub**! Rankings below show **all submitted models** from best to developing, helping you find the right model for your use case.

## ğŸ§­ Quick Navigation

- [ğŸŒ Cross-Domain](#cross-domain)
- [ğŸ§¬ Genomics](#genomics)
- [ğŸ§  Neurology](#neurology)

---

## ğŸŒ Cross-Domain

### ğŸŒ Clinical Report Generation Quality

âœï¸ **Task**: Generation | ğŸ¥ **Health Topic**: Automated Clinical Reporting

!!! info "Clinical Relevance"
    Foundation models increasingly generate clinical reports, radiology  interpretations, and patient summaries. Quality metrics must capture both linguistic fluency and clinical accuracy/safety.


#### ğŸ† Leaderboard

```
         ğŸ¥‡          
     [Me-LLaMA]    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
 ğŸ¥ˆ â”‚         â”‚ ğŸ¥‰  
[M3FM]â”‚         â”‚[OpenFlamin]
â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€
```

**All 6 models ranked by report_quality_score:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **Me-LLaMA** ğŸ‘‘ | 0.8750 |  | mimic_cxr_reports | 2024-02-05 |
| ğŸ¥ˆ | **M3FM** ğŸŒŸ | 0.8600 |  | mimic_cxr_reports | 2024-01-28 |
| ğŸ¥‰ | **OpenFlamingo** âœ¨ | 0.8400 |  | mimic_cxr_reports | 2024-01-20 |
| ğŸ… | TITAN | 0.8100 |  | mimic_cxr_reports | 2024-01-25 |
| ğŸ… | Med-Flamingo | 0.7800 |  | mimic_cxr_reports | 2024-01-18 |
| ğŸ–ï¸ | RadBERT | 0.6900 |  | mimic_cxr_reports | 2024-01-12 |


#### ğŸ“– Ranking Explanation

!!! abstract "Why These Rankings?"
    **ğŸ¥‡ Me-LLaMA** leads with report_quality_score=0.8750

    - Gap to ğŸ¥ˆ **M3FM**: +0.0150 (1.7% better)
    - Score range across all models: 0.1850


<details>
<summary>ğŸ“ <strong>How are models scored?</strong></summary>

---

### ğŸ¯ Primary Metric: `report_quality_score`

> Composite score of linguistic fluency + clinical accuracy (0.0-1.0)

---

### ğŸ“Š Metric Priority

| Priority | Metric | What it measures |
|:---:|:---|:---|
| 1 | `report_quality_score` | Composite clinical + linguistic quality |
| 2 | `clinical_accuracy` | Correctness of medical content |
| 3 | `bertscore` | Semantic similarity |
| 4 | `hallucination_rate` | Safety (lower = better) |

---

### ğŸ¥ Clinical Readiness Tiers

| Score | Tier | Deployment | Guidance |
|:---:|:---:|:---:|:---|
| â‰¥ 0.90 | â­ Excellent | Production | Clinical decision support ready |
| 0.80-0.89 | âœ… Good | Pilot | Needs prospective validation |
| 0.70-0.79 | ğŸ”¶ Fair | Research | Not for patient-facing use |
| < 0.70 | ğŸ“ˆ Developing | Development | Significant improvement needed |

---

### ğŸ“ Ranking Rules

1. Ranked by **primary metric** (higher = better)
2. Ties broken by secondary metrics
3. Best run per model used
4. 4 decimal precision

---

### âš–ï¸ Fairness Analysis

Models evaluated across:

| Category | Strata |
|:---|:---|
| ğŸ‘¤ Demographics | Age, sex, ethnicity |
| ğŸ”¬ Technical | Scanner, acquisition |
| ğŸ¥ Clinical | Disease stage, site |

> âš ï¸ Gaps >10% flagged for review

---

*Aligned with [ITU/WHO AI4H DEL3](https://www.itu.int/pub/T-FG-AI4H) standards.*

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | report_quality_score | clinical_accuracy | bertscore | bleu | finding_recall | hallucination_rate | finding_precision | flesch_kincaid |
|:---:|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| ğŸ¥‡  | Me-LLaMA | **0.8750** | 0.9200 | 0.9000 | 43.5000 | 0.8800 | 0.0400 | 0.9300 | 9.5000 |
| ğŸ¥ˆ  | M3FM | **0.8600** | 0.9100 | 0.8900 | 41.2000 | 0.8700 | 0.0450 | 0.9200 | 9.8000 |
| ğŸ¥‰  | OpenFlamingo | **0.8400** | 0.8900 | 0.8700 | 38.5000 | 0.8500 | 0.0600 | 0.9100 | 10.2000 |
| ğŸ…  | TITAN | **0.8100** | 0.8600 | 0.8500 | 35.2000 | 0.8200 | 0.0700 | 0.8800 | 10.8000 |
| ğŸ…  | Med-Flamingo | **0.7800** | 0.8200 | 0.8200 | 32.5000 | 0.7900 | 0.0900 | 0.8500 | 11.5000 |
| ğŸ–ï¸  | RadBERT | **0.6900** | 0.7200 | 0.7400 | 24.2000 | 0.6800 | 0.1500 | 0.7500 | 13.2000 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: report_quality_score (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

#### ğŸ“Š Granular Performance Breakdown

Expand sections below to see how models perform across different conditions:


<details>
<summary>ğŸ“„ <strong>OpenFlamingo</strong> by Report Type</summary>

| Report Type | clinical_accuracy | finding_recall | bertscore | N |
|---|---|---|---|---|
| ğŸ¥‡ chest_xray | 0.9100 | 0.8700 | 0.8800 | 2000 |
| ğŸ¥ˆ brain_mri | 0.8800 | 0.8400 | 0.8600 | 600 |
| ct_abdomen | 0.8600 | 0.8200 | 0.8500 | 800 |

</details>

<details>
<summary>ğŸ“Š <strong>OpenFlamingo</strong> by Complexity</summary>

| Complexity | clinical_accuracy | hallucination_rate | N |
|---|---|---|---|
| ğŸ¥‡ simple | 0.9400 | 0.0300 | 1500 |
| ğŸ¥ˆ moderate | 0.8800 | 0.0600 | 1200 |
| complex | 0.8200 | 0.1000 | 700 |

</details>

<details>
<summary>ğŸ“„ <strong>Med-Flamingo</strong> by Report Type</summary>

| Report Type | clinical_accuracy | finding_recall | bertscore | N |
|---|---|---|---|---|
| ğŸ¥‡ chest_xray | 0.8500 | 0.8100 | 0.8300 | 2000 |
| ğŸ¥ˆ brain_mri | 0.8000 | 0.7600 | 0.8000 | 600 |
| ct_abdomen | 0.7800 | 0.7400 | 0.7900 | 800 |

</details>

---
*Ranked by **report_quality_score** (higher is better). Last updated from 6 evaluation(s).*

### ğŸŒ Foundation Model Robustness Evaluation

ğŸ›¡ï¸ **Task**: Robustness Assessment | ğŸ¥ **Health Topic**: Model Reliability and Artifact Resilience

!!! info "Clinical Relevance"
    Clinical deployment of AI models requires robustness to real-world data variability including sensor noise, signal artifacts, and acquisition differences. This benchmark evaluates model stability under controlled perturbations that simulate common data quality issues.


#### ğŸ† Leaderboard

```
         ğŸ¥‡          
     [geneformer]    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
 ğŸ¥ˆ â”‚         â”‚ ğŸ¥‰  
[Brain-JEPA]â”‚         â”‚[BrainHarmo]
â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€
```

**All 7 models ranked by robustness_score:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **geneformer** ğŸ‘‘ | 0.9995 | â­ Excellent | - | 2025-11-27 |
| ğŸ¥ˆ | **Brain-JEPA** ğŸŒŸ | 0.8650 | âœ… Good | DS-TOY-NEURO-ROBUSTNESS | 2024-01-20 |
| ğŸ¥‰ | **BrainHarmony** âœ¨ | 0.8450 | âœ… Good | DS-TOY-NEURO-ROBUSTNESS | 2024-01-18 |
| ğŸ… | Geneformer | 0.8350 | âœ… Good | DS-TOY-GENOMICS | 2024-01-10 |
| ğŸ… | BrainLM | 0.8250 | âœ… Good | DS-TOY-NEURO-ROBUSTNESS | 2024-01-16 |
| ğŸ–ï¸ | HyenaDNA | 0.7950 | ğŸ”¶ Fair | DS-TOY-GENOMICS | 2024-01-12 |
| ğŸ–ï¸ | Baseline (Random/Majority) | 0.7810 | ğŸ”¶ Fair | - | 2025-11-27 |


#### ğŸ“– Ranking Explanation

!!! abstract "Why These Rankings?"
    **ğŸ¥‡ geneformer** leads with robustness_score=0.9995

    - Gap to ğŸ¥ˆ **Brain-JEPA**: +0.1345 (15.5% better)
    - Score range across all models: 0.2185
    - Performance distribution: â­ 1 excellent, âœ… 4 good, ğŸ”¶ 2 fair


<details>
<summary>ğŸ“ <strong>How are models scored?</strong></summary>

---

### ğŸ¯ Primary Metric: `robustness_score`

> Average performance retention under data perturbations (0.0-1.0)

---

### ğŸ“Š Metric Priority

| Priority | Metric | What it measures |
|:---:|:---|:---|
| 1 | `robustness_score` | Overall perturbation resilience |
| 2 | `dropout_rAUC` | Performance under missing data |
| 3 | `noise_rAUC` | Performance under noise |
| 4 | `perm_equivariance` | Input reordering consistency |

---

### ğŸ¥ Clinical Readiness Tiers

| Score | Tier | Deployment | Guidance |
|:---:|:---:|:---:|:---|
| â‰¥ 0.90 | â­ Excellent | Production | Clinical decision support ready |
| 0.80-0.89 | âœ… Good | Pilot | Needs prospective validation |
| 0.70-0.79 | ğŸ”¶ Fair | Research | Not for patient-facing use |
| < 0.70 | ğŸ“ˆ Developing | Development | Significant improvement needed |

---

### ğŸ“ Ranking Rules

1. Ranked by **primary metric** (higher = better)
2. Ties broken by secondary metrics
3. Best run per model used
4. 4 decimal precision

---

### âš–ï¸ Fairness Analysis

Models evaluated across:

| Category | Strata |
|:---|:---|
| ğŸ‘¤ Demographics | Age, sex, ethnicity |
| ğŸ”¬ Technical | Scanner, acquisition |
| ğŸ¥ Clinical | Disease stage, site |

> âš ï¸ Gaps >10% flagged for review

---

*Aligned with [ITU/WHO AI4H DEL3](https://www.itu.int/pub/T-FG-AI4H) standards.*

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | robustness_score | dropout_rAUC | expression_rAUC | line_noise_rAUC | masking_rAUC | noise_rAUC | perm_equivariance | shift_rAUC |
|:---:|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| ğŸ¥‡ â­ | geneformer | **0.9995** | 0.9995 | - | 0.9995 | - | 0.9995 | 0.9995 | 0.9995 |
| ğŸ¥ˆ âœ… | Brain-JEPA | **0.8650** | 0.8800 | - | 0.8400 | - | 0.8500 | 0.8900 | 0.8650 |
| ğŸ¥‰ âœ… | BrainHarmony | **0.8450** | 0.8600 | - | 0.8200 | - | 0.8300 | 0.8700 | 0.8450 |
| ğŸ… âœ… | Geneformer | **0.8350** | 0.8500 | 0.8100 | - | 0.8600 | 0.8200 | 0.8350 | - |
| ğŸ… âœ… | BrainLM | **0.8250** | 0.8400 | - | 0.8000 | - | 0.8100 | 0.8500 | 0.8250 |
| ğŸ–ï¸ ğŸ”¶ | HyenaDNA | **0.7950** | 0.8100 | 0.7700 | - | 0.8200 | 0.7800 | 0.8000 | - |
| ğŸ–ï¸ ğŸ”¶ | Baseline (Random/Majority) | **0.7810** | 0.7760 | - | 0.7737 | - | 0.7867 | 0.7819 | 0.7874 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: robustness_score (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

---
*Ranked by **robustness_score** (higher is better). Last updated from 11 evaluation(s).*

## ğŸ§¬ Genomics

### ğŸ§¬ Cell Type Annotation

ğŸ¯ **Task**: Classification | ğŸ¥ **Health Topic**: Single-cell Transcriptomics

*Predicting cell types from single-cell RNA-seq data.*

!!! info "Clinical Relevance"
    Automated characterization of immune cell populations.

#### ğŸ† Leaderboard

```
         ğŸ¥‡          
     [Evo 2]    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
 ğŸ¥ˆ â”‚         â”‚ ğŸ¥‰  
[Geneformer]â”‚         â”‚[SWIFT]
â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€
```

**All 6 models ranked by Accuracy:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **Evo 2** ğŸ‘‘ | 0.9250 | â­ Excellent | PBMC 68k | 2024-02-01 |
| ğŸ¥ˆ | **Geneformer** ğŸŒŸ | 0.9100 | â­ Excellent | PBMC 68k | 2023-11-01 |
| ğŸ¥‰ | **SWIFT** âœ¨ | 0.8950 | âœ… Good | PBMC 68k | 2024-01-15 |
| ğŸ… | Caduceus | 0.8850 | âœ… Good | PBMC 68k | 2024-01-12 |
| ğŸ… | HyenaDNA | 0.8700 | âœ… Good | PBMC 68k | 2024-01-08 |
| ğŸ–ï¸ | DNABERT-2 | 0.8500 | âœ… Good | PBMC 68k | 2024-01-05 |


#### ğŸ“– Ranking Explanation

!!! abstract "Why These Rankings?"
    **ğŸ¥‡ Evo 2** leads with Accuracy=0.9250

    - Gap to ğŸ¥ˆ **Geneformer**: +0.0150 (1.6% better)
    - Score range across all models: 0.0750
    - Performance distribution: â­ 2 excellent, âœ… 4 good


<details>
<summary>ğŸ“ <strong>How are models scored?</strong></summary>

---

### ğŸ¯ Primary Metric: `Accuracy`

> Proportion of correct predictions (0.0-1.0)

---

### ğŸ“Š Metric Priority

| Priority | Metric | What it measures |
|:---:|:---|:---|
| 1 | `AUROC` | Discrimination (best for imbalanced data) |
| 2 | `Accuracy` | Overall correctness |
| 3 | `F1-Score` | Precision-recall balance |
| 4 | `Sensitivity` | True positive rate |

---

### ğŸ¥ Clinical Readiness Tiers

| Score | Tier | Deployment | Guidance |
|:---:|:---:|:---:|:---|
| â‰¥ 0.90 | â­ Excellent | Production | Clinical decision support ready |
| 0.80-0.89 | âœ… Good | Pilot | Needs prospective validation |
| 0.70-0.79 | ğŸ”¶ Fair | Research | Not for patient-facing use |
| < 0.70 | ğŸ“ˆ Developing | Development | Significant improvement needed |

---

### ğŸ“ Ranking Rules

1. Ranked by **primary metric** (higher = better)
2. Ties broken by secondary metrics
3. Best run per model used
4. 4 decimal precision

---

### âš–ï¸ Fairness Analysis

Models evaluated across:

| Category | Strata |
|:---|:---|
| ğŸ‘¤ Demographics | Age, sex, ethnicity |
| ğŸ”¬ Technical | Scanner, acquisition |
| ğŸ¥ Clinical | Disease stage, site |

> âš ï¸ Gaps >10% flagged for review

---

*Aligned with [ITU/WHO AI4H DEL3](https://www.itu.int/pub/T-FG-AI4H) standards.*

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | Accuracy | F1-Score |
|:---:|:---|:---:|:---:|
| ğŸ¥‡ â­ | Evo 2 | **0.9250** | 0.8900 |
| ğŸ¥ˆ â­ | Geneformer | **0.9100** | 0.8500 |
| ğŸ¥‰ âœ… | SWIFT | **0.8950** | 0.8550 |
| ğŸ… âœ… | Caduceus | **0.8850** | 0.8400 |
| ğŸ… âœ… | HyenaDNA | **0.8700** | 0.8200 |
| ğŸ–ï¸ âœ… | DNABERT-2 | **0.8500** | 0.8000 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: Accuracy (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

---
*Ranked by **Accuracy** (higher is better). Last updated from 6 evaluation(s).*

## ğŸ§  Neurology

### ğŸ§  Alzheimer's Disease Classification using Brain MRI

ğŸ¯ **Task**: Classification | ğŸ¥ **Health Topic**: Alzheimer's Disease

*Binary classification of AD vs CN using structural MRI data.*

!!! info "Clinical Relevance"
    Automated screening for AD to assist radiological workflow.

#### ğŸ† Leaderboard

```
         ğŸ¥‡          
     [Brain-JEPA]    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
 ğŸ¥ˆ â”‚         â”‚ ğŸ¥‰  
[UNI]â”‚         â”‚[BrainLM]
â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€
```

**All 3 models ranked by AUROC:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **Brain-JEPA** ğŸ‘‘ | 0.9350 | â­ Excellent | ADNI | 2024-01-20 |
| ğŸ¥ˆ | **UNI** ğŸŒŸ | 0.9200 | â­ Excellent | Alzheimer's Disease Neuroimaging Initiative (ADNI) | 2023-10-27 |
| ğŸ¥‰ | **BrainLM** âœ¨ | 0.9100 | â­ Excellent | ADNI | 2024-01-15 |


#### ğŸ“– Ranking Explanation

!!! abstract "Why These Rankings?"
    **ğŸ¥‡ Brain-JEPA** leads with AUROC=0.9350

    - Gap to ğŸ¥ˆ **UNI**: +0.0150 (1.6% better)
    - Score range across all models: 0.0250
    - Performance distribution: â­ 3 excellent


<details>
<summary>ğŸ“ <strong>How are models scored?</strong></summary>

---

### ğŸ¯ Primary Metric: `AUROC`

> Area Under ROC Curve - measures discrimination ability (0.5 = random, 1.0 = perfect)

---

### ğŸ“Š Metric Priority

| Priority | Metric | What it measures |
|:---:|:---|:---|
| 1 | `AUROC` | Discrimination (best for imbalanced data) |
| 2 | `Accuracy` | Overall correctness |
| 3 | `F1-Score` | Precision-recall balance |
| 4 | `Sensitivity` | True positive rate |

---

### ğŸ¥ Clinical Readiness Tiers

| Score | Tier | Deployment | Guidance |
|:---:|:---:|:---:|:---|
| â‰¥ 0.90 | â­ Excellent | Production | Clinical decision support ready |
| 0.80-0.89 | âœ… Good | Pilot | Needs prospective validation |
| 0.70-0.79 | ğŸ”¶ Fair | Research | Not for patient-facing use |
| < 0.70 | ğŸ“ˆ Developing | Development | Significant improvement needed |

---

### ğŸ“ Ranking Rules

1. Ranked by **primary metric** (higher = better)
2. Ties broken by secondary metrics
3. Best run per model used
4. 4 decimal precision

---

### âš–ï¸ Fairness Analysis

Models evaluated across:

| Category | Strata |
|:---|:---|
| ğŸ‘¤ Demographics | Age, sex, ethnicity |
| ğŸ”¬ Technical | Scanner, acquisition |
| ğŸ¥ Clinical | Disease stage, site |

> âš ï¸ Gaps >10% flagged for review

---

*Aligned with [ITU/WHO AI4H DEL3](https://www.itu.int/pub/T-FG-AI4H) standards.*

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | AUROC | Accuracy |
|:---:|:---|:---:|:---:|
| ğŸ¥‡ â­ | Brain-JEPA | **0.9350** | 0.8950 |
| ğŸ¥ˆ â­ | UNI | **0.9200** | 0.8800 |
| ğŸ¥‰ â­ | BrainLM | **0.9100** | 0.8700 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: AUROC (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

---
*Ranked by **AUROC** (higher is better). Last updated from 3 evaluation(s).*

### ğŸ§  Brain Time-Series Modeling

ğŸ”„ **Task**: Reconstruction | ğŸ¥ **Health Topic**: Functional Brain Connectivity

*Evaluating ability to reconstruct masked fMRI voxel time-series.*

!!! info "Clinical Relevance"
    Foundation for understanding functional connectivity patterns.

#### ğŸ† Leaderboard

**All 1 models ranked by Correlation:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **BrainLM** ğŸ‘‘ | 0.7800 |  | UK Biobank fMRI tensors | 2025-11-15 |


<details>
<summary>ğŸ“ <strong>How are models scored?</strong></summary>

---

### ğŸ¯ Primary Metric: `Correlation`

> Pearson correlation between predicted and actual values (-1 to 1)

---

### ğŸ“Š Metric Priority

| Priority | Metric | What it measures |
|:---:|:---|:---|
| 1 | `AUROC` | Discrimination (best for imbalanced data) |
| 2 | `Accuracy` | Overall correctness |
| 3 | `F1-Score` | Precision-recall balance |
| 4 | `Sensitivity` | True positive rate |

---

### ğŸ¥ Clinical Readiness Tiers

| Score | Tier | Deployment | Guidance |
|:---:|:---:|:---:|:---|
| â‰¥ 0.90 | â­ Excellent | Production | Clinical decision support ready |
| 0.80-0.89 | âœ… Good | Pilot | Needs prospective validation |
| 0.70-0.79 | ğŸ”¶ Fair | Research | Not for patient-facing use |
| < 0.70 | ğŸ“ˆ Developing | Development | Significant improvement needed |

---

### ğŸ“ Ranking Rules

1. Ranked by **primary metric** (higher = better)
2. Ties broken by secondary metrics
3. Best run per model used
4. 4 decimal precision

---

### âš–ï¸ Fairness Analysis

Models evaluated across:

| Category | Strata |
|:---|:---|
| ğŸ‘¤ Demographics | Age, sex, ethnicity |
| ğŸ”¬ Technical | Scanner, acquisition |
| ğŸ¥ Clinical | Disease stage, site |

> âš ï¸ Gaps >10% flagged for review

---

*Aligned with [ITU/WHO AI4H DEL3](https://www.itu.int/pub/T-FG-AI4H) standards.*

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | Correlation | MSE |
|:---:|:---|:---:|:---:|
| ğŸ¥‡  | BrainLM | **0.7800** | 0.4500 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: Correlation (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

---
*Ranked by **Correlation** (higher is better). Last updated from 1 evaluation(s).*

### ğŸ§  Toy Classification Benchmark

ğŸ¯ **Task**: Classification | ğŸ¥ **Health Topic**: N/A

*A toy benchmark for testing the pipeline.*

#### ğŸ† Leaderboard

**All 2 models ranked by AUROC:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **Baseline (Random/Majority)** ğŸ‘‘ | 0.5597 | ğŸ“ˆ Developing | Toy fMRI Classification | 2025-11-27 |
| ğŸ¥ˆ | **BrainLM** ğŸŒŸ | 0.5193 | ğŸ“ˆ Developing | Toy fMRI Classification | 2025-11-27 |


#### ğŸ“– Ranking Explanation

!!! abstract "Why These Rankings?"
    **ğŸ¥‡ Baseline (Random/Majority)** leads with AUROC=0.5597

    - Gap to ğŸ¥ˆ **BrainLM**: +0.0404 (7.8% better)
    - Performance distribution: ğŸ“ˆ 2 developing


<details>
<summary>ğŸ“ <strong>How are models scored?</strong></summary>

---

### ğŸ¯ Primary Metric: `AUROC`

> Area Under ROC Curve - measures discrimination ability (0.5 = random, 1.0 = perfect)

---

### ğŸ“Š Metric Priority

| Priority | Metric | What it measures |
|:---:|:---|:---|
| 1 | `AUROC` | Discrimination (best for imbalanced data) |
| 2 | `Accuracy` | Overall correctness |
| 3 | `F1-Score` | Precision-recall balance |
| 4 | `Sensitivity` | True positive rate |

---

### ğŸ¥ Clinical Readiness Tiers

| Score | Tier | Deployment | Guidance |
|:---:|:---:|:---:|:---|
| â‰¥ 0.90 | â­ Excellent | Production | Clinical decision support ready |
| 0.80-0.89 | âœ… Good | Pilot | Needs prospective validation |
| 0.70-0.79 | ğŸ”¶ Fair | Research | Not for patient-facing use |
| < 0.70 | ğŸ“ˆ Developing | Development | Significant improvement needed |

---

### ğŸ“ Ranking Rules

1. Ranked by **primary metric** (higher = better)
2. Ties broken by secondary metrics
3. Best run per model used
4. 4 decimal precision

---

### âš–ï¸ Fairness Analysis

Models evaluated across:

| Category | Strata |
|:---|:---|
| ğŸ‘¤ Demographics | Age, sex, ethnicity |
| ğŸ”¬ Technical | Scanner, acquisition |
| ğŸ¥ Clinical | Disease stage, site |

> âš ï¸ Gaps >10% flagged for review

---

*Aligned with [ITU/WHO AI4H DEL3](https://www.itu.int/pub/T-FG-AI4H) standards.*

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | AUROC | Accuracy | F1-Score |
|:---:|:---|:---:|:---:|:---:|
| ğŸ¥‡ ğŸ“ˆ | Baseline (Random/Majority) | **0.5597** | 0.5750 | 0.5732 |
| ğŸ¥ˆ ğŸ“ˆ | BrainLM | **0.5193** | 0.5100 | 0.5100 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: AUROC (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

#### ğŸ“Š Granular Performance Breakdown

Expand sections below to see how models perform across different conditions:


<details>
<summary>ğŸ”¬ <strong>Baseline (Random/Majority)</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ GE | 0.6373 | 0.6286 | 0.6274 | 70 |
| ğŸ¥ˆ Siemens | 0.5844 | 0.5789 | 0.5788 | 57 |
| Philips | 0.4662 | 0.5205 | 0.5147 | 73 |

</details>

<details>
<summary>ğŸ¥ <strong>Baseline (Random/Majority)</strong> by Site</summary>

| Site | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ SiteC | 0.6348 | 0.5915 | 0.5912 | 71 |
| ğŸ¥ˆ SiteB | 0.6305 | 0.6316 | 0.6298 | 57 |
| SiteA | 0.4201 | 0.5139 | 0.5093 | 72 |

</details>

<details>
<summary>ğŸ©º <strong>Baseline (Random/Majority)</strong> by Disease Stage</summary>

| Disease Stage | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ MCI | 0.6085 | 0.6000 | 0.5987 | 70 |
| ğŸ¥ˆ CN | 0.5559 | 0.5429 | 0.5414 | 70 |
| AD | 0.4955 | 0.5833 | 0.5804 | 60 |

</details>

<details>
<summary>ğŸ‘¤ <strong>Baseline (Random/Majority)</strong> by Sex</summary>

| Sex | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ M | 0.6061 | 0.6111 | 0.6045 | 108 |
| F | 0.5021 | 0.5326 | 0.5326 | 92 |

</details>

<details>
<summary>ğŸ“… <strong>Baseline (Random/Majority)</strong> by Age Group</summary>

| Age Group | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ age_80-100 | 0.6000 | 0.5455 | 0.5299 | 11 |
| ğŸ¥ˆ age_60-80 | 0.5943 | 0.5857 | 0.5788 | 70 |
| ğŸ¥‰ age_20-40 | 0.5819 | 0.5741 | 0.5668 | 54 |
| age_40-60 | 0.4810 | 0.5692 | 0.5513 | 65 |

</details>

<details>
<summary>ğŸ”¬ <strong>BrainLM</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ Philips | 0.6226 | 0.5479 | 0.5476 | 73 |
| ğŸ¥ˆ Siemens | 0.5099 | 0.5088 | 0.5086 | 57 |
| GE | 0.4087 | 0.4714 | 0.4687 | 70 |

</details>

<details>
<summary>ğŸ¥ <strong>BrainLM</strong> by Site</summary>

| Site | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ SiteA | 0.5791 | 0.5417 | 0.5394 | 72 |
| ğŸ¥ˆ SiteC | 0.4944 | 0.5070 | 0.5035 | 71 |
| SiteB | 0.4643 | 0.4737 | 0.4722 | 57 |

</details>

<details>
<summary>ğŸ©º <strong>BrainLM</strong> by Disease Stage</summary>

| Disease Stage | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ MCI | 0.6372 | 0.6143 | 0.6136 | 70 |
| ğŸ¥ˆ AD | 0.4649 | 0.4500 | 0.4462 | 60 |
| CN | 0.4286 | 0.4571 | 0.4571 | 70 |

</details>

<details>
<summary>ğŸ‘¤ <strong>BrainLM</strong> by Sex</summary>

| Sex | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ F | 0.5513 | 0.5217 | 0.5208 | 92 |
| M | 0.4881 | 0.5000 | 0.4985 | 108 |

</details>

<details>
<summary>ğŸ“… <strong>BrainLM</strong> by Age Group</summary>

| Age Group | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ age_40-60 | 0.5730 | 0.5538 | 0.5430 | 65 |
| ğŸ¥ˆ age_80-100 | 0.5667 | 0.4545 | 0.4500 | 11 |
| ğŸ¥‰ age_60-80 | 0.5357 | 0.5286 | 0.5238 | 70 |
| age_20-40 | 0.4528 | 0.4444 | 0.4444 | 54 |

</details>

<details>
<summary>ğŸŒ <strong>BrainLM</strong> by Ethnicity</summary>

| Ethnicity | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ Asian | 0.6667 | 0.5862 | 0.5817 | 29 |
| ğŸ¥ˆ Black | 0.6165 | 0.6316 | 0.6306 | 38 |
| ğŸ¥‰ White | 0.4717 | 0.4545 | 0.4522 | 77 |
| Hispanic | 0.4630 | 0.5116 | 0.5106 | 43 |
| Other | 0.2857 | 0.3077 | 0.3077 | 13 |

</details>

---
*Ranked by **AUROC** (higher is better). Last updated from 6 evaluation(s).*

### ğŸ§  fMRI Foundation Model Benchmark (Granular)

ğŸ“‹ **Task**: Classification/Reconstruction | ğŸ¥ **Health Topic**: Functional Brain Imaging Analysis

!!! info "Clinical Relevance"
    Foundation models for fMRI must generalize across diverse acquisition  parameters, scanner manufacturers, and preprocessing pipelines. This benchmark provides granular rankings to identify optimal model-data matches.


#### ğŸ† Leaderboard

```
         ğŸ¥‡          
     [Brain-JEPA]    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
 ğŸ¥ˆ â”‚         â”‚ ğŸ¥‰  
[BrainLM]â”‚         â”‚[BrainBERT]
â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€
```

**All 5 models ranked by AUROC:**

| Rank | Model | Score | Performance | Dataset | Date |
| :---: | :--- | :---: | :---: | :--- | :---: |
| ğŸ¥‡ | **Brain-JEPA** ğŸ‘‘ | 0.9250 | â­ Excellent | hcp_1200 | 2024-01-22 |
| ğŸ¥ˆ | **BrainLM** ğŸŒŸ | 0.9100 | â­ Excellent | hcp_1200 | 2024-01-15 |
| ğŸ¥‰ | **BrainBERT** âœ¨ | 0.8700 | âœ… Good | hcp_1200 | 2024-01-10 |
| ğŸ… | BrainMT | 0.8500 | âœ… Good | hcp_1200 | 2024-01-18 |
| ğŸ… | NeuroClips | 0.8300 | âœ… Good | hcp_1200 | 2024-01-05 |


#### ğŸ“– Ranking Explanation

!!! abstract "Why These Rankings?"
    **ğŸ¥‡ Brain-JEPA** leads with AUROC=0.9250

    - Gap to ğŸ¥ˆ **BrainLM**: +0.0150 (1.6% better)
    - Score range across all models: 0.0950
    - Performance distribution: â­ 2 excellent, âœ… 3 good


<details>
<summary>ğŸ“ <strong>How are models scored?</strong></summary>

---

### ğŸ¯ Primary Metric: `AUROC`

> Area Under ROC Curve - measures discrimination ability (0.5 = random, 1.0 = perfect)

---

### ğŸ“Š Metric Priority

| Priority | Metric | What it measures |
|:---:|:---|:---|
| 1 | `AUROC` | Discrimination (best for imbalanced data) |
| 2 | `Accuracy` | Overall correctness |
| 3 | `F1-Score` | Precision-recall balance |
| 4 | `Sensitivity` | True positive rate |

---

### ğŸ¥ Clinical Readiness Tiers

| Score | Tier | Deployment | Guidance |
|:---:|:---:|:---:|:---|
| â‰¥ 0.90 | â­ Excellent | Production | Clinical decision support ready |
| 0.80-0.89 | âœ… Good | Pilot | Needs prospective validation |
| 0.70-0.79 | ğŸ”¶ Fair | Research | Not for patient-facing use |
| < 0.70 | ğŸ“ˆ Developing | Development | Significant improvement needed |

---

### ğŸ“ Ranking Rules

1. Ranked by **primary metric** (higher = better)
2. Ties broken by secondary metrics
3. Best run per model used
4. 4 decimal precision

---

### âš–ï¸ Fairness Analysis

Models evaluated across:

| Category | Strata |
|:---|:---|
| ğŸ‘¤ Demographics | Age, sex, ethnicity |
| ğŸ”¬ Technical | Scanner, acquisition |
| ğŸ¥ Clinical | Disease stage, site |

> âš ï¸ Gaps >10% flagged for review

---

*Aligned with [ITU/WHO AI4H DEL3](https://www.itu.int/pub/T-FG-AI4H) standards.*

</details>


#### ğŸ“‹ Complete Metrics Comparison

| Rank | Model | AUROC | Accuracy | F1-Score | Correlation | MSE |
|:---:|:---|:---:|:---:|:---:|:---:|:---:|
| ğŸ¥‡ â­ | Brain-JEPA | **0.9250** | 0.8900 | 0.8800 | 0.8300 | 0.3900 |
| ğŸ¥ˆ â­ | BrainLM | **0.9100** | 0.8700 | 0.8600 | 0.8100 | 0.4200 |
| ğŸ¥‰ âœ… | BrainBERT | **0.8700** | 0.8200 | 0.8100 | 0.7600 | 0.5100 |
| ğŸ… âœ… | BrainMT | **0.8500** | 0.8100 | 0.8000 | 0.7400 | 0.5500 |
| ğŸ… âœ… | NeuroClips | **0.8300** | 0.7900 | 0.7800 | 0.7200 | 0.5800 |

!!! tip "Legend"
    ğŸ“Š **Primary metric**: AUROC (bold) | â­ Excellent (â‰¥0.9) | âœ… Good (â‰¥0.8) | ğŸ”¶ Fair (â‰¥0.7) | ğŸ“ˆ Developing (<0.7)

#### ğŸ“Š Granular Performance Breakdown

Expand sections below to see how models perform across different conditions:


<details>
<summary>ğŸ”¬ <strong>Brain-JEPA</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ Siemens | 0.9400 | 0.9100 | 0.9000 | 450 |
| ğŸ¥ˆ Philips | 0.9200 | 0.8800 | 0.8700 | 370 |
| GE | 0.9100 | 0.8700 | 0.8600 | 380 |

</details>

<details>
<summary>ğŸ“¡ <strong>Brain-JEPA</strong> by Acquisition Type</summary>

| Acquisition Type | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ resting_state | 0.9350 | 0.9000 | 0.8900 | 600 |
| task_based | 0.9100 | 0.8700 | 0.8600 | 400 |

</details>

<details>
<summary>ğŸ”¬ <strong>BrainLM</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ Siemens | 0.9300 | 0.8900 | 0.8800 | 450 |
| ğŸ¥ˆ Philips | 0.9000 | 0.8600 | 0.8500 | 370 |
| GE | 0.8800 | 0.8400 | 0.8300 | 380 |

</details>

<details>
<summary>ğŸ¥ <strong>BrainLM</strong> by Site</summary>

| Site | AUROC | Accuracy | N |
|---|---|---|---|
| ğŸ¥‡ WashU | 0.9300 | 0.8900 | 220 |
| ğŸ¥ˆ MGH | 0.9200 | 0.8800 | 200 |
| ğŸ¥‰ Oxford | 0.9100 | 0.8700 | 200 |
| UCLA | 0.9000 | 0.8600 | 180 |
| UMinn | 0.8900 | 0.8500 | 200 |

</details>

<details>
<summary>ğŸ“¡ <strong>BrainLM</strong> by Acquisition Type</summary>

| Acquisition Type | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ resting_state | 0.9200 | 0.8800 | 0.8700 | 600 |
| ğŸ¥ˆ language | 0.9100 | 0.8700 | - | 100 |
| ğŸ¥‰ working_memory | 0.9000 | 0.8600 | - | 150 |
| task_based | 0.8900 | 0.8500 | 0.8400 | 400 |
| motor | 0.8800 | 0.8400 | - | 150 |

</details>

<details>
<summary>âš™ï¸ <strong>BrainLM</strong> by Preprocessing</summary>

| Preprocessing | AUROC | Accuracy | N |
|---|---|---|---|
| ğŸ¥‡ fmriprep | 0.9200 | 0.8800 | 500 |
| ğŸ¥ˆ hcp | 0.9100 | 0.8700 | 400 |
| minimal | 0.8500 | 0.8100 | 300 |

</details>

<details>
<summary>ğŸ§² <strong>BrainLM</strong> by Field Strength</summary>

| Field Strength | AUROC | Accuracy | N |
|---|---|---|---|
| ğŸ¥‡ 7T | 0.9400 | 0.9100 | 100 |
| 3T | 0.9100 | 0.8700 | 900 |

</details>

<details>
<summary>ğŸ”¬ <strong>BrainBERT</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ Siemens | 0.8900 | 0.8400 | 0.8300 | 450 |
| ğŸ¥ˆ GE | 0.8600 | 0.8100 | 0.8000 | 380 |
| Philips | 0.8500 | 0.8000 | 0.7900 | 370 |

</details>

<details>
<summary>ğŸ“¡ <strong>BrainBERT</strong> by Acquisition Type</summary>

| Acquisition Type | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ resting_state | 0.8800 | 0.8300 | 0.8200 | 600 |
| task_based | 0.8500 | 0.8000 | 0.7900 | 400 |

</details>

<details>
<summary>ğŸ”¬ <strong>BrainMT</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ Siemens | 0.8700 | 0.8300 | 0.8200 | 450 |
| ğŸ¥ˆ GE | 0.8400 | 0.8000 | 0.7900 | 380 |
| Philips | 0.8300 | 0.7900 | 0.7800 | 370 |

</details>

<details>
<summary>ğŸ”¬ <strong>NeuroClips</strong> by Scanner</summary>

| Scanner | AUROC | Accuracy | F1-Score | N |
|---|---|---|---|---|
| ğŸ¥‡ Siemens | 0.8500 | 0.8100 | 0.8000 | 450 |
| ğŸ¥ˆ GE | 0.8200 | 0.7800 | 0.7700 | 380 |
| Philips | 0.8100 | 0.7700 | 0.7600 | 370 |

</details>

---
*Ranked by **AUROC** (higher is better). Last updated from 5 evaluation(s).*

---

## ğŸš€ Get Your Model on the Leaderboard

Want to see your Foundation Model ranked here?

1. ğŸ“¥ **Download** the benchmark suite and run locally
2. ğŸ§ª **Evaluate** your model: `python -m fmbench run --help`
3. ğŸ“¤ **Submit** your results via [GitHub Issue](https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub/issues/new?template=benchmark_submission.md)

ğŸ’¡ **Propose new evaluation protocols** via [Issue](https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub/issues/new?template=protocol_proposal.md)

!!! note "Curated Benchmark Hub"
    All submissions are reviewed before being added. See [Submission Guide](../contributing/submission_guide.md) for details.

*Aligned with [ITU/WHO FG-AI4H](https://www.itu.int/pub/T-FG-AI4H) standards for healthcare AI evaluation.*
