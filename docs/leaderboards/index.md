# ğŸ† Foundation Model Leaderboards

!!! success "Benchmark Hub Overview"
    ğŸ“Š **7** Benchmarks | ğŸ¤– **21** Models | ğŸ“ˆ **38** Evaluations


> **What is this?** This page ranks AI models for healthcare applications. 
> Higher-ranked models perform better on standardized tests.
> 
> **How to read it:** Each table shows models from best (ğŸ¥‡) to developing (ğŸ“ˆ).
> Click "How are scores calculated?" for details on what the numbers mean.

## ğŸ§­ Jump To

- [ğŸŒ Overall Rankings](#overall-rankings-all-modalities) â€” Best across all categories
- [ğŸ§¬ Genomics](#genomics)
- [ğŸ§  Brain Imaging (MRI/fMRI)](#brain-imaging-mrifmri)

---

## ğŸŒ Overall Rankings (All Modalities)

*Best score per model across all benchmarks*

| Rank | Model | Best Score | Benchmark | Modality |
|:---:|:---|:---:|:---|:---|
| ğŸ¥‡ | **geneformer** ğŸ‘‘ | 0.9995 | Foundation Model Robustne | ğŸ“Š Other |
| ğŸ¥ˆ | **Brain-JEPA** | 0.9350 | Alzheimer's Disease Class | ğŸ§  Brain Imaging ( |
| ğŸ¥‰ | **Evo 2** | 0.9250 | Cell Type Annotation | ğŸ§¬ Genomics |
| ğŸ… | UNI | 0.9200 | Alzheimer's Disease Class | ğŸ§  Brain Imaging ( |
| ğŸ… | Geneformer | 0.9100 | Cell Type Annotation | ğŸ§¬ Genomics |
| ğŸ–ï¸ | BrainLM | 0.9100 | fMRI Foundation Model Ben | ğŸ§  Brain Imaging ( |
| ğŸ–ï¸ | SWIFT | 0.8950 | Cell Type Annotation | ğŸ§¬ Genomics |
| ğŸ–ï¸ | Caduceus | 0.8850 | Cell Type Annotation | ğŸ§¬ Genomics |
| ğŸ–ï¸ | Me-LLaMA | 0.8750 | Clinical Report Generatio | ğŸ§¬ Genomics |
| ğŸ–ï¸ | BrainBERT | 0.8700 | fMRI Foundation Model Ben | ğŸ§  Brain Imaging ( |
| #11 | HyenaDNA | 0.8700 | Cell Type Annotation | ğŸ§¬ Genomics |
| #12 | M3FM | 0.8600 | Clinical Report Generatio | ğŸ§¬ Genomics |
| #13 | DNABERT-2 | 0.8500 | Cell Type Annotation | ğŸ§¬ Genomics |
| #14 | BrainMT | 0.8500 | fMRI Foundation Model Ben | ğŸ§  Brain Imaging ( |
| #15 | BrainHarmony | 0.8450 | Foundation Model Robustne | ğŸ“Š Other |
| #16 | OpenFlamingo | 0.8400 | Clinical Report Generatio | ğŸ§¬ Genomics |
| #17 | NeuroClips | 0.8300 | fMRI Foundation Model Ben | ğŸ§  Brain Imaging ( |
| #18 | TITAN | 0.8100 | Clinical Report Generatio | ğŸ§¬ Genomics |
| #19 | Baseline (Random/Majority) | 0.7810 | Foundation Model Robustne | ğŸ“Š Other |
| #20 | Med-Flamingo | 0.7800 | Clinical Report Generatio | ğŸ§¬ Genomics |
| #21 | RadBERT | 0.6900 | Clinical Report Generatio | ğŸ§¬ Genomics |

!!! abstract "Performance Distribution"
    â­ 6 Excellent | âœ… 12 Good | ğŸ”¶ 2 Fair | ğŸ“ˆ 1 Developing

---

## ğŸ§¬ Genomics

### ğŸ¯ Classification

#### Cell Type Annotation

*Predicting cell types from single-cell RNA-seq data.*


<div align="center">

```
                    ğŸ†                    
                                          
              ğŸ¥‡   Evo 2                 
                 (0.925)                 
             â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—             
             â•‘               â•‘             
   ğŸ¥ˆ Geneformer   â•‘               â•‘   ğŸ¥‰   SWIFT      
      (0.910)      â•‘               â•‘      (0.895)      
  â•”â•â•â•â•â•â•â•â•â•â•â•â•               â•šâ•â•â•â•â•â•â•â•â•â•â•â•—  
  â•‘                                       â•‘  
â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•
```

</div>

**6 models ranked by `Accuracy`:**

| Rank | Model | Score | Level | Details |
|:---:|:---|:---:|:---:|:---|
| ğŸ¥‡ | **Evo 2** ğŸ‘‘ | 0.9250 | â­ Excellent | PBMC 68k, 2024-02-01 |
| ğŸ¥ˆ | **Geneformer** | 0.9100 | â­ Excellent | PBMC 68k, 2023-11-01 |
| ğŸ¥‰ | **SWIFT** | 0.8950 | âœ… Good | PBMC 68k, 2024-01-15 |
| ğŸ… | Caduceus | 0.8850 | âœ… Good | PBMC 68k, 2024-01-12 |
| ğŸ… | HyenaDNA | 0.8700 | âœ… Good | PBMC 68k, 2024-01-08 |
| ğŸ–ï¸ | DNABERT-2 | 0.8500 | âœ… Good | PBMC 68k, 2024-01-05 |

!!! tip "Quick Comparison"
    **ğŸ¥‡ Evo 2** leads with Accuracy = **0.9250**

    - Gap to ğŸ¥ˆ Geneformer: +0.0150
    - Score spread (best to worst): 0.0750


<details>
<summary>ğŸ“ <strong>How are scores calculated?</strong> (click to expand)</summary>

---

### ğŸ¯ What We Measure: `Accuracy`

> **Accuracy**
>
> Percentage of correct predictions
>
> ğŸ“ Range: 0% â†’ 100% (or 0.0 â†’ 1.0)

---

### ğŸ“Š What Do Scores Mean?

| Score | Rating | What It Means |
|:---:|:---:|:---|
| **â‰¥ 0.90** | â­ Excellent | Ready for real-world use with monitoring |
| **0.80-0.89** | âœ… Good | Promising, needs more testing |
| **0.70-0.79** | ğŸ”¶ Fair | Research use only |
| **< 0.70** | ğŸ“ˆ Developing | Needs more work |

---

### ğŸ“ How We Rank

1. **Higher score = Better ranking** (except for error metrics)
2. If scores tie, we look at secondary metrics
3. Only the best run from each model counts

---

!!! info "Standards Alignment"
    This follows [ITU/WHO AI4H](https://www.itu.int/pub/T-FG-AI4H) guidelines for healthcare AI evaluation.

</details>

---

### âœï¸ Generation

#### Clinical Report Generation Quality


<div align="center">

```
                    ğŸ†                    
                                          
              ğŸ¥‡   Me-LLaMA                
                 (0.875)                 
             â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—             
             â•‘               â•‘             
   ğŸ¥ˆ     M3FM       â•‘               â•‘   ğŸ¥‰ OpenFlamingo   
      (0.860)      â•‘               â•‘      (0.840)      
  â•”â•â•â•â•â•â•â•â•â•â•â•â•               â•šâ•â•â•â•â•â•â•â•â•â•â•â•—  
  â•‘                                       â•‘  
â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•
```

</div>

**6 models ranked by `report_quality_score`:**

| Rank | Model | Score | Level | Details |
|:---:|:---|:---:|:---:|:---|
| ğŸ¥‡ | **Me-LLaMA** ğŸ‘‘ | 0.8750 | âœ… Good | mimic_cxr_reports, 2024-02-05 |
| ğŸ¥ˆ | **M3FM** | 0.8600 | âœ… Good | mimic_cxr_reports, 2024-01-28 |
| ğŸ¥‰ | **OpenFlamingo** | 0.8400 | âœ… Good | mimic_cxr_reports, 2024-01-20 |
| ğŸ… | TITAN | 0.8100 | âœ… Good | mimic_cxr_reports, 2024-01-25 |
| ğŸ… | Med-Flamingo | 0.7800 | ğŸ”¶ Fair | mimic_cxr_reports, 2024-01-18 |
| ğŸ–ï¸ | RadBERT | 0.6900 | ğŸ“ˆ Developing | mimic_cxr_reports, 2024-01-12 |

!!! tip "Quick Comparison"
    **ğŸ¥‡ Me-LLaMA** leads with report_quality_score = **0.8750**

    - Gap to ğŸ¥ˆ M3FM: +0.0150
    - Score spread (best to worst): 0.1850


<details>
<summary>ğŸ“ <strong>How are scores calculated?</strong> (click to expand)</summary>

---

### ğŸ¯ What We Measure: `report_quality_score`

> **Report Quality Score**
>
> Overall quality of generated medical reports
>
> ğŸ“ Range: 0.0 (poor) â†’ 1.0 (excellent)

---

### ğŸ“Š What Do Scores Mean?

| Score | Rating | What It Means |
|:---:|:---:|:---|
| **â‰¥ 0.90** | â­ Excellent | Ready for real-world use with monitoring |
| **0.80-0.89** | âœ… Good | Promising, needs more testing |
| **0.70-0.79** | ğŸ”¶ Fair | Research use only |
| **< 0.70** | ğŸ“ˆ Developing | Needs more work |

---

### ğŸ“ How We Rank

1. **Higher score = Better ranking** (except for error metrics)
2. If scores tie, we look at secondary metrics
3. Only the best run from each model counts

---

!!! info "Standards Alignment"
    This follows [ITU/WHO AI4H](https://www.itu.int/pub/T-FG-AI4H) guidelines for healthcare AI evaluation.

</details>

---

## ğŸ§  Brain Imaging (MRI/fMRI)

### ğŸ¯ Classification

#### Toy Classification Benchmark

*A toy benchmark for testing the pipeline.*

**2 models ranked by `AUROC`:**

| Rank | Model | Score | Level | Details |
|:---:|:---|:---:|:---:|:---|
| ğŸ¥‡ | **Baseline (Random/Majority)** ğŸ‘‘ | 0.5597 | ğŸ“ˆ Developing | Toy fMRI Classificat, 2025-11-27 |
| ğŸ¥ˆ | **BrainLM** | 0.5193 | ğŸ“ˆ Developing | Toy fMRI Classificat, 2025-11-27 |

!!! tip "Quick Comparison"
    **ğŸ¥‡ Baseline (Random/Majority)** leads with AUROC = **0.5597**

    - Gap to ğŸ¥ˆ BrainLM: +0.0404


<details>
<summary>ğŸ“ <strong>How are scores calculated?</strong> (click to expand)</summary>

---

### ğŸ¯ What We Measure: `AUROC`

> **Area Under ROC Curve**
>
> How well the model distinguishes between classes
>
> ğŸ“ Range: 0.5 (random guess) â†’ 1.0 (perfect)

---

### ğŸ“Š What Do Scores Mean?

| Score | Rating | What It Means |
|:---:|:---:|:---|
| **â‰¥ 0.90** | â­ Excellent | Ready for real-world use with monitoring |
| **0.80-0.89** | âœ… Good | Promising, needs more testing |
| **0.70-0.79** | ğŸ”¶ Fair | Research use only |
| **< 0.70** | ğŸ“ˆ Developing | Needs more work |

---

### ğŸ“ How We Rank

1. **Higher score = Better ranking** (except for error metrics)
2. If scores tie, we look at secondary metrics
3. Only the best run from each model counts

---

!!! info "Standards Alignment"
    This follows [ITU/WHO AI4H](https://www.itu.int/pub/T-FG-AI4H) guidelines for healthcare AI evaluation.

</details>

---

#### Alzheimer's Disease Classification using Brain MRI

*Binary classification of AD vs CN using structural MRI data.*


<div align="center">

```
                    ğŸ†                    
                                          
              ğŸ¥‡ Brain-JEPA              
                 (0.935)                 
             â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—             
             â•‘               â•‘             
   ğŸ¥ˆ    UNI       â•‘               â•‘   ğŸ¥‰  BrainLM     
      (0.920)      â•‘               â•‘      (0.910)      
  â•”â•â•â•â•â•â•â•â•â•â•â•â•               â•šâ•â•â•â•â•â•â•â•â•â•â•â•—  
  â•‘                                       â•‘  
â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•
```

</div>

**3 models ranked by `AUROC`:**

| Rank | Model | Score | Level | Details |
|:---:|:---|:---:|:---:|:---|
| ğŸ¥‡ | **Brain-JEPA** ğŸ‘‘ | 0.9350 | â­ Excellent | ADNI, 2024-01-20 |
| ğŸ¥ˆ | **UNI** | 0.9200 | â­ Excellent | Alzheimer's Disease , 2023-10-27 |
| ğŸ¥‰ | **BrainLM** | 0.9100 | â­ Excellent | ADNI, 2024-01-15 |

!!! tip "Quick Comparison"
    **ğŸ¥‡ Brain-JEPA** leads with AUROC = **0.9350**

    - Gap to ğŸ¥ˆ UNI: +0.0150
    - Score spread (best to worst): 0.0250


<details>
<summary>ğŸ“ <strong>How are scores calculated?</strong> (click to expand)</summary>

---

### ğŸ¯ What We Measure: `AUROC`

> **Area Under ROC Curve**
>
> How well the model distinguishes between classes
>
> ğŸ“ Range: 0.5 (random guess) â†’ 1.0 (perfect)

---

### ğŸ“Š What Do Scores Mean?

| Score | Rating | What It Means |
|:---:|:---:|:---|
| **â‰¥ 0.90** | â­ Excellent | Ready for real-world use with monitoring |
| **0.80-0.89** | âœ… Good | Promising, needs more testing |
| **0.70-0.79** | ğŸ”¶ Fair | Research use only |
| **< 0.70** | ğŸ“ˆ Developing | Needs more work |

---

### ğŸ“ How We Rank

1. **Higher score = Better ranking** (except for error metrics)
2. If scores tie, we look at secondary metrics
3. Only the best run from each model counts

---

!!! info "Standards Alignment"
    This follows [ITU/WHO AI4H](https://www.itu.int/pub/T-FG-AI4H) guidelines for healthcare AI evaluation.

</details>

---

### ğŸ“‹ Classification/Reconstruction

#### fMRI Foundation Model Benchmark (Granular)


<div align="center">

```
                    ğŸ†                    
                                          
              ğŸ¥‡ Brain-JEPA              
                 (0.925)                 
             â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—             
             â•‘               â•‘             
   ğŸ¥ˆ  BrainLM     â•‘               â•‘   ğŸ¥‰ BrainBERT    
      (0.910)      â•‘               â•‘      (0.870)      
  â•”â•â•â•â•â•â•â•â•â•â•â•â•               â•šâ•â•â•â•â•â•â•â•â•â•â•â•—  
  â•‘                                       â•‘  
â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•
```

</div>

**5 models ranked by `AUROC`:**

| Rank | Model | Score | Level | Details |
|:---:|:---|:---:|:---:|:---|
| ğŸ¥‡ | **Brain-JEPA** ğŸ‘‘ | 0.9250 | â­ Excellent | hcp_1200, 2024-01-22 |
| ğŸ¥ˆ | **BrainLM** | 0.9100 | â­ Excellent | hcp_1200, 2024-01-15 |
| ğŸ¥‰ | **BrainBERT** | 0.8700 | âœ… Good | hcp_1200, 2024-01-10 |
| ğŸ… | BrainMT | 0.8500 | âœ… Good | hcp_1200, 2024-01-18 |
| ğŸ… | NeuroClips | 0.8300 | âœ… Good | hcp_1200, 2024-01-05 |

!!! tip "Quick Comparison"
    **ğŸ¥‡ Brain-JEPA** leads with AUROC = **0.9250**

    - Gap to ğŸ¥ˆ BrainLM: +0.0150
    - Score spread (best to worst): 0.0950


<details>
<summary>ğŸ“ <strong>How are scores calculated?</strong> (click to expand)</summary>

---

### ğŸ¯ What We Measure: `AUROC`

> **Area Under ROC Curve**
>
> How well the model distinguishes between classes
>
> ğŸ“ Range: 0.5 (random guess) â†’ 1.0 (perfect)

---

### ğŸ“Š What Do Scores Mean?

| Score | Rating | What It Means |
|:---:|:---:|:---|
| **â‰¥ 0.90** | â­ Excellent | Ready for real-world use with monitoring |
| **0.80-0.89** | âœ… Good | Promising, needs more testing |
| **0.70-0.79** | ğŸ”¶ Fair | Research use only |
| **< 0.70** | ğŸ“ˆ Developing | Needs more work |

---

### ğŸ“ How We Rank

1. **Higher score = Better ranking** (except for error metrics)
2. If scores tie, we look at secondary metrics
3. Only the best run from each model counts

---

!!! info "Standards Alignment"
    This follows [ITU/WHO AI4H](https://www.itu.int/pub/T-FG-AI4H) guidelines for healthcare AI evaluation.

</details>

---

### ğŸ”„ Reconstruction

#### Brain Time-Series Modeling

*Evaluating ability to reconstruct masked fMRI voxel time-series.*

**1 models ranked by `Correlation`:**

| Rank | Model | Score | Level | Details |
|:---:|:---|:---:|:---:|:---|
| ğŸ¥‡ | **BrainLM** ğŸ‘‘ | 0.7800 | ğŸ”¶ Fair | UK Biobank fMRI tens, 2025-11-15 |


<details>
<summary>ğŸ“ <strong>How are scores calculated?</strong> (click to expand)</summary>

---

### ğŸ¯ What We Measure: `Correlation`

> **Correlation**: Performance measure

---

### ğŸ“Š What Do Scores Mean?

| Score | Rating | What It Means |
|:---:|:---:|:---|
| **â‰¥ 0.90** | â­ Excellent | Ready for real-world use with monitoring |
| **0.80-0.89** | âœ… Good | Promising, needs more testing |
| **0.70-0.79** | ğŸ”¶ Fair | Research use only |
| **< 0.70** | ğŸ“ˆ Developing | Needs more work |

---

### ğŸ“ How We Rank

1. **Higher score = Better ranking** (except for error metrics)
2. If scores tie, we look at secondary metrics
3. Only the best run from each model counts

---

!!! info "Standards Alignment"
    This follows [ITU/WHO AI4H](https://www.itu.int/pub/T-FG-AI4H) guidelines for healthcare AI evaluation.

</details>

---

## ğŸ“‹ Other Benchmarks

### Foundation Model Robustness Evaluation

| Rank | Model | Score | Level | Details |
|:---:|:---|:---:|:---:|:---|
| ğŸ¥‡ | **geneformer** ğŸ‘‘ | 0.9995 | â­ Excellent | -, 2025-11-27 |
| ğŸ¥ˆ | **Brain-JEPA** | 0.8650 | âœ… Good | DS-TOY-NEURO-ROBUSTN, 2024-01-20 |
| ğŸ¥‰ | **BrainHarmony** | 0.8450 | âœ… Good | DS-TOY-NEURO-ROBUSTN, 2024-01-18 |
| ğŸ… | Geneformer | 0.8350 | âœ… Good | DS-TOY-GENOMICS, 2024-01-10 |
| ğŸ… | BrainLM | 0.8250 | âœ… Good | DS-TOY-NEURO-ROBUSTN, 2024-01-16 |
| ğŸ–ï¸ | HyenaDNA | 0.7950 | ğŸ”¶ Fair | DS-TOY-GENOMICS, 2024-01-12 |
| ğŸ–ï¸ | Baseline (Random/Majority) | 0.7810 | ğŸ”¶ Fair | -, 2025-11-27 |
| ğŸ–ï¸ | Baseline (Random/Majority) | 0.7810 | ğŸ”¶ Fair | -, 2025-11-27 |
| ğŸ–ï¸ | Baseline (Random/Majority) | 0.7810 | ğŸ”¶ Fair | -, 2025-11-27 |
| ğŸ–ï¸ | Baseline (Random/Majority) | 0.7749 | ğŸ”¶ Fair | -, 2025-11-27 |
| #11 | Baseline (Random/Majority) | 0.4554 | ğŸ“ˆ Developing | -, 2025-11-27 |

---


## ğŸš€ Add Your Model

Want your model on this leaderboard?

1. **Download** the benchmark toolkit
2. **Run locally** on your model (your code stays private!)
3. **Submit results** via [GitHub Issue](https://github.com/allison-eunse/ai4h-inspired-fm-benchmark-hub/issues/new?template=benchmark_submission.md)

[ğŸ“¥ Get Started](../index.md){ .md-button .md-button--primary }
[ğŸ“– Submission Guide](../contributing/submission_guide.md){ .md-button }

---

*Aligned with [ITU/WHO FG-AI4H](https://www.itu.int/pub/T-FG-AI4H) standards for healthcare AI evaluation.*
